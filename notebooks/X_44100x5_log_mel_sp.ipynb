{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/audio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/root/anaconda3/envs/audio/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 10968078746),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:1, GPU, 10968078746)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import librosa\n",
    "import shutil\n",
    "import keras\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "\n",
    "from keras.optimizers import *\n",
    "\n",
    "from keras.regularizers import *\n",
    "\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.applications import *\n",
    "from keras.utils import *\n",
    "from keras.callbacks import *\n",
    "from sklearn.model_selection import *\n",
    "from keras.preprocessing.image import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2, 3'\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session)\n",
    "from keras import backend as K\n",
    "K.get_session().list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../audio-data/')\n",
    "train_path = '../audio-data/audio_train/'\n",
    "test_path = '../audio-data/audio_test/'\n",
    "train = pd.read_csv('../audio-data/train.csv')\n",
    "test = pd.read_csv('../audio-data/sample_submission.csv')\n",
    "# print('training samples: ', len(os.listdir(train_path)))\n",
    "# print('test samples: ', len(os.listdir(test_path)))\n",
    "# print('training labels: ', len(train.label.unique()))\n",
    "# print(train.head())\n",
    "LABELS = list(train.label.unique())\n",
    "label_idx = {label: i for i, label in enumerate(LABELS)}\n",
    "train.set_index('fname', inplace=True)\n",
    "test.set_index('fname', inplace=True)\n",
    "train['label_idx'] = train.label.apply(lambda x: label_idx[x])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                sampling_rate=44100, audio_duration=2, n_classes=41,\n",
    "                use_mfcc=False, n_folds=10, learning_rate=0.0001,\n",
    "                max_epochs=50, n_mfcc=20, use_log_sp=False, use_mixup=False, alpha=0.2, use_log_mel_sp=False):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = n_classes\n",
    "        self.use_log_sp = use_log_sp\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.use_mixup = use_mixup\n",
    "        self.use_log_mel_sp = use_log_mel_sp\n",
    "        self.alpha = alpha\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_folds = n_folds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            # np.floor 计算比每一个元素小或相等的最大的整数\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        elif self.use_log_sp:\n",
    "            self.dim = (self.audio_duration*100-1, self.sampling_rate//100+1, 3)\n",
    "        elif self.use_log_mel_sp:\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram.shape[0] = time*100 - 1\n",
    "# spectrogram.shape[1] = rate/100 + 1\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "def audio_norm(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data - min_data) / (max_data - min_data + 1e-6)\n",
    "    return data - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, _ = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        c = np.random.uniform(v_l, v_h)\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9473, 256, 431, 1) (9400, 256, 431, 1) (9473, 41)\n",
      "CPU times: user 271 ms, sys: 10.8 s, total: 11.1 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = np.load('X_44100x5_log_mel_sp_train.npy')\n",
    "X_test = np.load('X_44100x5_log_mel_sp_test.npy')\n",
    "# X_train = np.load('X_1d_train.npy')\n",
    "# X_test = np.load('X_1d_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9473, 256, 431, 1) (9400, 256, 431, 1)\n",
      "CPU times: user 20.6 s, sys: 27.2 s, total: 47.8 s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def normalization(x):\n",
    "    mean = np.mean(x, axis=0)\n",
    "    std = np.std(x, axis=0)\n",
    "    x = (x - mean) / std\n",
    "    return x\n",
    "X_train = normalization(X_train)\n",
    "X_test = normalization(X_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9473, 256, 431, 1) (9473, 41)\n",
      "CPU times: user 6.57 s, sys: 13.8 s, total: 20.4 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def mixup(data, targets, alpha):\n",
    "        size = data.shape[0]\n",
    "        weight = np.random.beta(alpha, alpha, size)\n",
    "      \n",
    "        x_weight = weight.reshape(size, 1, 1, 1)\n",
    "#         x_weight = weight.reshape(size, 1, 1)\n",
    "        y_weight = weight.reshape(size, 1)\n",
    "        index = np.random.permutation(size)\n",
    "        x1, x2 = data, data[index]\n",
    "        x = x1 * x_weight + x2 * (1 - x_weight)\n",
    "        y1, y2 = targets, targets[index]\n",
    "        y = y1 * y_weight + y2 * (1 - y_weight)\n",
    "        return x, y\n",
    "X_train_mixup, y_train_mixup = mixup(X_train, y_train, 2)\n",
    "print(X_train_mixup.shape, y_train_mixup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self,model,filepath, monitor='val_loss', verbose=1,\n",
    "                 save_best_only=True, save_weights_only=True,\n",
    "                 mode='auto', period=1):\n",
    "        self.single_model = model\n",
    "        super(ParallelModelCheckpoint,self).__init__(filepath, monitor, verbose,save_best_only, save_weights_only,mode, period)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        super(ParallelModelCheckpoint,self).set_model(self.single_model)\n",
    "\n",
    "\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, path):\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        self.best_loss = np.inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs['val_loss']\n",
    "        if val_loss < self.best_loss:\n",
    "            print(\"\\nValidation loss decreased from {} to {}, saving model\".format(self.best_loss, val_loss))\n",
    "            self.model.save_weights(self.path, overwrite=True)\n",
    "            self.best_loss = val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    nclass = config.n_classes\n",
    "    \n",
    "    inp = Input(shape=(config.dim[0], config.dim[1], 1))\n",
    "    x = Convolution2D(32, (3,3), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "    x = Convolution2D(32, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "    x = Convolution2D(64, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "    x = Convolution2D(64, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "\n",
    "    x = Convolution2D(128, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "    x = Convolution2D(128, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "#     x = Flatten()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256)(x)\n",
    "# #     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "    out = Dense(nclass, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.summary()\n",
    "    print(len(model.layers))\n",
    "    print(config.dim)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (8108, 256, 431, 1) (8108, 41)\n",
      "val shape:  (1365, 256, 431, 1) (1365, 41)\n",
      "Fold:  0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 8108 samples, validate on 1365 samples\n",
      "Epoch 1/300\n",
      "8108/8108 [==============================] - 45s 5ms/step - loss: 2.9255 - acc: 0.1996 - val_loss: 3.6289 - val_acc: 0.1480\n",
      "\n",
      "Validation loss decreased from inf to 3.6288983229752425, saving model\n",
      "Epoch 2/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 2.3213 - acc: 0.3336 - val_loss: 3.5279 - val_acc: 0.2769\n",
      "\n",
      "Validation loss decreased from 3.6288983229752425 to 3.5278671882964754, saving model\n",
      "Epoch 3/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 1.9154 - acc: 0.4531 - val_loss: 4.9817 - val_acc: 0.2293\n",
      "Epoch 4/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 1.6185 - acc: 0.5430 - val_loss: 2.4941 - val_acc: 0.3832\n",
      "\n",
      "Validation loss decreased from 3.5278671882964754 to 2.494111867178054, saving model\n",
      "Epoch 5/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 1.4439 - acc: 0.5920 - val_loss: 1.8529 - val_acc: 0.5201\n",
      "\n",
      "Validation loss decreased from 2.494111867178054 to 1.8529397328694661, saving model\n",
      "Epoch 6/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 1.2864 - acc: 0.6433 - val_loss: 3.3511 - val_acc: 0.3414\n",
      "Epoch 7/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 1.1378 - acc: 0.6804 - val_loss: 1.7297 - val_acc: 0.5590\n",
      "\n",
      "Validation loss decreased from 1.8529397328694661 to 1.72967376045255, saving model\n",
      "Epoch 8/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 1.0551 - acc: 0.7021 - val_loss: 2.2967 - val_acc: 0.5187\n",
      "Epoch 9/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.9616 - acc: 0.7289 - val_loss: 2.0380 - val_acc: 0.5326\n",
      "Epoch 10/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.8744 - acc: 0.7496 - val_loss: 1.7776 - val_acc: 0.5663\n",
      "Epoch 11/300\n",
      "8108/8108 [==============================] - 46s 6ms/step - loss: 0.8063 - acc: 0.7674 - val_loss: 1.6792 - val_acc: 0.5832\n",
      "\n",
      "Validation loss decreased from 1.72967376045255 to 1.6792163628798265, saving model\n",
      "Epoch 12/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.7281 - acc: 0.7911 - val_loss: 1.9432 - val_acc: 0.5348\n",
      "Epoch 13/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.6656 - acc: 0.8080 - val_loss: 1.3411 - val_acc: 0.6681\n",
      "\n",
      "Validation loss decreased from 1.6792163628798265 to 1.3410554447453538, saving model\n",
      "Epoch 14/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.6319 - acc: 0.8152 - val_loss: 1.4031 - val_acc: 0.6542\n",
      "Epoch 15/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.5565 - acc: 0.8424 - val_loss: 1.2102 - val_acc: 0.7172\n",
      "\n",
      "Validation loss decreased from 1.3410554447453538 to 1.2101703795757923, saving model\n",
      "Epoch 16/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.5011 - acc: 0.8538 - val_loss: 1.6221 - val_acc: 0.6396\n",
      "Epoch 17/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.4857 - acc: 0.8571 - val_loss: 1.3600 - val_acc: 0.6813\n",
      "Epoch 18/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.4414 - acc: 0.8707 - val_loss: 1.3550 - val_acc: 0.6901\n",
      "Epoch 19/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.3780 - acc: 0.8902 - val_loss: 1.9025 - val_acc: 0.6205\n",
      "Epoch 20/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.3748 - acc: 0.8875 - val_loss: 1.1698 - val_acc: 0.7421\n",
      "\n",
      "Validation loss decreased from 1.2101703795757923 to 1.1697847263280288, saving model\n",
      "Epoch 21/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.3025 - acc: 0.9156 - val_loss: 1.4493 - val_acc: 0.6989\n",
      "Epoch 22/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.2769 - acc: 0.9188 - val_loss: 1.1533 - val_acc: 0.7575\n",
      "\n",
      "Validation loss decreased from 1.1697847263280288 to 1.1532698383261433, saving model\n",
      "Epoch 23/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.2436 - acc: 0.9297 - val_loss: 1.3047 - val_acc: 0.7326\n",
      "Epoch 24/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.2716 - acc: 0.9197 - val_loss: 1.3082 - val_acc: 0.7282\n",
      "Epoch 25/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.2314 - acc: 0.9287 - val_loss: 1.3609 - val_acc: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.2062 - acc: 0.9382 - val_loss: 1.4244 - val_acc: 0.7209\n",
      "Epoch 27/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.1455 - acc: 0.9597 - val_loss: 1.4868 - val_acc: 0.7055\n",
      "Epoch 28/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.1625 - acc: 0.9489 - val_loss: 1.4292 - val_acc: 0.6974\n",
      "Epoch 29/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0780 - acc: 0.9815 - val_loss: 0.8938 - val_acc: 0.8168\n",
      "\n",
      "Validation loss decreased from 1.1532698383261433 to 0.8937533621386294, saving model\n",
      "Epoch 30/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0481 - acc: 0.9917 - val_loss: 0.8878 - val_acc: 0.8271\n",
      "\n",
      "Validation loss decreased from 0.8937533621386294 to 0.8878387273012937, saving model\n",
      "Epoch 31/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0426 - acc: 0.9940 - val_loss: 0.8919 - val_acc: 0.8300\n",
      "Epoch 32/300\n",
      "8108/8108 [==============================] - 46s 6ms/step - loss: 0.0375 - acc: 0.9965 - val_loss: 0.8988 - val_acc: 0.8256\n",
      "Epoch 33/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0346 - acc: 0.9970 - val_loss: 0.9032 - val_acc: 0.8249\n",
      "Epoch 34/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0324 - acc: 0.9965 - val_loss: 0.8967 - val_acc: 0.8227\n",
      "Epoch 35/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0297 - acc: 0.9975 - val_loss: 0.9120 - val_acc: 0.8271\n",
      "Epoch 36/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0288 - acc: 0.9980 - val_loss: 0.9125 - val_acc: 0.8256\n",
      "Epoch 37/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0272 - acc: 0.9983 - val_loss: 0.9139 - val_acc: 0.8242\n",
      "Epoch 38/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0257 - acc: 0.9988 - val_loss: 0.9147 - val_acc: 0.8249\n",
      "Epoch 39/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0256 - acc: 0.9986 - val_loss: 0.9147 - val_acc: 0.8256\n",
      "Epoch 40/300\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0266 - acc: 0.9981 - val_loss: 0.9152 - val_acc: 0.8264\n",
      "9473/9473 [==============================] - 26s 3ms/step\n",
      "9400/9400 [==============================] - 26s 3ms/step\n",
      "train shape:  (8111, 256, 431, 1) (8111, 41)\n",
      "val shape:  (1362, 256, 431, 1) (1362, 41)\n",
      "Fold:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 8111 samples, validate on 1362 samples\n",
      "Epoch 1/300\n",
      "8111/8111 [==============================] - 50s 6ms/step - loss: 2.9129 - acc: 0.2072 - val_loss: 4.0303 - val_acc: 0.1116\n",
      "\n",
      "Validation loss decreased from inf to 4.030252194789713, saving model\n",
      "Epoch 2/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 2.3035 - acc: 0.3546 - val_loss: 4.9119 - val_acc: 0.1417\n",
      "Epoch 3/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 1.9011 - acc: 0.4687 - val_loss: 2.7931 - val_acc: 0.3135\n",
      "\n",
      "Validation loss decreased from 4.030252194789713 to 2.793066073794372, saving model\n",
      "Epoch 4/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 1.6359 - acc: 0.5420 - val_loss: 2.0333 - val_acc: 0.4486\n",
      "\n",
      "Validation loss decreased from 2.793066073794372 to 2.033334204103803, saving model\n",
      "Epoch 5/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 1.4307 - acc: 0.5966 - val_loss: 1.7678 - val_acc: 0.5345\n",
      "\n",
      "Validation loss decreased from 2.033334204103803 to 1.7678423488017572, saving model\n",
      "Epoch 6/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 1.2788 - acc: 0.6447 - val_loss: 2.1601 - val_acc: 0.4824\n",
      "Epoch 7/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 1.1580 - acc: 0.6780 - val_loss: 2.1091 - val_acc: 0.4853\n",
      "Epoch 8/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 1.0446 - acc: 0.7115 - val_loss: 1.9351 - val_acc: 0.4838\n",
      "Epoch 9/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.9360 - acc: 0.7399 - val_loss: 1.4284 - val_acc: 0.6167\n",
      "\n",
      "Validation loss decreased from 1.7678423488017572 to 1.4284272204411712, saving model\n",
      "Epoch 10/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 0.8735 - acc: 0.7507 - val_loss: 1.3042 - val_acc: 0.6535\n",
      "\n",
      "Validation loss decreased from 1.4284272204411712 to 1.3041614288800614, saving model\n",
      "Epoch 11/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 0.7584 - acc: 0.7865 - val_loss: 1.7192 - val_acc: 0.5756\n",
      "Epoch 12/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.7281 - acc: 0.7920 - val_loss: 2.2183 - val_acc: 0.5029\n",
      "Epoch 13/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.6689 - acc: 0.8090 - val_loss: 1.5862 - val_acc: 0.5910\n",
      "Epoch 14/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 0.6314 - acc: 0.8216 - val_loss: 1.6979 - val_acc: 0.5859\n",
      "Epoch 15/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 0.5794 - acc: 0.8312 - val_loss: 1.5111 - val_acc: 0.6233\n",
      "Epoch 16/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.5073 - acc: 0.8558 - val_loss: 1.7721 - val_acc: 0.5991\n",
      "Epoch 17/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.3362 - acc: 0.9091 - val_loss: 0.8909 - val_acc: 0.7584\n",
      "\n",
      "Validation loss decreased from 1.3041614288800614 to 0.8908930314970086, saving model\n",
      "Epoch 18/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2775 - acc: 0.9326 - val_loss: 0.8745 - val_acc: 0.7577\n",
      "\n",
      "Validation loss decreased from 0.8908930314970086 to 0.8744789136138782, saving model\n",
      "Epoch 19/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2602 - acc: 0.9370 - val_loss: 0.9064 - val_acc: 0.7592\n",
      "Epoch 20/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2470 - acc: 0.9407 - val_loss: 0.8663 - val_acc: 0.7658\n",
      "\n",
      "Validation loss decreased from 0.8744789136138782 to 0.8663492965978323, saving model\n",
      "Epoch 21/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2325 - acc: 0.9445 - val_loss: 0.8641 - val_acc: 0.7709\n",
      "\n",
      "Validation loss decreased from 0.8663492965978323 to 0.8640685254781781, saving model\n",
      "Epoch 22/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2273 - acc: 0.9476 - val_loss: 0.8694 - val_acc: 0.7783\n",
      "Epoch 23/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2125 - acc: 0.9514 - val_loss: 0.8741 - val_acc: 0.7717\n",
      "Epoch 24/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2064 - acc: 0.9525 - val_loss: 0.8865 - val_acc: 0.7680\n",
      "Epoch 25/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2011 - acc: 0.9539 - val_loss: 0.8738 - val_acc: 0.7761\n",
      "Epoch 26/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.1918 - acc: 0.9581 - val_loss: 0.8909 - val_acc: 0.7665\n",
      "Epoch 27/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.1879 - acc: 0.9598 - val_loss: 0.8835 - val_acc: 0.7731\n",
      "Epoch 28/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.1706 - acc: 0.9647 - val_loss: 0.8714 - val_acc: 0.7812\n",
      "Epoch 29/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.1658 - acc: 0.9676 - val_loss: 0.8745 - val_acc: 0.7783\n",
      "Epoch 30/300\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.1651 - acc: 0.9672 - val_loss: 0.8749 - val_acc: 0.7775\n",
      "Epoch 31/300\n",
      "8111/8111 [==============================] - 45s 6ms/step - loss: 0.1657 - acc: 0.9652 - val_loss: 0.8748 - val_acc: 0.7797\n",
      "9473/9473 [==============================] - 27s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8112, 256, 431, 1) (8112, 41)\n",
      "val shape:  (1361, 256, 431, 1) (1361, 41)\n",
      "Fold:  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 8112 samples, validate on 1361 samples\n",
      "Epoch 1/300\n",
      "8112/8112 [==============================] - 48s 6ms/step - loss: 2.9329 - acc: 0.1897 - val_loss: 4.0750 - val_acc: 0.1132\n",
      "\n",
      "Validation loss decreased from inf to 4.075018326558233, saving model\n",
      "Epoch 2/300\n",
      "8112/8112 [==============================] - 44s 5ms/step - loss: 2.4383 - acc: 0.3183 - val_loss: 2.8203 - val_acc: 0.2682\n",
      "\n",
      "Validation loss decreased from 4.075018326558233 to 2.8202702972838143, saving model\n",
      "Epoch 3/300\n",
      "8112/8112 [==============================] - 44s 5ms/step - loss: 2.0302 - acc: 0.4237 - val_loss: 2.8535 - val_acc: 0.3270\n",
      "Epoch 4/300\n",
      "8112/8112 [==============================] - 45s 5ms/step - loss: 1.7186 - acc: 0.5099 - val_loss: 2.8868 - val_acc: 0.3181\n",
      "Epoch 5/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 1.5018 - acc: 0.5761 - val_loss: 2.1430 - val_acc: 0.4526\n",
      "\n",
      "Validation loss decreased from 2.8202702972838143 to 2.1430226736958633, saving model\n",
      "Epoch 6/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8112/8112 [==============================] - 44s 5ms/step - loss: 1.3326 - acc: 0.6255 - val_loss: 2.0759 - val_acc: 0.4761\n",
      "\n",
      "Validation loss decreased from 2.1430226736958633 to 2.075886449946979, saving model\n",
      "Epoch 7/300\n",
      "8112/8112 [==============================] - 45s 5ms/step - loss: 1.1948 - acc: 0.6691 - val_loss: 1.8394 - val_acc: 0.5364\n",
      "\n",
      "Validation loss decreased from 2.075886449946979 to 1.8394100101969857, saving model\n",
      "Epoch 8/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 1.0828 - acc: 0.7023 - val_loss: 1.8321 - val_acc: 0.5415\n",
      "\n",
      "Validation loss decreased from 1.8394100101969857 to 1.8320850184697774, saving model\n",
      "Epoch 9/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.9977 - acc: 0.7173 - val_loss: 1.5925 - val_acc: 0.6047\n",
      "\n",
      "Validation loss decreased from 1.8320850184697774 to 1.592461321741058, saving model\n",
      "Epoch 10/300\n",
      "8112/8112 [==============================] - 46s 6ms/step - loss: 0.8827 - acc: 0.7493 - val_loss: 1.5312 - val_acc: 0.6076\n",
      "\n",
      "Validation loss decreased from 1.592461321741058 to 1.5311667544275938, saving model\n",
      "Epoch 11/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.8335 - acc: 0.7706 - val_loss: 1.2711 - val_acc: 0.6730\n",
      "\n",
      "Validation loss decreased from 1.5311667544275938 to 1.2710677104167951, saving model\n",
      "Epoch 12/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.7406 - acc: 0.7862 - val_loss: 1.7124 - val_acc: 0.5922\n",
      "Epoch 13/300\n",
      "8112/8112 [==============================] - 44s 5ms/step - loss: 0.6871 - acc: 0.8061 - val_loss: 1.3666 - val_acc: 0.6458\n",
      "Epoch 14/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.6695 - acc: 0.8062 - val_loss: 1.5517 - val_acc: 0.6157\n",
      "Epoch 15/300\n",
      "8112/8112 [==============================] - 44s 5ms/step - loss: 0.5876 - acc: 0.8310 - val_loss: 1.3105 - val_acc: 0.6613\n",
      "Epoch 16/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.5347 - acc: 0.8432 - val_loss: 1.9874 - val_acc: 0.5584\n",
      "Epoch 17/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.4901 - acc: 0.8580 - val_loss: 1.2397 - val_acc: 0.6965\n",
      "\n",
      "Validation loss decreased from 1.2710677104167951 to 1.2397375239071646, saving model\n",
      "Epoch 18/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.4484 - acc: 0.8704 - val_loss: 1.7440 - val_acc: 0.6238\n",
      "Epoch 19/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.4019 - acc: 0.8840 - val_loss: 1.3994 - val_acc: 0.6965\n",
      "Epoch 20/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.3639 - acc: 0.8932 - val_loss: 1.0741 - val_acc: 0.7583\n",
      "\n",
      "Validation loss decreased from 1.2397375239071646 to 1.074134856309197, saving model\n",
      "Epoch 21/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.3381 - acc: 0.8953 - val_loss: 1.0197 - val_acc: 0.7509\n",
      "\n",
      "Validation loss decreased from 1.074134856309197 to 1.0196874822263415, saving model\n",
      "Epoch 22/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.2994 - acc: 0.9149 - val_loss: 1.2470 - val_acc: 0.6958\n",
      "Epoch 23/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.2113 - acc: 0.9384 - val_loss: 1.2590 - val_acc: 0.7362\n",
      "Epoch 26/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.2250 - acc: 0.9306 - val_loss: 1.3354 - val_acc: 0.7010\n",
      "Epoch 27/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.1802 - acc: 0.9472 - val_loss: 1.6811 - val_acc: 0.6730\n",
      "Epoch 28/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0953 - acc: 0.9794 - val_loss: 0.8034 - val_acc: 0.8207\n",
      "\n",
      "Validation loss decreased from 1.0196874822263415 to 0.8033517592974571, saving model\n",
      "Epoch 29/300\n",
      "8112/8112 [==============================] - 45s 5ms/step - loss: 0.0650 - acc: 0.9895 - val_loss: 0.7961 - val_acc: 0.8229\n",
      "\n",
      "Validation loss decreased from 0.8033517592974571 to 0.7961462074829146, saving model\n",
      "Epoch 30/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0590 - acc: 0.9920 - val_loss: 0.8012 - val_acc: 0.8288\n",
      "Epoch 31/300\n",
      "8112/8112 [==============================] - 44s 5ms/step - loss: 0.0555 - acc: 0.9931 - val_loss: 0.8074 - val_acc: 0.8237\n",
      "Epoch 32/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0515 - acc: 0.9940 - val_loss: 0.8070 - val_acc: 0.8295\n",
      "Epoch 33/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0471 - acc: 0.9937 - val_loss: 0.8059 - val_acc: 0.8266\n",
      "Epoch 34/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0436 - acc: 0.9957 - val_loss: 0.7989 - val_acc: 0.8222\n",
      "Epoch 35/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0450 - acc: 0.9947 - val_loss: 0.8179 - val_acc: 0.8237\n",
      "Epoch 36/300\n",
      "8112/8112 [==============================] - 46s 6ms/step - loss: 0.0414 - acc: 0.9967 - val_loss: 0.8085 - val_acc: 0.8251\n",
      "Epoch 37/300\n",
      "8112/8112 [==============================] - 46s 6ms/step - loss: 0.0418 - acc: 0.9957 - val_loss: 0.8081 - val_acc: 0.8273\n",
      "Epoch 38/300\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0410 - acc: 0.9964 - val_loss: 0.8093 - val_acc: 0.8281\n",
      "Epoch 39/300\n",
      "8112/8112 [==============================] - 45s 5ms/step - loss: 0.0393 - acc: 0.9978 - val_loss: 0.8078 - val_acc: 0.8288\n",
      "9473/9473 [==============================] - 27s 3ms/step\n",
      "9400/9400 [==============================] - 26s 3ms/step\n",
      "train shape:  (8115, 256, 431, 1) (8115, 41)\n",
      "val shape:  (1358, 256, 431, 1) (1358, 41)\n",
      "Fold:  3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8115 samples, validate on 1358 samples\n",
      "Epoch 1/300\n",
      "8115/8115 [==============================] - 49s 6ms/step - loss: 2.9465 - acc: 0.2067 - val_loss: 4.7848 - val_acc: 0.1119\n",
      "\n",
      "Validation loss decreased from inf to 4.784766863652989, saving model\n",
      "Epoch 2/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 2.3690 - acc: 0.3373 - val_loss: 3.2228 - val_acc: 0.2121\n",
      "\n",
      "Validation loss decreased from 4.784766863652989 to 3.2227633540163336, saving model\n",
      "Epoch 3/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 1.9101 - acc: 0.4657 - val_loss: 2.7766 - val_acc: 0.2946\n",
      "\n",
      "Validation loss decreased from 3.2227633540163336 to 2.7766234607865075, saving model\n",
      "Epoch 4/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 1.6489 - acc: 0.5326 - val_loss: 3.1918 - val_acc: 0.2673\n",
      "Epoch 5/300\n",
      "8115/8115 [==============================] - 45s 5ms/step - loss: 1.4325 - acc: 0.6000 - val_loss: 2.4393 - val_acc: 0.4293\n",
      "\n",
      "Validation loss decreased from 2.7766234607865075 to 2.4392727386969004, saving model\n",
      "Epoch 6/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 1.2781 - acc: 0.6442 - val_loss: 1.7062 - val_acc: 0.5376\n",
      "\n",
      "Validation loss decreased from 2.4392727386969004 to 1.7061841682762742, saving model\n",
      "Epoch 7/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 1.1391 - acc: 0.6815 - val_loss: 2.4852 - val_acc: 0.4094\n",
      "Epoch 8/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 1.0385 - acc: 0.7057 - val_loss: 1.6385 - val_acc: 0.5854\n",
      "\n",
      "Validation loss decreased from 1.7061841682762742 to 1.638481173845384, saving model\n",
      "Epoch 9/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.9389 - acc: 0.7351 - val_loss: 2.2909 - val_acc: 0.4786\n",
      "Epoch 10/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.8658 - acc: 0.7588 - val_loss: 1.6395 - val_acc: 0.5891\n",
      "Epoch 11/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.7940 - acc: 0.7771 - val_loss: 1.2191 - val_acc: 0.6819\n",
      "\n",
      "Validation loss decreased from 1.638481173845384 to 1.2191354650812052, saving model\n",
      "Epoch 12/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.7333 - acc: 0.7877 - val_loss: 1.3616 - val_acc: 0.6568\n",
      "Epoch 13/300\n",
      "8115/8115 [==============================] - 45s 5ms/step - loss: 0.6800 - acc: 0.8070 - val_loss: 1.4985 - val_acc: 0.6215\n",
      "Epoch 14/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.6269 - acc: 0.8213 - val_loss: 1.5415 - val_acc: 0.6605\n",
      "Epoch 15/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.5619 - acc: 0.8357 - val_loss: 1.4403 - val_acc: 0.6723\n",
      "Epoch 16/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.5172 - acc: 0.8513 - val_loss: 2.2280 - val_acc: 0.5552\n",
      "Epoch 17/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.4690 - acc: 0.8579 - val_loss: 1.5504 - val_acc: 0.6414\n",
      "Epoch 18/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.3036 - acc: 0.9206 - val_loss: 0.8310 - val_acc: 0.7761\n",
      "\n",
      "Validation loss decreased from 1.2191354650812052 to 0.8310241866006416, saving model\n",
      "Epoch 19/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.2496 - acc: 0.9404 - val_loss: 0.8476 - val_acc: 0.7761\n",
      "Epoch 20/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.2285 - acc: 0.9459 - val_loss: 0.8465 - val_acc: 0.7820\n",
      "Epoch 21/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.2209 - acc: 0.9478 - val_loss: 0.8099 - val_acc: 0.7894\n",
      "\n",
      "Validation loss decreased from 0.8310241866006416 to 0.8098772256820297, saving model\n",
      "Epoch 22/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.2059 - acc: 0.9516 - val_loss: 0.8094 - val_acc: 0.7916\n",
      "\n",
      "Validation loss decreased from 0.8098772256820297 to 0.809417827315464, saving model\n",
      "Epoch 23/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.1990 - acc: 0.9548 - val_loss: 0.8194 - val_acc: 0.7946\n",
      "Epoch 24/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.1898 - acc: 0.9581 - val_loss: 0.8221 - val_acc: 0.7982\n",
      "Epoch 25/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.1819 - acc: 0.9583 - val_loss: 0.8106 - val_acc: 0.7931\n",
      "Epoch 26/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.1744 - acc: 0.9618 - val_loss: 0.8290 - val_acc: 0.7916\n",
      "Epoch 27/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.1677 - acc: 0.9635 - val_loss: 0.8396 - val_acc: 0.7872\n",
      "Epoch 28/300\n",
      "8115/8115 [==============================] - 45s 5ms/step - loss: 0.1593 - acc: 0.9666 - val_loss: 0.8655 - val_acc: 0.7813\n",
      "Epoch 29/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.1494 - acc: 0.9694 - val_loss: 0.8295 - val_acc: 0.7968\n",
      "Epoch 30/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.1457 - acc: 0.9725 - val_loss: 0.8271 - val_acc: 0.7982\n",
      "Epoch 31/300\n",
      "8115/8115 [==============================] - 44s 5ms/step - loss: 0.1417 - acc: 0.9725 - val_loss: 0.8267 - val_acc: 0.7997\n",
      "Epoch 32/300\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.1434 - acc: 0.9718 - val_loss: 0.8262 - val_acc: 0.7990\n",
      "9473/9473 [==============================] - 25s 3ms/step\n",
      "9400/9400 [==============================] - 25s 3ms/step\n",
      "train shape:  (8121, 256, 431, 1) (8121, 41)\n",
      "val shape:  (1352, 256, 431, 1) (1352, 41)\n",
      "Fold:  4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8121 samples, validate on 1352 samples\n",
      "Epoch 1/300\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 2.8728 - acc: 0.2177 - val_loss: 5.7302 - val_acc: 0.1013\n",
      "\n",
      "Validation loss decreased from inf to 5.730176327496591, saving model\n",
      "Epoch 2/300\n",
      "8121/8121 [==============================] - 46s 6ms/step - loss: 2.2222 - acc: 0.3761 - val_loss: 2.4785 - val_acc: 0.3291\n",
      "\n",
      "Validation loss decreased from 5.730176327496591 to 2.4785092088597764, saving model\n",
      "Epoch 3/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 1.8349 - acc: 0.4834 - val_loss: 3.1429 - val_acc: 0.2936\n",
      "Epoch 4/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 1.5716 - acc: 0.5581 - val_loss: 1.9853 - val_acc: 0.4667\n",
      "\n",
      "Validation loss decreased from 2.4785092088597764 to 1.985259339654234, saving model\n",
      "Epoch 5/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 1.3678 - acc: 0.6193 - val_loss: 1.5234 - val_acc: 0.6021\n",
      "\n",
      "Validation loss decreased from 1.985259339654234 to 1.5234238381921892, saving model\n",
      "Epoch 6/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 1.2233 - acc: 0.6599 - val_loss: 1.9977 - val_acc: 0.4963\n",
      "Epoch 7/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 1.1029 - acc: 0.6907 - val_loss: 2.4242 - val_acc: 0.4393\n",
      "Epoch 8/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.9989 - acc: 0.7196 - val_loss: 3.6137 - val_acc: 0.3462\n",
      "Epoch 9/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.9053 - acc: 0.7426 - val_loss: 1.3602 - val_acc: 0.6516\n",
      "\n",
      "Validation loss decreased from 1.5234238381921892 to 1.360217403378007, saving model\n",
      "Epoch 10/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.8216 - acc: 0.7691 - val_loss: 1.8673 - val_acc: 0.5732\n",
      "Epoch 11/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.7503 - acc: 0.7857 - val_loss: 1.5656 - val_acc: 0.5828\n",
      "Epoch 12/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.6826 - acc: 0.8068 - val_loss: 1.4265 - val_acc: 0.6398\n",
      "Epoch 13/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.6217 - acc: 0.8165 - val_loss: 1.2784 - val_acc: 0.6716\n",
      "\n",
      "Validation loss decreased from 1.360217403378007 to 1.2784422586655475, saving model\n",
      "Epoch 14/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.5578 - acc: 0.8429 - val_loss: 1.5804 - val_acc: 0.6331\n",
      "Epoch 15/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.5461 - acc: 0.8420 - val_loss: 2.2824 - val_acc: 0.5725\n",
      "Epoch 16/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.4989 - acc: 0.8549 - val_loss: 1.1512 - val_acc: 0.7189\n",
      "\n",
      "Validation loss decreased from 1.2784422586655475 to 1.1511600497206287, saving model\n",
      "Epoch 17/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.4304 - acc: 0.8714 - val_loss: 1.2942 - val_acc: 0.7034\n",
      "Epoch 18/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.3939 - acc: 0.8830 - val_loss: 1.5665 - val_acc: 0.6457\n",
      "Epoch 19/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.3795 - acc: 0.8852 - val_loss: 1.1002 - val_acc: 0.7382\n",
      "\n",
      "Validation loss decreased from 1.1511600497206287 to 1.1002345071036435, saving model\n",
      "Epoch 20/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.3177 - acc: 0.9070 - val_loss: 1.5264 - val_acc: 0.6731\n",
      "Epoch 21/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.2895 - acc: 0.9137 - val_loss: 1.4705 - val_acc: 0.6783\n",
      "Epoch 22/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.2638 - acc: 0.9248 - val_loss: 1.4897 - val_acc: 0.6930\n",
      "Epoch 23/300\n",
      "8121/8121 [==============================] - 44s 5ms/step - loss: 0.2292 - acc: 0.9320 - val_loss: 2.0305 - val_acc: 0.6472\n",
      "Epoch 24/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.1944 - acc: 0.9431 - val_loss: 1.2302 - val_acc: 0.7419\n",
      "Epoch 25/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.2190 - acc: 0.9361 - val_loss: 1.6359 - val_acc: 0.6672\n",
      "Epoch 26/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.1063 - acc: 0.9741 - val_loss: 0.7910 - val_acc: 0.8158\n",
      "\n",
      "Validation loss decreased from 1.1002345071036435 to 0.791000278038386, saving model\n",
      "Epoch 27/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.0706 - acc: 0.9879 - val_loss: 0.7894 - val_acc: 0.8092\n",
      "\n",
      "Validation loss decreased from 0.791000278038386 to 0.7894082534948045, saving model\n",
      "Epoch 28/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.0624 - acc: 0.9913 - val_loss: 0.7815 - val_acc: 0.8107\n",
      "\n",
      "Validation loss decreased from 0.7894082534948045 to 0.781532592321994, saving model\n",
      "Epoch 29/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.0574 - acc: 0.9913 - val_loss: 0.7980 - val_acc: 0.8129\n",
      "Epoch 30/300\n",
      "8121/8121 [==============================] - 43s 5ms/step - loss: 0.0520 - acc: 0.9931 - val_loss: 0.7977 - val_acc: 0.8077\n",
      "Epoch 31/300\n",
      "8121/8121 [==============================] - 45s 5ms/step - loss: 0.0487 - acc: 0.9940 - val_loss: 0.7980 - val_acc: 0.8084\n",
      "Epoch 32/300\n",
      "8121/8121 [==============================] - 45s 5ms/step - loss: 0.0466 - acc: 0.9951 - val_loss: 0.7986 - val_acc: 0.8107\n",
      "Epoch 33/300\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0430 - acc: 0.9956 - val_loss: 0.8049 - val_acc: 0.8077\n",
      "Epoch 34/300\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0401 - acc: 0.9966 - val_loss: 0.8061 - val_acc: 0.8107\n",
      "Epoch 35/300\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0389 - acc: 0.9964 - val_loss: 0.8035 - val_acc: 0.8084\n",
      "Epoch 36/300\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0392 - acc: 0.9958 - val_loss: 0.8035 - val_acc: 0.8077\n",
      "Epoch 37/300\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0379 - acc: 0.9967 - val_loss: 0.8040 - val_acc: 0.8077\n",
      "Epoch 38/300\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0386 - acc: 0.9963 - val_loss: 0.8045 - val_acc: 0.8070\n",
      "9473/9473 [==============================] - 26s 3ms/step\n",
      "9400/9400 [==============================] - 25s 3ms/step\n",
      "train shape:  (8125, 256, 431, 1) (8125, 41)\n",
      "val shape:  (1348, 256, 431, 1) (1348, 41)\n",
      "Fold:  5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8125 samples, validate on 1348 samples\n",
      "Epoch 1/300\n",
      "8125/8125 [==============================] - 48s 6ms/step - loss: 2.9274 - acc: 0.1985 - val_loss: 3.9202 - val_acc: 0.1142\n",
      "\n",
      "Validation loss decreased from inf to 3.920248627308922, saving model\n",
      "Epoch 2/300\n",
      "8125/8125 [==============================] - 45s 5ms/step - loss: 2.3814 - acc: 0.3328 - val_loss: 7.5827 - val_acc: 0.1001\n",
      "Epoch 3/300\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 1.9537 - acc: 0.4428 - val_loss: 3.2713 - val_acc: 0.2634\n",
      "\n",
      "Validation loss decreased from 3.920248627308922 to 3.2712998814681873, saving model\n",
      "Epoch 4/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 1.6383 - acc: 0.5333 - val_loss: 2.3672 - val_acc: 0.3902\n",
      "\n",
      "Validation loss decreased from 3.2712998814681873 to 2.3671821769691714, saving model\n",
      "Epoch 5/300\n",
      "8125/8125 [==============================] - 45s 5ms/step - loss: 1.4418 - acc: 0.5903 - val_loss: 2.3172 - val_acc: 0.4332\n",
      "\n",
      "Validation loss decreased from 2.3671821769691714 to 2.3171965524067866, saving model\n",
      "Epoch 6/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 1.2762 - acc: 0.6370 - val_loss: 2.6349 - val_acc: 0.4162\n",
      "Epoch 7/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 1.1499 - acc: 0.6710 - val_loss: 1.5602 - val_acc: 0.5853\n",
      "\n",
      "Validation loss decreased from 2.3171965524067866 to 1.5602062643458652, saving model\n",
      "Epoch 8/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 1.0603 - acc: 0.7018 - val_loss: 1.7881 - val_acc: 0.5371\n",
      "Epoch 9/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.9496 - acc: 0.7329 - val_loss: 1.4815 - val_acc: 0.6039\n",
      "\n",
      "Validation loss decreased from 1.5602062643458652 to 1.4814677259688562, saving model\n",
      "Epoch 10/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.8523 - acc: 0.7521 - val_loss: 1.4387 - val_acc: 0.6083\n",
      "\n",
      "Validation loss decreased from 1.4814677259688562 to 1.4387312455417849, saving model\n",
      "Epoch 11/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.7944 - acc: 0.7724 - val_loss: 1.2171 - val_acc: 0.6907\n",
      "\n",
      "Validation loss decreased from 1.4387312455417849 to 1.2171408077729562, saving model\n",
      "Epoch 12/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.7188 - acc: 0.7946 - val_loss: 1.8044 - val_acc: 0.5556\n",
      "Epoch 13/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.6571 - acc: 0.8096 - val_loss: 1.6316 - val_acc: 0.5957\n",
      "Epoch 14/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.6234 - acc: 0.8208 - val_loss: 2.1941 - val_acc: 0.5319\n",
      "Epoch 15/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.5617 - acc: 0.8404 - val_loss: 1.7158 - val_acc: 0.6217\n",
      "Epoch 16/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.5173 - acc: 0.8506 - val_loss: 1.7294 - val_acc: 0.5556\n",
      "Epoch 17/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.4596 - acc: 0.8651 - val_loss: 1.4608 - val_acc: 0.6691\n",
      "Epoch 18/300\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 0.2949 - acc: 0.9201 - val_loss: 0.8906 - val_acc: 0.7826\n",
      "\n",
      "Validation loss decreased from 1.2171408077729562 to 0.8906180264334297, saving model\n",
      "Epoch 19/300\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 0.2462 - acc: 0.9393 - val_loss: 0.8635 - val_acc: 0.7871\n",
      "\n",
      "Validation loss decreased from 0.8906180264334297 to 0.8635350972502451, saving model\n",
      "Epoch 20/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.2244 - acc: 0.9454 - val_loss: 0.8841 - val_acc: 0.7812\n",
      "Epoch 21/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.2075 - acc: 0.9536 - val_loss: 0.8674 - val_acc: 0.7886\n",
      "Epoch 22/300\n",
      "8125/8125 [==============================] - 45s 5ms/step - loss: 0.1980 - acc: 0.9558 - val_loss: 0.8904 - val_acc: 0.7841\n",
      "Epoch 23/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.1935 - acc: 0.9574 - val_loss: 0.8754 - val_acc: 0.7834\n",
      "Epoch 24/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.1807 - acc: 0.9614 - val_loss: 0.8672 - val_acc: 0.7901\n",
      "Epoch 25/300\n",
      "8125/8125 [==============================] - 45s 5ms/step - loss: 0.1744 - acc: 0.9622 - val_loss: 0.9055 - val_acc: 0.7797\n",
      "Epoch 26/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.1626 - acc: 0.9668 - val_loss: 0.8807 - val_acc: 0.7871\n",
      "Epoch 27/300\n",
      "8125/8125 [==============================] - 44s 5ms/step - loss: 0.1577 - acc: 0.9684 - val_loss: 0.8767 - val_acc: 0.7878\n",
      "Epoch 28/300\n",
      "8125/8125 [==============================] - 45s 5ms/step - loss: 0.1574 - acc: 0.9669 - val_loss: 0.8788 - val_acc: 0.7864\n",
      "Epoch 29/300\n",
      "8125/8125 [==============================] - 45s 6ms/step - loss: 0.1545 - acc: 0.9685 - val_loss: 0.8773 - val_acc: 0.7923\n",
      "9473/9473 [==============================] - 26s 3ms/step\n",
      "9400/9400 [==============================] - 26s 3ms/step\n",
      "train shape:  (8146, 256, 431, 1) (8146, 41)\n",
      "val shape:  (1327, 256, 431, 1) (1327, 41)\n",
      "Fold:  6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_8 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8146 samples, validate on 1327 samples\n",
      "Epoch 1/300\n",
      "8146/8146 [==============================] - 47s 6ms/step - loss: 2.9312 - acc: 0.2098 - val_loss: 5.8837 - val_acc: 0.0686\n",
      "\n",
      "Validation loss decreased from inf to 5.883746755509423, saving model\n",
      "Epoch 2/300\n",
      "8146/8146 [==============================] - 45s 5ms/step - loss: 2.3033 - acc: 0.3499 - val_loss: 2.8950 - val_acc: 0.2690\n",
      "\n",
      "Validation loss decreased from 5.883746755509423 to 2.8949762623204087, saving model\n",
      "Epoch 3/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 1.9376 - acc: 0.4494 - val_loss: 2.5051 - val_acc: 0.3210\n",
      "\n",
      "Validation loss decreased from 2.8949762623204087 to 2.505142078780553, saving model\n",
      "Epoch 4/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 1.6946 - acc: 0.5182 - val_loss: 3.5836 - val_acc: 0.2185\n",
      "Epoch 5/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 1.4717 - acc: 0.5811 - val_loss: 1.8077 - val_acc: 0.5041\n",
      "\n",
      "Validation loss decreased from 2.505142078780553 to 1.8076874947494013, saving model\n",
      "Epoch 6/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 1.3181 - acc: 0.6266 - val_loss: 1.5174 - val_acc: 0.5863\n",
      "\n",
      "Validation loss decreased from 1.8076874947494013 to 1.517361645601527, saving model\n",
      "Epoch 7/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 1.1751 - acc: 0.6705 - val_loss: 2.0730 - val_acc: 0.5034\n",
      "Epoch 8/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 1.0390 - acc: 0.7147 - val_loss: 1.5064 - val_acc: 0.6194\n",
      "\n",
      "Validation loss decreased from 1.517361645601527 to 1.5063562401273507, saving model\n",
      "Epoch 9/300\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.9735 - acc: 0.7192 - val_loss: 1.3749 - val_acc: 0.6413\n",
      "\n",
      "Validation loss decreased from 1.5063562401273507 to 1.3748614292173522, saving model\n",
      "Epoch 10/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.7319 - acc: 0.7987 - val_loss: 2.0869 - val_acc: 0.5252\n",
      "Epoch 13/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.6797 - acc: 0.8033 - val_loss: 1.2501 - val_acc: 0.6797\n",
      "\n",
      "Validation loss decreased from 1.2604577995909725 to 1.2501407837454501, saving model\n",
      "Epoch 14/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.6309 - acc: 0.8137 - val_loss: 1.3639 - val_acc: 0.6737\n",
      "Epoch 15/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.5579 - acc: 0.8355 - val_loss: 1.8292 - val_acc: 0.6157\n",
      "Epoch 16/300\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.5257 - acc: 0.8484 - val_loss: 1.2213 - val_acc: 0.6858\n",
      "\n",
      "Validation loss decreased from 1.2501407837454501 to 1.2213207538988506, saving model\n",
      "Epoch 17/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.5000 - acc: 0.8529 - val_loss: 1.8855 - val_acc: 0.5953\n",
      "Epoch 18/300\n",
      "8146/8146 [==============================] - 45s 5ms/step - loss: 0.4604 - acc: 0.8614 - val_loss: 1.6179 - val_acc: 0.6963\n",
      "Epoch 19/300\n",
      "8146/8146 [==============================] - 44s 5ms/step - loss: 0.4182 - acc: 0.8750 - val_loss: 1.2160 - val_acc: 0.7249\n",
      "\n",
      "Validation loss decreased from 1.2213207538988506 to 1.2160105547153959, saving model\n",
      "Epoch 20/300\n",
      "8146/8146 [==============================] - 45s 5ms/step - loss: 0.3439 - acc: 0.8987 - val_loss: 1.7342 - val_acc: 0.6345\n",
      "Epoch 21/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.3105 - acc: 0.9088 - val_loss: 1.2021 - val_acc: 0.7362\n",
      "\n",
      "Validation loss decreased from 1.2160105547153959 to 1.2020750784209193, saving model\n",
      "Epoch 22/300\n",
      "8146/8146 [==============================] - 45s 5ms/step - loss: 0.2914 - acc: 0.9160 - val_loss: 1.2846 - val_acc: 0.7182\n",
      "Epoch 23/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.2533 - acc: 0.9256 - val_loss: 1.2567 - val_acc: 0.7347\n",
      "Epoch 24/300\n",
      "8146/8146 [==============================] - 45s 5ms/step - loss: 0.2440 - acc: 0.9268 - val_loss: 1.6415 - val_acc: 0.6647\n",
      "Epoch 25/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.2386 - acc: 0.9287 - val_loss: 1.3289 - val_acc: 0.7219\n",
      "Epoch 26/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.1948 - acc: 0.9429 - val_loss: 1.3104 - val_acc: 0.7460\n",
      "Epoch 27/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.1901 - acc: 0.9390 - val_loss: 1.4836 - val_acc: 0.7136\n",
      "Epoch 28/300\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0975 - acc: 0.9766 - val_loss: 0.8750 - val_acc: 0.8003\n",
      "\n",
      "Validation loss decreased from 1.2020750784209193 to 0.8750301053313807, saving model\n",
      "Epoch 29/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.0624 - acc: 0.9902 - val_loss: 0.8575 - val_acc: 0.8056\n",
      "\n",
      "Validation loss decreased from 0.8750301053313807 to 0.8575478714035253, saving model\n",
      "Epoch 30/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.0571 - acc: 0.9912 - val_loss: 0.8508 - val_acc: 0.8026\n",
      "\n",
      "Validation loss decreased from 0.8575478714035253 to 0.850798665174615, saving model\n",
      "Epoch 31/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.0500 - acc: 0.9925 - val_loss: 0.8555 - val_acc: 0.8093\n",
      "Epoch 32/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.0480 - acc: 0.9931 - val_loss: 0.8623 - val_acc: 0.8056\n",
      "Epoch 33/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.0439 - acc: 0.9955 - val_loss: 0.8557 - val_acc: 0.8078\n",
      "Epoch 34/300\n",
      "8146/8146 [==============================] - 45s 5ms/step - loss: 0.0424 - acc: 0.9952 - val_loss: 0.8645 - val_acc: 0.8048\n",
      "Epoch 35/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.0412 - acc: 0.9953 - val_loss: 0.8729 - val_acc: 0.8003\n",
      "Epoch 36/300\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0382 - acc: 0.9959 - val_loss: 0.8706 - val_acc: 0.8071\n",
      "Epoch 37/300\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0368 - acc: 0.9968 - val_loss: 0.8727 - val_acc: 0.8063\n",
      "Epoch 38/300\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0361 - acc: 0.9966 - val_loss: 0.8746 - val_acc: 0.8071\n",
      "Epoch 39/300\n",
      "8146/8146 [==============================] - 45s 6ms/step - loss: 0.0338 - acc: 0.9967 - val_loss: 0.8742 - val_acc: 0.8063\n",
      "Epoch 40/300\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0367 - acc: 0.9968 - val_loss: 0.8737 - val_acc: 0.8078\n",
      "9473/9473 [==============================] - 26s 3ms/step\n",
      "9400/9400 [==============================] - 25s 3ms/step\n",
      "CPU times: user 1h 47min 28s, sys: 1h 8min 18s, total: 2h 55min 46s\n",
      "Wall time: 3h 14min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# log mel sp 44100x5\n",
    "            \n",
    "config = Config(sampling_rate=44100, audio_duration=5, n_classes=41, use_log_mel_sp=True, n_folds=7, max_epochs=300, n_mfcc=128*2)\n",
    "PREDICTION_FOLDER = \"predictions_log_mel_sp_44000x5\"\n",
    "CHECKPOINT_FOLDER = 'checkpionts_log_mel_sp_44000x5'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "#     y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "    X = X_train[train_split]\n",
    "    y = y_train[train_split]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model(config)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    \n",
    "#     adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6, amsgrad=True)\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                        optimizer=adam,\n",
    "#                          metrics=['acc'])\n",
    "    sgd = SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #     checkpoint = ModelCheckpoint(CHECKPOINT_FOLDER+'/best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    checkpoint = CustomModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks_list = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=[checkpoint], \n",
    "                        batch_size=batch_size, epochs=config.max_epochs, shuffle=True)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (8108, 256, 431, 1) (8108, 41)\n",
      "val shape:  (1365, 256, 431, 1) (1365, 41)\n",
      "Fold:  0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_9 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 8108 samples, validate on 1365 samples\n",
      "Epoch 1/10\n",
      "8108/8108 [==============================] - 50s 6ms/step - loss: 0.0423 - acc: 0.9952 - val_loss: 0.8994 - val_acc: 0.8315\n",
      "\n",
      "Validation loss decreased from inf to 0.8994396817553174, saving model\n",
      "Epoch 2/10\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0434 - acc: 0.9943 - val_loss: 0.8996 - val_acc: 0.8308\n",
      "Epoch 3/10\n",
      "8108/8108 [==============================] - 48s 6ms/step - loss: 0.0409 - acc: 0.9959 - val_loss: 0.8996 - val_acc: 0.8322\n",
      "Epoch 4/10\n",
      "8108/8108 [==============================] - 47s 6ms/step - loss: 0.0411 - acc: 0.9941 - val_loss: 0.8997 - val_acc: 0.8315\n",
      "Epoch 5/10\n",
      "8108/8108 [==============================] - 46s 6ms/step - loss: 0.0409 - acc: 0.9954 - val_loss: 0.9006 - val_acc: 0.8308\n",
      "Epoch 6/10\n",
      "8108/8108 [==============================] - 46s 6ms/step - loss: 0.0405 - acc: 0.9958 - val_loss: 0.9010 - val_acc: 0.8315\n",
      "Epoch 7/10\n",
      "8108/8108 [==============================] - 45s 6ms/step - loss: 0.0406 - acc: 0.9940 - val_loss: 0.9010 - val_acc: 0.8315\n",
      "Epoch 8/10\n",
      "8108/8108 [==============================] - 46s 6ms/step - loss: 0.0419 - acc: 0.9946 - val_loss: 0.9015 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "8108/8108 [==============================] - 46s 6ms/step - loss: 0.0427 - acc: 0.9948 - val_loss: 0.9008 - val_acc: 0.8300\n",
      "Epoch 10/10\n",
      "8108/8108 [==============================] - 46s 6ms/step - loss: 0.0410 - acc: 0.9951 - val_loss: 0.9003 - val_acc: 0.8315\n",
      "9473/9473 [==============================] - 28s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8111, 256, 431, 1) (8111, 41)\n",
      "val shape:  (1362, 256, 431, 1) (1362, 41)\n",
      "Fold:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_10  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8111 samples, validate on 1362 samples\n",
      "Epoch 1/10\n",
      "8111/8111 [==============================] - 49s 6ms/step - loss: 0.2169 - acc: 0.9502 - val_loss: 0.8602 - val_acc: 0.7680\n",
      "\n",
      "Validation loss decreased from inf to 0.860225069715413, saving model\n",
      "Epoch 2/10\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2193 - acc: 0.9495 - val_loss: 0.8610 - val_acc: 0.7687\n",
      "Epoch 3/10\n",
      "8111/8111 [==============================] - 47s 6ms/step - loss: 0.2163 - acc: 0.9525 - val_loss: 0.8601 - val_acc: 0.7680\n",
      "\n",
      "Validation loss decreased from 0.860225069715413 to 0.8601464948640171, saving model\n",
      "Epoch 4/10\n",
      "8111/8111 [==============================] - 47s 6ms/step - loss: 0.2147 - acc: 0.9520 - val_loss: 0.8607 - val_acc: 0.7673\n",
      "Epoch 5/10\n",
      "8111/8111 [==============================] - 47s 6ms/step - loss: 0.2191 - acc: 0.9506 - val_loss: 0.8603 - val_acc: 0.7680\n",
      "Epoch 6/10\n",
      "8111/8111 [==============================] - 47s 6ms/step - loss: 0.2144 - acc: 0.9522 - val_loss: 0.8604 - val_acc: 0.7695\n",
      "Epoch 7/10\n",
      "8111/8111 [==============================] - 47s 6ms/step - loss: 0.2161 - acc: 0.9515 - val_loss: 0.8599 - val_acc: 0.7687\n",
      "\n",
      "Validation loss decreased from 0.8601464948640171 to 0.8599245737302671, saving model\n",
      "Epoch 8/10\n",
      "8111/8111 [==============================] - 46s 6ms/step - loss: 0.2143 - acc: 0.9517 - val_loss: 0.8604 - val_acc: 0.7687\n",
      "Epoch 9/10\n",
      "8111/8111 [==============================] - 47s 6ms/step - loss: 0.2177 - acc: 0.9499 - val_loss: 0.8605 - val_acc: 0.7680\n",
      "Epoch 10/10\n",
      "8111/8111 [==============================] - 47s 6ms/step - loss: 0.2184 - acc: 0.9514 - val_loss: 0.8605 - val_acc: 0.7687\n",
      "9473/9473 [==============================] - 28s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8112, 256, 431, 1) (8112, 41)\n",
      "val shape:  (1361, 256, 431, 1) (1361, 41)\n",
      "Fold:  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_55 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_11  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 8112 samples, validate on 1361 samples\n",
      "Epoch 1/10\n",
      "8112/8112 [==============================] - 52s 6ms/step - loss: 0.0595 - acc: 0.9921 - val_loss: 0.8062 - val_acc: 0.8303\n",
      "\n",
      "Validation loss decreased from inf to 0.8061658630556784, saving model\n",
      "Epoch 2/10\n",
      "8112/8112 [==============================] - 45s 6ms/step - loss: 0.0579 - acc: 0.9927 - val_loss: 0.8063 - val_acc: 0.8288\n",
      "Epoch 3/10\n",
      "8112/8112 [==============================] - 48s 6ms/step - loss: 0.0590 - acc: 0.9924 - val_loss: 0.8056 - val_acc: 0.8303\n",
      "\n",
      "Validation loss decreased from 0.8061658630556784 to 0.8056182890232655, saving model\n",
      "Epoch 4/10\n",
      "8112/8112 [==============================] - 46s 6ms/step - loss: 0.0597 - acc: 0.9921 - val_loss: 0.8063 - val_acc: 0.8288\n",
      "Epoch 5/10\n",
      "8112/8112 [==============================] - 47s 6ms/step - loss: 0.0580 - acc: 0.9925 - val_loss: 0.8062 - val_acc: 0.8295\n",
      "Epoch 6/10\n",
      "8112/8112 [==============================] - 47s 6ms/step - loss: 0.0582 - acc: 0.9926 - val_loss: 0.8056 - val_acc: 0.8303\n",
      "\n",
      "Validation loss decreased from 0.8056182890232655 to 0.8056114179347036, saving model\n",
      "Epoch 7/10\n",
      "8112/8112 [==============================] - 46s 6ms/step - loss: 0.0597 - acc: 0.9924 - val_loss: 0.8057 - val_acc: 0.8303\n",
      "Epoch 8/10\n",
      "8112/8112 [==============================] - 47s 6ms/step - loss: 0.0599 - acc: 0.9924 - val_loss: 0.8055 - val_acc: 0.8310\n",
      "\n",
      "Validation loss decreased from 0.8056114179347036 to 0.8055264391415615, saving model\n",
      "Epoch 9/10\n",
      "8112/8112 [==============================] - 46s 6ms/step - loss: 0.0589 - acc: 0.9922 - val_loss: 0.8065 - val_acc: 0.8310\n",
      "Epoch 10/10\n",
      "8112/8112 [==============================] - 47s 6ms/step - loss: 0.0604 - acc: 0.9924 - val_loss: 0.8067 - val_acc: 0.8281\n",
      "9473/9473 [==============================] - 29s 3ms/step\n",
      "9400/9400 [==============================] - 28s 3ms/step\n",
      "train shape:  (8115, 256, 431, 1) (8115, 41)\n",
      "val shape:  (1358, 256, 431, 1) (1358, 41)\n",
      "Fold:  3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_56 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_12  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8115 samples, validate on 1358 samples\n",
      "Epoch 1/10\n",
      "8115/8115 [==============================] - 52s 6ms/step - loss: 0.1893 - acc: 0.9591 - val_loss: 0.8115 - val_acc: 0.7865\n",
      "\n",
      "Validation loss decreased from inf to 0.8114897305905732, saving model\n",
      "Epoch 2/10\n",
      "8115/8115 [==============================] - 45s 6ms/step - loss: 0.1893 - acc: 0.9577 - val_loss: 0.8119 - val_acc: 0.7887\n",
      "Epoch 3/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1897 - acc: 0.9602 - val_loss: 0.8117 - val_acc: 0.7894\n",
      "Epoch 4/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1921 - acc: 0.9560 - val_loss: 0.8119 - val_acc: 0.7887\n",
      "Epoch 5/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1928 - acc: 0.9558 - val_loss: 0.8118 - val_acc: 0.7887\n",
      "Epoch 6/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1875 - acc: 0.9596 - val_loss: 0.8119 - val_acc: 0.7872\n",
      "Epoch 7/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1912 - acc: 0.9576 - val_loss: 0.8118 - val_acc: 0.7879\n",
      "Epoch 8/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1895 - acc: 0.9583 - val_loss: 0.8122 - val_acc: 0.7865\n",
      "Epoch 9/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1884 - acc: 0.9585 - val_loss: 0.8123 - val_acc: 0.7872\n",
      "Epoch 10/10\n",
      "8115/8115 [==============================] - 47s 6ms/step - loss: 0.1919 - acc: 0.9582 - val_loss: 0.8119 - val_acc: 0.7887\n",
      "9473/9473 [==============================] - 29s 3ms/step\n",
      "9400/9400 [==============================] - 28s 3ms/step\n",
      "train shape:  (8121, 256, 431, 1) (8121, 41)\n",
      "val shape:  (1352, 256, 431, 1) (1352, 41)\n",
      "Fold:  4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_62 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_13  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 8121 samples, validate on 1352 samples\n",
      "Epoch 1/10\n",
      "8121/8121 [==============================] - 50s 6ms/step - loss: 0.0575 - acc: 0.9908 - val_loss: 0.7925 - val_acc: 0.8084\n",
      "\n",
      "Validation loss decreased from inf to 0.7925104918564565, saving model\n",
      "Epoch 2/10\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0540 - acc: 0.9935 - val_loss: 0.7919 - val_acc: 0.8092\n",
      "\n",
      "Validation loss decreased from 0.7925104918564565 to 0.7919338694690953, saving model\n",
      "Epoch 3/10\n",
      "8121/8121 [==============================] - 47s 6ms/step - loss: 0.0550 - acc: 0.9931 - val_loss: 0.7929 - val_acc: 0.8062\n",
      "Epoch 4/10\n",
      "8121/8121 [==============================] - 46s 6ms/step - loss: 0.0557 - acc: 0.9922 - val_loss: 0.7930 - val_acc: 0.8092\n",
      "Epoch 5/10\n",
      "8121/8121 [==============================] - 47s 6ms/step - loss: 0.0545 - acc: 0.9936 - val_loss: 0.7944 - val_acc: 0.8077\n",
      "Epoch 6/10\n",
      "8121/8121 [==============================] - 46s 6ms/step - loss: 0.0540 - acc: 0.9927 - val_loss: 0.7940 - val_acc: 0.8092\n",
      "Epoch 7/10\n",
      "8121/8121 [==============================] - 46s 6ms/step - loss: 0.0550 - acc: 0.9931 - val_loss: 0.7940 - val_acc: 0.8092\n",
      "Epoch 8/10\n",
      "8121/8121 [==============================] - 47s 6ms/step - loss: 0.0568 - acc: 0.9917 - val_loss: 0.7938 - val_acc: 0.8077\n",
      "Epoch 9/10\n",
      "8121/8121 [==============================] - 46s 6ms/step - loss: 0.0551 - acc: 0.9920 - val_loss: 0.7949 - val_acc: 0.8092\n",
      "Epoch 10/10\n",
      "8121/8121 [==============================] - 45s 6ms/step - loss: 0.0554 - acc: 0.9921 - val_loss: 0.7952 - val_acc: 0.8107\n",
      "9473/9473 [==============================] - 29s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8125, 256, 431, 1) (8125, 41)\n",
      "val shape:  (1348, 256, 431, 1) (1348, 41)\n",
      "Fold:  5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_67 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_68 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_14  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8125 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "8125/8125 [==============================] - 51s 6ms/step - loss: 0.2202 - acc: 0.9493 - val_loss: 0.8678 - val_acc: 0.7908\n",
      "\n",
      "Validation loss decreased from inf to 0.8677879699702079, saving model\n",
      "Epoch 2/10\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 0.2191 - acc: 0.9511 - val_loss: 0.8672 - val_acc: 0.7908\n",
      "\n",
      "Validation loss decreased from 0.8677879699702079 to 0.8671770688334275, saving model\n",
      "Epoch 3/10\n",
      "8125/8125 [==============================] - 47s 6ms/step - loss: 0.2197 - acc: 0.9497 - val_loss: 0.8675 - val_acc: 0.7908\n",
      "Epoch 4/10\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 0.2218 - acc: 0.9478 - val_loss: 0.8670 - val_acc: 0.7908\n",
      "\n",
      "Validation loss decreased from 0.8671770688334275 to 0.8670146671353178, saving model\n",
      "Epoch 5/10\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 0.2201 - acc: 0.9487 - val_loss: 0.8668 - val_acc: 0.7908\n",
      "\n",
      "Validation loss decreased from 0.8670146671353178 to 0.8667577261033327, saving model\n",
      "Epoch 6/10\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 0.2181 - acc: 0.9505 - val_loss: 0.8668 - val_acc: 0.7908\n",
      "Epoch 7/10\n",
      "8125/8125 [==============================] - 47s 6ms/step - loss: 0.2202 - acc: 0.9495 - val_loss: 0.8666 - val_acc: 0.7908\n",
      "\n",
      "Validation loss decreased from 0.8667577261033327 to 0.8666219865621374, saving model\n",
      "Epoch 8/10\n",
      "8125/8125 [==============================] - 47s 6ms/step - loss: 0.2233 - acc: 0.9479 - val_loss: 0.8669 - val_acc: 0.7908\n",
      "Epoch 9/10\n",
      "8125/8125 [==============================] - 46s 6ms/step - loss: 0.2200 - acc: 0.9477 - val_loss: 0.8674 - val_acc: 0.7908\n",
      "Epoch 10/10\n",
      "8125/8125 [==============================] - 47s 6ms/step - loss: 0.2200 - acc: 0.9493 - val_loss: 0.8673 - val_acc: 0.7908\n",
      "9473/9473 [==============================] - 29s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8146, 256, 431, 1) (8146, 41)\n",
      "val shape:  (1327, 256, 431, 1) (1327, 41)\n",
      "Fold:  6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_90 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_15  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 8146 samples, validate on 1327 samples\n",
      "Epoch 1/10\n",
      "8146/8146 [==============================] - 50s 6ms/step - loss: 0.0503 - acc: 0.9923 - val_loss: 0.8596 - val_acc: 0.8026\n",
      "\n",
      "Validation loss decreased from inf to 0.8596409401142606, saving model\n",
      "Epoch 2/10\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0493 - acc: 0.9937 - val_loss: 0.8592 - val_acc: 0.8041\n",
      "\n",
      "Validation loss decreased from 0.8596409401142606 to 0.8591954583230554, saving model\n",
      "Epoch 3/10\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0488 - acc: 0.9947 - val_loss: 0.8584 - val_acc: 0.8041\n",
      "\n",
      "Validation loss decreased from 0.8591954583230554 to 0.8583763486564654, saving model\n",
      "Epoch 4/10\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0496 - acc: 0.9947 - val_loss: 0.8583 - val_acc: 0.8041\n",
      "\n",
      "Validation loss decreased from 0.8583763486564654 to 0.858279547120003, saving model\n",
      "Epoch 5/10\n",
      "8146/8146 [==============================] - 47s 6ms/step - loss: 0.0497 - acc: 0.9945 - val_loss: 0.8592 - val_acc: 0.8041\n",
      "Epoch 6/10\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0482 - acc: 0.9936 - val_loss: 0.8602 - val_acc: 0.8041\n",
      "Epoch 7/10\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0478 - acc: 0.9939 - val_loss: 0.8591 - val_acc: 0.8041\n",
      "Epoch 8/10\n",
      "8146/8146 [==============================] - 47s 6ms/step - loss: 0.0482 - acc: 0.9952 - val_loss: 0.8595 - val_acc: 0.8041\n",
      "Epoch 9/10\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0483 - acc: 0.9939 - val_loss: 0.8603 - val_acc: 0.8041\n",
      "Epoch 10/10\n",
      "8146/8146 [==============================] - 46s 6ms/step - loss: 0.0507 - acc: 0.9948 - val_loss: 0.8588 - val_acc: 0.8033\n",
      "9473/9473 [==============================] - 27s 3ms/step\n",
      "9400/9400 [==============================] - 25s 3ms/step\n",
      "CPU times: user 33min 39s, sys: 23min 28s, total: 57min 8s\n",
      "Wall time: 1h 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# log_mel_sp_44100x5 fine tune \n",
    "config = Config(sampling_rate=44100, audio_duration=5, n_classes=41, use_log_mel_sp=True, n_folds=7, max_epochs=300, n_mfcc=128*2)\n",
    "PREDICTION_FOLDER = \"predictions_log_mel_sp_44000x5\"\n",
    "CHECKPOINT_FOLDER = 'checkpionts_log_mel_sp_44000x5'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "#     y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "    X = X_train[train_split]\n",
    "    y = y_train[train_split]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model(config)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    \n",
    "#     adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6, amsgrad=True)\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                        optimizer=adam,\n",
    "#                          metrics=['acc'])\n",
    "    sgd = SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #     checkpoint = ModelCheckpoint(CHECKPOINT_FOLDER+'/best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    checkpoint = CustomModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks_list = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=[checkpoint], \n",
    "                        batch_size=batch_size, epochs=10, shuffle=True)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (8111, 256, 431, 1) (8111, 41)\n",
      "val shape:  (1362, 256, 431, 1) (1362, 41)\n",
      "Fold:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "126/126 [==============================] - 40s 320ms/step - loss: 3.2568 - acc: 0.1461 - val_loss: 3.5960 - val_acc: 0.1131\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.59597, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 2/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 2.9262 - acc: 0.2458 - val_loss: 3.2863 - val_acc: 0.2423\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.59597 to 3.28632, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 3/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 2.6271 - acc: 0.3576 - val_loss: 2.2207 - val_acc: 0.3612\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.28632 to 2.22069, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 4/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 2.3994 - acc: 0.4358 - val_loss: 2.5335 - val_acc: 0.3781\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 2.2318 - acc: 0.4974 - val_loss: 1.7048 - val_acc: 0.5338\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.22069 to 1.70476, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 6/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 2.1071 - acc: 0.5419 - val_loss: 1.7791 - val_acc: 0.5081\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 2.0132 - acc: 0.5743 - val_loss: 1.5560 - val_acc: 0.5617\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.70476 to 1.55596, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 8/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.9087 - acc: 0.6163 - val_loss: 1.7341 - val_acc: 0.5228\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.8364 - acc: 0.6374 - val_loss: 1.5639 - val_acc: 0.5749\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.7357 - acc: 0.6755 - val_loss: 1.3658 - val_acc: 0.6278\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.55596 to 1.36579, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 11/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.7020 - acc: 0.6853 - val_loss: 1.3490 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.36579 to 1.34898, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 12/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.6338 - acc: 0.7087 - val_loss: 2.3718 - val_acc: 0.3979\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.6050 - acc: 0.7180 - val_loss: 1.2676 - val_acc: 0.6571\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.34898 to 1.26760, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 14/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.5610 - acc: 0.7191 - val_loss: 1.3747 - val_acc: 0.6336\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.5272 - acc: 0.7454 - val_loss: 1.2058 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.26760 to 1.20576, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 16/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.4849 - acc: 0.7500 - val_loss: 2.2344 - val_acc: 0.4611\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.4502 - acc: 0.7719 - val_loss: 1.2661 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3967 - acc: 0.7853 - val_loss: 1.3567 - val_acc: 0.6535\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.4002 - acc: 0.7845 - val_loss: 1.5042 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.3687 - acc: 0.7954 - val_loss: 1.1129 - val_acc: 0.7107\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.20576 to 1.11288, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 21/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 38s 303ms/step - loss: 1.3452 - acc: 0.8027 - val_loss: 1.2627 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.3240 - acc: 0.8116 - val_loss: 1.0699 - val_acc: 0.7276\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.11288 to 1.06991, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 23/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.2841 - acc: 0.8127 - val_loss: 1.4018 - val_acc: 0.6402\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.2818 - acc: 0.8189 - val_loss: 1.2972 - val_acc: 0.6652\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.2710 - acc: 0.8259 - val_loss: 1.2742 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.2537 - acc: 0.8322 - val_loss: 1.3686 - val_acc: 0.6535\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.2199 - acc: 0.8418 - val_loss: 1.1012 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.2132 - acc: 0.8445 - val_loss: 1.0701 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.1124 - acc: 0.8779 - val_loss: 0.8187 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.06991 to 0.81870, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 30/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0784 - acc: 0.8828 - val_loss: 0.8133 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.81870 to 0.81331, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 31/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0596 - acc: 0.8883 - val_loss: 0.8012 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.81331 to 0.80123, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 32/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0667 - acc: 0.8909 - val_loss: 0.8013 - val_acc: 0.7959\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0651 - acc: 0.8853 - val_loss: 0.8008 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.80123 to 0.80076, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 34/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0449 - acc: 0.8921 - val_loss: 0.8057 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0469 - acc: 0.8860 - val_loss: 0.8086 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0508 - acc: 0.8860 - val_loss: 0.8118 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0330 - acc: 0.8951 - val_loss: 0.8031 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "126/126 [==============================] - 39s 308ms/step - loss: 1.0436 - acc: 0.8945 - val_loss: 0.8048 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "126/126 [==============================] - 39s 307ms/step - loss: 1.0487 - acc: 0.8919 - val_loss: 0.8110 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0181 - acc: 0.8952 - val_loss: 0.7981 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.80076 to 0.79811, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 41/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0213 - acc: 0.8921 - val_loss: 0.7969 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.79811 to 0.79688, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 42/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0120 - acc: 0.8968 - val_loss: 0.7930 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.79688 to 0.79298, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_1.h5\n",
      "Epoch 43/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0182 - acc: 0.8940 - val_loss: 0.7959 - val_acc: 0.7959\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0187 - acc: 0.8978 - val_loss: 0.7957 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0372 - acc: 0.8929 - val_loss: 0.7964 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0319 - acc: 0.8953 - val_loss: 0.7948 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "126/126 [==============================] - 39s 309ms/step - loss: 1.0101 - acc: 0.8968 - val_loss: 0.7966 - val_acc: 0.7922\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0206 - acc: 0.8907 - val_loss: 0.7940 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "126/126 [==============================] - 39s 308ms/step - loss: 1.0241 - acc: 0.8922 - val_loss: 0.7938 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0110 - acc: 0.8972 - val_loss: 0.7939 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.0208 - acc: 0.8981 - val_loss: 0.7939 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0201 - acc: 0.8972 - val_loss: 0.7940 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.0193 - acc: 0.8960 - val_loss: 0.7943 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.0149 - acc: 0.9012 - val_loss: 0.7945 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0257 - acc: 0.8979 - val_loss: 0.7936 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0178 - acc: 0.8904 - val_loss: 0.7937 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.0223 - acc: 0.8956 - val_loss: 0.7934 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "9473/9473 [==============================] - 27s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8112, 256, 431, 1) (8112, 41)\n",
      "val shape:  (1361, 256, 431, 1) (1361, 41)\n",
      "Fold:  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 3.2649 - acc: 0.1531 - val_loss: 4.1082 - val_acc: 0.1117\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.10816, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 2/300\n",
      "126/126 [==============================] - 39s 309ms/step - loss: 2.8666 - acc: 0.2639 - val_loss: 3.4295 - val_acc: 0.1940\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.10816 to 3.42952, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 3/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 2.5792 - acc: 0.3712 - val_loss: 2.9871 - val_acc: 0.2278\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.42952 to 2.98713, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 4/300\n",
      "126/126 [==============================] - 39s 309ms/step - loss: 2.3804 - acc: 0.4348 - val_loss: 2.9931 - val_acc: 0.2711\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 2.2199 - acc: 0.4996 - val_loss: 1.9816 - val_acc: 0.4526\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.98713 to 1.98158, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 6/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 2.1278 - acc: 0.5337 - val_loss: 2.2009 - val_acc: 0.4210\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 2.0247 - acc: 0.5769 - val_loss: 1.8103 - val_acc: 0.5107\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.98158 to 1.81028, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 8/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.9053 - acc: 0.6101 - val_loss: 1.6802 - val_acc: 0.5496\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.81028 to 1.68022, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 9/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.8215 - acc: 0.6440 - val_loss: 1.8304 - val_acc: 0.5342\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.7769 - acc: 0.6587 - val_loss: 1.4639 - val_acc: 0.6076\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.68022 to 1.46387, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 11/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.6940 - acc: 0.6832 - val_loss: 1.4324 - val_acc: 0.6143\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.46387 to 1.43243, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 12/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.6484 - acc: 0.7004 - val_loss: 1.1948 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.43243 to 1.19483, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 13/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.6062 - acc: 0.7176 - val_loss: 1.2334 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.5580 - acc: 0.7262 - val_loss: 1.1776 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.19483 to 1.17763, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 15/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.5321 - acc: 0.7392 - val_loss: 1.1941 - val_acc: 0.6716\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.4729 - acc: 0.7561 - val_loss: 1.1564 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.17763 to 1.15643, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 17/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.4512 - acc: 0.7675 - val_loss: 1.4471 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.4218 - acc: 0.7775 - val_loss: 1.3560 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.3821 - acc: 0.7932 - val_loss: 1.1970 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3782 - acc: 0.7956 - val_loss: 0.9744 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.15643 to 0.97436, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 21/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.3515 - acc: 0.7995 - val_loss: 1.1315 - val_acc: 0.7164\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.3382 - acc: 0.8089 - val_loss: 1.0276 - val_acc: 0.7348\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.2974 - acc: 0.8186 - val_loss: 1.0887 - val_acc: 0.7142\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.2735 - acc: 0.8271 - val_loss: 1.0237 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.2686 - acc: 0.8295 - val_loss: 0.9743 - val_acc: 0.7458\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.97436 to 0.97431, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 26/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.2528 - acc: 0.8307 - val_loss: 1.0010 - val_acc: 0.7377\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.1398 - acc: 0.8695 - val_loss: 0.7593 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.97431 to 0.75927, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 28/300\n",
      "126/126 [==============================] - 37s 298ms/step - loss: 1.1003 - acc: 0.8775 - val_loss: 0.7488 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.75927 to 0.74878, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 29/300\n",
      "126/126 [==============================] - 39s 308ms/step - loss: 1.0982 - acc: 0.8760 - val_loss: 0.7524 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0955 - acc: 0.8709 - val_loss: 0.7306 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.74878 to 0.73061, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 31/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.0907 - acc: 0.8780 - val_loss: 0.7258 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.73061 to 0.72581, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 32/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0881 - acc: 0.8801 - val_loss: 0.7407 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "126/126 [==============================] - 39s 307ms/step - loss: 1.0702 - acc: 0.8900 - val_loss: 0.7391 - val_acc: 0.8148\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "126/126 [==============================] - 37s 298ms/step - loss: 1.0740 - acc: 0.8873 - val_loss: 0.7402 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "126/126 [==============================] - 37s 290ms/step - loss: 1.0571 - acc: 0.8919 - val_loss: 0.7382 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.0711 - acc: 0.8909 - val_loss: 0.7215 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.72581 to 0.72152, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 37/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.0569 - acc: 0.8826 - val_loss: 0.7244 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "126/126 [==============================] - 36s 283ms/step - loss: 1.0691 - acc: 0.8870 - val_loss: 0.7365 - val_acc: 0.8163\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.0621 - acc: 0.8905 - val_loss: 0.7276 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0418 - acc: 0.8945 - val_loss: 0.7216 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0398 - acc: 0.8867 - val_loss: 0.7350 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0377 - acc: 0.8903 - val_loss: 0.7310 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0402 - acc: 0.8912 - val_loss: 0.7181 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.72152 to 0.71814, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 44/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0366 - acc: 0.8969 - val_loss: 0.7181 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.71814 to 0.71808, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 45/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0509 - acc: 0.8894 - val_loss: 0.7160 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.71808 to 0.71597, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 46/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0138 - acc: 0.8965 - val_loss: 0.7164 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0300 - acc: 0.8904 - val_loss: 0.7129 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.71597 to 0.71286, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 48/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0309 - acc: 0.8962 - val_loss: 0.7135 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0165 - acc: 0.8965 - val_loss: 0.7125 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.71286 to 0.71248, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_2.h5\n",
      "Epoch 50/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0219 - acc: 0.8935 - val_loss: 0.7163 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0200 - acc: 0.8937 - val_loss: 0.7165 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0130 - acc: 0.8968 - val_loss: 0.7145 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.0315 - acc: 0.8932 - val_loss: 0.7147 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0250 - acc: 0.8903 - val_loss: 0.7159 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0221 - acc: 0.8940 - val_loss: 0.7147 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0312 - acc: 0.8967 - val_loss: 0.7132 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0358 - acc: 0.8907 - val_loss: 0.7131 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0270 - acc: 0.8914 - val_loss: 0.7134 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0256 - acc: 0.8960 - val_loss: 0.7139 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0353 - acc: 0.8956 - val_loss: 0.7136 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0320 - acc: 0.8926 - val_loss: 0.7135 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0366 - acc: 0.8858 - val_loss: 0.7133 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0449 - acc: 0.8909 - val_loss: 0.7134 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.0302 - acc: 0.8914 - val_loss: 0.7142 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "9473/9473 [==============================] - 26s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8115, 256, 431, 1) (8115, 41)\n",
      "val shape:  (1358, 256, 431, 1) (1358, 41)\n",
      "Fold:  3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 3.2468 - acc: 0.1637 - val_loss: 4.1959 - val_acc: 0.0891\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.19594, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 2/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 2.8797 - acc: 0.2630 - val_loss: 4.6670 - val_acc: 0.0832\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 2.5631 - acc: 0.3642 - val_loss: 3.3879 - val_acc: 0.2113\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.19594 to 3.38787, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 4/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 2.3709 - acc: 0.4433 - val_loss: 2.4575 - val_acc: 0.3373\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.38787 to 2.45754, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 5/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 2.2335 - acc: 0.4950 - val_loss: 2.2968 - val_acc: 0.4190\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.45754 to 2.29675, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 6/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 2.0783 - acc: 0.5443 - val_loss: 2.8038 - val_acc: 0.3277\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.9932 - acc: 0.5828 - val_loss: 1.6844 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.29675 to 1.68438, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 8/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.8984 - acc: 0.6090 - val_loss: 1.4822 - val_acc: 0.6082\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.68438 to 1.48220, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 9/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.8305 - acc: 0.6358 - val_loss: 1.6845 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.7447 - acc: 0.6710 - val_loss: 1.6337 - val_acc: 0.5604\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.7104 - acc: 0.6825 - val_loss: 2.1015 - val_acc: 0.4934\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.6751 - acc: 0.6964 - val_loss: 1.5732 - val_acc: 0.5839\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.5185 - acc: 0.7458 - val_loss: 1.3957 - val_acc: 0.6370\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.4635 - acc: 0.7622 - val_loss: 1.1941 - val_acc: 0.6892\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.28732 to 1.19413, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 18/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.4418 - acc: 0.7692 - val_loss: 1.8075 - val_acc: 0.5744\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.4114 - acc: 0.7768 - val_loss: 1.1670 - val_acc: 0.6959\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.19413 to 1.16701, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 20/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.3804 - acc: 0.7886 - val_loss: 1.1623 - val_acc: 0.7047\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.16701 to 1.16228, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 21/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.3680 - acc: 0.7995 - val_loss: 1.2192 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3334 - acc: 0.8078 - val_loss: 1.1441 - val_acc: 0.7010\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.16228 to 1.14407, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 23/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3118 - acc: 0.8068 - val_loss: 1.1127 - val_acc: 0.7187\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.14407 to 1.11266, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 24/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.2911 - acc: 0.8207 - val_loss: 1.3598 - val_acc: 0.6340\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.2631 - acc: 0.8292 - val_loss: 1.0215 - val_acc: 0.7386\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.11266 to 1.02151, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 26/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.2649 - acc: 0.8309 - val_loss: 1.3972 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.2415 - acc: 0.8364 - val_loss: 1.1037 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.2152 - acc: 0.8408 - val_loss: 1.1243 - val_acc: 0.7165\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.2091 - acc: 0.8456 - val_loss: 1.5015 - val_acc: 0.6289\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.2157 - acc: 0.8404 - val_loss: 1.0621 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.2084 - acc: 0.8433 - val_loss: 1.1154 - val_acc: 0.7283\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.1113 - acc: 0.8787 - val_loss: 0.8129 - val_acc: 0.7828\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.02151 to 0.81288, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 33/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0695 - acc: 0.8919 - val_loss: 0.8020 - val_acc: 0.7835\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.81288 to 0.80198, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 34/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0615 - acc: 0.8863 - val_loss: 0.7943 - val_acc: 0.7828\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.80198 to 0.79430, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 35/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0509 - acc: 0.8895 - val_loss: 0.7954 - val_acc: 0.7887\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0564 - acc: 0.8937 - val_loss: 0.8092 - val_acc: 0.7835\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0515 - acc: 0.8915 - val_loss: 0.7902 - val_acc: 0.7887\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.79430 to 0.79021, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 38/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0549 - acc: 0.8869 - val_loss: 0.7860 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.79021 to 0.78596, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 39/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0397 - acc: 0.8842 - val_loss: 0.7891 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0300 - acc: 0.8937 - val_loss: 0.7801 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.78596 to 0.78014, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 41/300\n",
      "126/126 [==============================] - 39s 307ms/step - loss: 1.0335 - acc: 0.8937 - val_loss: 0.7945 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0272 - acc: 0.8932 - val_loss: 0.7817 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.0272 - acc: 0.8893 - val_loss: 0.7998 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0179 - acc: 0.8971 - val_loss: 0.7775 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.78014 to 0.77752, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 45/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0115 - acc: 0.8927 - val_loss: 0.7785 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0195 - acc: 0.8931 - val_loss: 0.8259 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0236 - acc: 0.8950 - val_loss: 0.7894 - val_acc: 0.7887\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0153 - acc: 0.8938 - val_loss: 0.7858 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0155 - acc: 0.8963 - val_loss: 0.8084 - val_acc: 0.7887\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0008 - acc: 0.8994 - val_loss: 0.7961 - val_acc: 0.7806\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 0.9857 - acc: 0.9019 - val_loss: 0.7822 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 0.9977 - acc: 0.9020 - val_loss: 0.7813 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0074 - acc: 0.9000 - val_loss: 0.7819 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 0.9982 - acc: 0.8969 - val_loss: 0.7772 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.77752 to 0.77719, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 55/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.0048 - acc: 0.8919 - val_loss: 0.7790 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0055 - acc: 0.8969 - val_loss: 0.7779 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0044 - acc: 0.8984 - val_loss: 0.7765 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.77719 to 0.77655, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 58/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 0.9969 - acc: 0.8997 - val_loss: 0.7794 - val_acc: 0.7887\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0028 - acc: 0.8965 - val_loss: 0.7802 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 0.9915 - acc: 0.9004 - val_loss: 0.7770 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 0.9972 - acc: 0.8991 - val_loss: 0.7742 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.77655 to 0.77417, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_3.h5\n",
      "Epoch 62/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 0.9873 - acc: 0.8982 - val_loss: 0.7742 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 0.9989 - acc: 0.8989 - val_loss: 0.7765 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 0.9770 - acc: 0.8988 - val_loss: 0.7752 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 0.9804 - acc: 0.9014 - val_loss: 0.7801 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 0.9863 - acc: 0.9024 - val_loss: 0.7762 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 0.9957 - acc: 0.8988 - val_loss: 0.7776 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 0.9894 - acc: 0.9000 - val_loss: 0.7771 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 0.9789 - acc: 0.9028 - val_loss: 0.7778 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 0.9810 - acc: 0.9024 - val_loss: 0.7763 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 0.9791 - acc: 0.9010 - val_loss: 0.7771 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0043 - acc: 0.8976 - val_loss: 0.7758 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/300\n",
      "126/126 [==============================] - 37s 298ms/step - loss: 0.9875 - acc: 0.9003 - val_loss: 0.7771 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/300\n",
      "126/126 [==============================] - 37s 298ms/step - loss: 0.9840 - acc: 0.9031 - val_loss: 0.7769 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 0.9955 - acc: 0.9020 - val_loss: 0.7766 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 0.9945 - acc: 0.8957 - val_loss: 0.7766 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "9473/9473 [==============================] - 28s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8121, 256, 431, 1) (8121, 41)\n",
      "val shape:  (1352, 256, 431, 1) (1352, 41)\n",
      "Fold:  4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "126/126 [==============================] - 39s 307ms/step - loss: 3.2376 - acc: 0.1576 - val_loss: 3.2409 - val_acc: 0.1575\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.24089, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 2/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 2.8684 - acc: 0.2625 - val_loss: 4.6752 - val_acc: 0.2078\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 2.5846 - acc: 0.3684 - val_loss: 2.4643 - val_acc: 0.3536\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.24089 to 2.46435, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 4/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 2.3609 - acc: 0.4469 - val_loss: 2.3379 - val_acc: 0.3587\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.46435 to 2.33790, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 5/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 2.2116 - acc: 0.5089 - val_loss: 2.2307 - val_acc: 0.3979\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.33790 to 2.23069, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 6/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 2.0890 - acc: 0.5510 - val_loss: 1.6368 - val_acc: 0.5510\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.23069 to 1.63679, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 7/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 1.9594 - acc: 0.5936 - val_loss: 1.4686 - val_acc: 0.6036\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.63679 to 1.46855, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 8/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.8769 - acc: 0.6229 - val_loss: 1.3961 - val_acc: 0.5954\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.46855 to 1.39606, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 9/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 1.7958 - acc: 0.6518 - val_loss: 1.4712 - val_acc: 0.5925\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.7231 - acc: 0.6751 - val_loss: 1.2261 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.39606 to 1.22607, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 11/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.6613 - acc: 0.6931 - val_loss: 1.2015 - val_acc: 0.6886\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.22607 to 1.20152, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 12/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.6378 - acc: 0.7067 - val_loss: 1.1949 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.20152 to 1.19489, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 13/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.5913 - acc: 0.7088 - val_loss: 0.9819 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.19489 to 0.98186, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 14/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.5582 - acc: 0.7316 - val_loss: 1.1586 - val_acc: 0.6768\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.5337 - acc: 0.7330 - val_loss: 1.2836 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.4890 - acc: 0.7556 - val_loss: 1.0309 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "126/126 [==============================] - 37s 290ms/step - loss: 1.4501 - acc: 0.7677 - val_loss: 1.0864 - val_acc: 0.7027\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.4259 - acc: 0.7748 - val_loss: 1.4802 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.3701 - acc: 0.7932 - val_loss: 1.2556 - val_acc: 0.6731\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.2621 - acc: 0.8335 - val_loss: 0.7041 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.98186 to 0.70407, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 21/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.2510 - acc: 0.8414 - val_loss: 0.6847 - val_acc: 0.8225\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.70407 to 0.68468, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 22/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.2037 - acc: 0.8438 - val_loss: 0.6867 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.2029 - acc: 0.8478 - val_loss: 0.6754 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.68468 to 0.67538, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 24/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.1838 - acc: 0.8537 - val_loss: 0.6757 - val_acc: 0.8321\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.1681 - acc: 0.8529 - val_loss: 0.6888 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "126/126 [==============================] - 37s 291ms/step - loss: 1.1800 - acc: 0.8555 - val_loss: 0.6898 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "126/126 [==============================] - 37s 292ms/step - loss: 1.1591 - acc: 0.8571 - val_loss: 0.6788 - val_acc: 0.8328\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 1.1789 - acc: 0.8581 - val_loss: 0.6753 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.67538 to 0.67534, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 29/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.1561 - acc: 0.8646 - val_loss: 0.6988 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.1321 - acc: 0.8684 - val_loss: 0.6616 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.67534 to 0.66156, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 31/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.1329 - acc: 0.8637 - val_loss: 0.6602 - val_acc: 0.8284\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.66156 to 0.66022, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 32/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.1379 - acc: 0.8661 - val_loss: 0.6624 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.1304 - acc: 0.8672 - val_loss: 0.6545 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.66022 to 0.65451, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 34/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.1389 - acc: 0.8647 - val_loss: 0.6563 - val_acc: 0.8321\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.1429 - acc: 0.8635 - val_loss: 0.6611 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.1328 - acc: 0.8689 - val_loss: 0.6582 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.1348 - acc: 0.8693 - val_loss: 0.6547 - val_acc: 0.8321\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.1490 - acc: 0.8646 - val_loss: 0.6542 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.65451 to 0.65424, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_4.h5\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 37s 295ms/step - loss: 1.1348 - acc: 0.8659 - val_loss: 0.6566 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.1298 - acc: 0.8690 - val_loss: 0.6566 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.1403 - acc: 0.8625 - val_loss: 0.6595 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "126/126 [==============================] - 37s 292ms/step - loss: 1.1329 - acc: 0.8601 - val_loss: 0.6632 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.1353 - acc: 0.8704 - val_loss: 0.6574 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.1286 - acc: 0.8621 - val_loss: 0.6594 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 1.1344 - acc: 0.8635 - val_loss: 0.6573 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "126/126 [==============================] - 37s 291ms/step - loss: 1.1465 - acc: 0.8667 - val_loss: 0.6564 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 1.1290 - acc: 0.8756 - val_loss: 0.6565 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.1203 - acc: 0.8667 - val_loss: 0.6575 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 1.1182 - acc: 0.8699 - val_loss: 0.6571 - val_acc: 0.8284\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.1349 - acc: 0.8723 - val_loss: 0.6570 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 1.1303 - acc: 0.8681 - val_loss: 0.6574 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.1403 - acc: 0.8652 - val_loss: 0.6567 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "126/126 [==============================] - 37s 293ms/step - loss: 1.1407 - acc: 0.8686 - val_loss: 0.6561 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "9473/9473 [==============================] - 27s 3ms/step\n",
      "9400/9400 [==============================] - 28s 3ms/step\n",
      "train shape:  (8125, 256, 431, 1) (8125, 41)\n",
      "val shape:  (1348, 256, 431, 1) (1348, 41)\n",
      "Fold:  5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 3.2545 - acc: 0.1494 - val_loss: 5.7399 - val_acc: 0.0349\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.73993, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 2/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 2.8464 - acc: 0.2814 - val_loss: 4.7623 - val_acc: 0.1313\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.73993 to 4.76230, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 3/300\n",
      "126/126 [==============================] - 37s 294ms/step - loss: 2.5396 - acc: 0.3948 - val_loss: 2.6791 - val_acc: 0.2767\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.76230 to 2.67912, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 4/300\n",
      "126/126 [==============================] - 37s 295ms/step - loss: 2.3554 - acc: 0.4559 - val_loss: 2.0478 - val_acc: 0.4332\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.67912 to 2.04780, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 5/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 2.1628 - acc: 0.5264 - val_loss: 1.9846 - val_acc: 0.4763\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.04780 to 1.98465, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 6/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 2.0532 - acc: 0.5561 - val_loss: 1.8699 - val_acc: 0.4696\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.98465 to 1.86990, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 7/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.9486 - acc: 0.5933 - val_loss: 1.7803 - val_acc: 0.5096\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.86990 to 1.78029, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 8/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.8525 - acc: 0.6343 - val_loss: 1.6655 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.78029 to 1.66546, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 9/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.8064 - acc: 0.6522 - val_loss: 1.6682 - val_acc: 0.5475\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.7047 - acc: 0.6746 - val_loss: 1.4466 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.66546 to 1.44658, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 11/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.6578 - acc: 0.6936 - val_loss: 1.4223 - val_acc: 0.6246\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.44658 to 1.42226, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 12/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.6313 - acc: 0.7111 - val_loss: 1.3725 - val_acc: 0.6328\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.42226 to 1.37247, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 13/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.5772 - acc: 0.7236 - val_loss: 1.4022 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.5287 - acc: 0.7417 - val_loss: 1.8737 - val_acc: 0.5304\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.5011 - acc: 0.7526 - val_loss: 1.3663 - val_acc: 0.6588\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.37247 to 1.36629, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 16/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.4675 - acc: 0.7656 - val_loss: 1.7059 - val_acc: 0.5653\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.4267 - acc: 0.7710 - val_loss: 1.4274 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.4134 - acc: 0.7775 - val_loss: 1.1990 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.36629 to 1.19898, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 19/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3929 - acc: 0.7834 - val_loss: 1.2230 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3563 - acc: 0.7979 - val_loss: 1.2913 - val_acc: 0.6728\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3435 - acc: 0.8067 - val_loss: 1.3191 - val_acc: 0.6684\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.3189 - acc: 0.8061 - val_loss: 1.4128 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.2771 - acc: 0.8198 - val_loss: 1.5055 - val_acc: 0.6291\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.2872 - acc: 0.8248 - val_loss: 1.2341 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.1766 - acc: 0.8575 - val_loss: 0.8562 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.19898 to 0.85620, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 26/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.1399 - acc: 0.8606 - val_loss: 0.8444 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.85620 to 0.84435, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 27/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.1312 - acc: 0.8705 - val_loss: 0.8345 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.84435 to 0.83455, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 28/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.1226 - acc: 0.8673 - val_loss: 0.8348 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.1198 - acc: 0.8738 - val_loss: 0.8319 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.83455 to 0.83189, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 30/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.1110 - acc: 0.8733 - val_loss: 0.8324 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.1119 - acc: 0.8715 - val_loss: 0.8364 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0984 - acc: 0.8811 - val_loss: 0.8432 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.1117 - acc: 0.8707 - val_loss: 0.8467 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0885 - acc: 0.8827 - val_loss: 0.8176 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.83189 to 0.81762, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 35/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0862 - acc: 0.8797 - val_loss: 0.8371 - val_acc: 0.7893\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0822 - acc: 0.8819 - val_loss: 0.8226 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0901 - acc: 0.8800 - val_loss: 0.8210 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0686 - acc: 0.8824 - val_loss: 0.8319 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0814 - acc: 0.8781 - val_loss: 0.8331 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0749 - acc: 0.8795 - val_loss: 0.8339 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0520 - acc: 0.8852 - val_loss: 0.8129 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.81762 to 0.81294, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 42/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0493 - acc: 0.8863 - val_loss: 0.8119 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.81294 to 0.81188, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 43/300\n",
      "126/126 [==============================] - 39s 306ms/step - loss: 1.0648 - acc: 0.8886 - val_loss: 0.8141 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0647 - acc: 0.8829 - val_loss: 0.8142 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0608 - acc: 0.8865 - val_loss: 0.8130 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0630 - acc: 0.8875 - val_loss: 0.8148 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0547 - acc: 0.8824 - val_loss: 0.8135 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0441 - acc: 0.8895 - val_loss: 0.8143 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.0366 - acc: 0.8926 - val_loss: 0.8132 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0517 - acc: 0.8901 - val_loss: 0.8128 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0603 - acc: 0.8848 - val_loss: 0.8122 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0565 - acc: 0.8817 - val_loss: 0.8123 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0563 - acc: 0.8885 - val_loss: 0.8122 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0497 - acc: 0.8870 - val_loss: 0.8126 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "126/126 [==============================] - 38s 302ms/step - loss: 1.0475 - acc: 0.8896 - val_loss: 0.8123 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "126/126 [==============================] - 38s 299ms/step - loss: 1.0620 - acc: 0.8834 - val_loss: 0.8119 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0553 - acc: 0.8841 - val_loss: 0.8115 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.81188 to 0.81153, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_5.h5\n",
      "Epoch 58/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0544 - acc: 0.8843 - val_loss: 0.8119 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.0681 - acc: 0.8867 - val_loss: 0.8120 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0558 - acc: 0.8898 - val_loss: 0.8128 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.0468 - acc: 0.8834 - val_loss: 0.8123 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/300\n",
      "126/126 [==============================] - 37s 296ms/step - loss: 1.0590 - acc: 0.8875 - val_loss: 0.8126 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0642 - acc: 0.8857 - val_loss: 0.8124 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0610 - acc: 0.8868 - val_loss: 0.8119 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "126/126 [==============================] - 38s 305ms/step - loss: 1.0456 - acc: 0.8899 - val_loss: 0.8118 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/300\n",
      "126/126 [==============================] - 38s 298ms/step - loss: 1.0549 - acc: 0.8844 - val_loss: 0.8117 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/300\n",
      "126/126 [==============================] - 37s 297ms/step - loss: 1.0547 - acc: 0.8834 - val_loss: 0.8120 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0593 - acc: 0.8901 - val_loss: 0.8121 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/300\n",
      "126/126 [==============================] - 38s 304ms/step - loss: 1.0571 - acc: 0.8832 - val_loss: 0.8118 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/300\n",
      "126/126 [==============================] - 38s 303ms/step - loss: 1.0605 - acc: 0.8829 - val_loss: 0.8123 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/300\n",
      "126/126 [==============================] - 38s 300ms/step - loss: 1.0634 - acc: 0.8870 - val_loss: 0.8123 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/300\n",
      "126/126 [==============================] - 38s 301ms/step - loss: 1.0517 - acc: 0.8855 - val_loss: 0.8124 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "9473/9473 [==============================] - 28s 3ms/step\n",
      "9400/9400 [==============================] - 27s 3ms/step\n",
      "train shape:  (8146, 256, 431, 1) (8146, 41)\n",
      "val shape:  (1327, 256, 431, 1) (1327, 41)\n",
      "Fold:  6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "127/127 [==============================] - 38s 302ms/step - loss: 3.2319 - acc: 0.1603 - val_loss: 3.0858 - val_acc: 0.1876\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.08575, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 2/300\n",
      "127/127 [==============================] - 38s 301ms/step - loss: 2.8909 - acc: 0.2582 - val_loss: 3.4558 - val_acc: 0.2163\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/300\n",
      "127/127 [==============================] - 38s 302ms/step - loss: 2.6287 - acc: 0.3533 - val_loss: 2.3106 - val_acc: 0.3474\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.08575 to 2.31058, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 4/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 2.3641 - acc: 0.4465 - val_loss: 2.4397 - val_acc: 0.3708\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "127/127 [==============================] - 38s 297ms/step - loss: 2.1838 - acc: 0.5145 - val_loss: 2.3465 - val_acc: 0.3775\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 2.0684 - acc: 0.5570 - val_loss: 1.9325 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.31058 to 1.93247, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 7/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.9728 - acc: 0.5955 - val_loss: 1.4091 - val_acc: 0.6112\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.93247 to 1.40911, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 8/300\n",
      "127/127 [==============================] - 39s 305ms/step - loss: 1.8768 - acc: 0.6265 - val_loss: 2.0887 - val_acc: 0.4740\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "127/127 [==============================] - 38s 297ms/step - loss: 1.7890 - acc: 0.6528 - val_loss: 2.0263 - val_acc: 0.4732\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "127/127 [==============================] - 39s 304ms/step - loss: 1.7136 - acc: 0.6815 - val_loss: 1.3038 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.40911 to 1.30379, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 11/300\n",
      "127/127 [==============================] - 38s 301ms/step - loss: 1.6760 - acc: 0.6912 - val_loss: 1.8587 - val_acc: 0.5328\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "127/127 [==============================] - 38s 302ms/step - loss: 1.6263 - acc: 0.7094 - val_loss: 1.2205 - val_acc: 0.6677\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.30379 to 1.22047, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 13/300\n",
      "127/127 [==============================] - 38s 299ms/step - loss: 1.5514 - acc: 0.7376 - val_loss: 1.5100 - val_acc: 0.6044\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "127/127 [==============================] - 38s 299ms/step - loss: 1.5189 - acc: 0.7395 - val_loss: 1.2359 - val_acc: 0.6556\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "127/127 [==============================] - 38s 299ms/step - loss: 1.4917 - acc: 0.7574 - val_loss: 1.0488 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.22047 to 1.04878, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 16/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.4578 - acc: 0.7745 - val_loss: 1.0493 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "127/127 [==============================] - 38s 296ms/step - loss: 1.4107 - acc: 0.7794 - val_loss: 1.7543 - val_acc: 0.5411\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "127/127 [==============================] - 38s 303ms/step - loss: 1.4014 - acc: 0.7867 - val_loss: 1.1186 - val_acc: 0.7084\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "127/127 [==============================] - 38s 301ms/step - loss: 1.3727 - acc: 0.7913 - val_loss: 1.6981 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "127/127 [==============================] - 38s 301ms/step - loss: 1.3218 - acc: 0.8097 - val_loss: 1.4924 - val_acc: 0.6285\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "127/127 [==============================] - 38s 301ms/step - loss: 1.3227 - acc: 0.8065 - val_loss: 1.6359 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.1988 - acc: 0.8509 - val_loss: 0.7671 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.04878 to 0.76705, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 23/300\n",
      "127/127 [==============================] - 38s 296ms/step - loss: 1.1688 - acc: 0.8605 - val_loss: 0.7589 - val_acc: 0.8169\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.76705 to 0.75888, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 24/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.1609 - acc: 0.8627 - val_loss: 0.7501 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.75888 to 0.75008, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 25/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.1580 - acc: 0.8647 - val_loss: 0.7421 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.75008 to 0.74212, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 26/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.1364 - acc: 0.8653 - val_loss: 0.7497 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.1354 - acc: 0.8741 - val_loss: 0.7355 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.74212 to 0.73549, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 28/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.1345 - acc: 0.8654 - val_loss: 0.7515 - val_acc: 0.8124\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "127/127 [==============================] - 38s 299ms/step - loss: 1.1041 - acc: 0.8778 - val_loss: 0.7541 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "127/127 [==============================] - 38s 296ms/step - loss: 1.1168 - acc: 0.8708 - val_loss: 0.7581 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.1104 - acc: 0.8783 - val_loss: 0.7338 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.73549 to 0.73377, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 32/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.0925 - acc: 0.8812 - val_loss: 0.7544 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.1006 - acc: 0.8819 - val_loss: 0.7540 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "127/127 [==============================] - 37s 294ms/step - loss: 1.0943 - acc: 0.8783 - val_loss: 0.7421 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "127/127 [==============================] - 38s 301ms/step - loss: 1.1043 - acc: 0.8778 - val_loss: 0.7324 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.73377 to 0.73245, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 36/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.0742 - acc: 0.8826 - val_loss: 0.7482 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "127/127 [==============================] - 38s 299ms/step - loss: 1.0974 - acc: 0.8776 - val_loss: 0.7429 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "127/127 [==============================] - 38s 302ms/step - loss: 1.0833 - acc: 0.8815 - val_loss: 0.7690 - val_acc: 0.8169\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.0878 - acc: 0.8820 - val_loss: 0.7519 - val_acc: 0.8139\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "127/127 [==============================] - 38s 297ms/step - loss: 1.0738 - acc: 0.8814 - val_loss: 0.7643 - val_acc: 0.8199\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.0656 - acc: 0.8848 - val_loss: 0.7552 - val_acc: 0.8154\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.0534 - acc: 0.8832 - val_loss: 0.7331 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "127/127 [==============================] - 38s 299ms/step - loss: 1.0494 - acc: 0.8900 - val_loss: 0.7318 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.73245 to 0.73183, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 44/300\n",
      "127/127 [==============================] - 38s 298ms/step - loss: 1.0662 - acc: 0.8867 - val_loss: 0.7280 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.73183 to 0.72795, saving model to checkpoints_log_mel_sp_44000x5_mixup/best_6.h5\n",
      "Epoch 45/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.0665 - acc: 0.8845 - val_loss: 0.7325 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "127/127 [==============================] - 38s 300ms/step - loss: 1.0828 - acc: 0.8842 - val_loss: 0.7344 - val_acc: 0.8312\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "127/127 [==============================] - 38s 299ms/step - loss: 1.0520 - acc: 0.8941 - val_loss: 0.7329 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "127/127 [==============================] - 37s 293ms/step - loss: 1.0439 - acc: 0.8906 - val_loss: 0.7325 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "127/127 [==============================] - 37s 292ms/step - loss: 1.0579 - acc: 0.8879 - val_loss: 0.7354 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "127/127 [==============================] - 37s 292ms/step - loss: 1.0603 - acc: 0.8939 - val_loss: 0.7294 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "127/127 [==============================] - 37s 289ms/step - loss: 1.0549 - acc: 0.8880 - val_loss: 0.7312 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "127/127 [==============================] - 37s 292ms/step - loss: 1.0781 - acc: 0.8839 - val_loss: 0.7310 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "127/127 [==============================] - 36s 287ms/step - loss: 1.0543 - acc: 0.8905 - val_loss: 0.7315 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "127/127 [==============================] - 36s 287ms/step - loss: 1.0676 - acc: 0.8885 - val_loss: 0.7308 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "127/127 [==============================] - 36s 283ms/step - loss: 1.0495 - acc: 0.8866 - val_loss: 0.7325 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "127/127 [==============================] - 36s 283ms/step - loss: 1.0604 - acc: 0.8798 - val_loss: 0.7320 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "127/127 [==============================] - 36s 284ms/step - loss: 1.0625 - acc: 0.8882 - val_loss: 0.7324 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "127/127 [==============================] - 36s 286ms/step - loss: 1.0524 - acc: 0.8866 - val_loss: 0.7327 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "127/127 [==============================] - 36s 285ms/step - loss: 1.0483 - acc: 0.8832 - val_loss: 0.7315 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "9473/9473 [==============================] - 27s 3ms/step\n",
      "9400/9400 [==============================] - 26s 3ms/step\n",
      "CPU times: user 3h 22min 55s, sys: 3h 29min 44s, total: 6h 52min 40s\n",
      "Wall time: 4h 7min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# log mel sp 44100x5 mixup\n",
    "config = Config(sampling_rate=44100, audio_duration=5, n_classes=41, use_log_mel_sp=True, n_folds=7, max_epochs=300, n_mfcc=128*2)\n",
    "PREDICTION_FOLDER = \"predictions_log_mel_sp_44000x5_mixup\"\n",
    "CHECKPOINT_FOLDER = 'checkpoints_log_mel_sp_44000x5_mixup'\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "#     y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "    if i < 1:\n",
    "        continue\n",
    "    X = X_train[train_split]\n",
    "    y = y_train[train_split]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model(config)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#     model.load_weights('checkpoint_1d_24000x5' + '/best_99.h5')\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-7, amsgrad=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=adam,\n",
    "                         metrics=['acc'])\n",
    "\n",
    "#     sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#     parallel_model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=sgd,\n",
    "#                   metrics=['acc'])\n",
    "\n",
    "    checkpoint = ParallelModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    training_generator = MixupGenerator(X, y, batch_size=batch_size, alpha=0.5, datagen=None)()\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        steps_per_epoch=X.shape[0] // batch_size,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=config.max_epochs, verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "# Fine tune\n",
    "#     model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5' % i)\n",
    "#     K.set_value(model.optimizer.lr, 0.00001)\n",
    "#     parallel_model.fit_generator(mixupgen(),\n",
    "#                         steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "#                         epochs=10,\n",
    "#                         validation_data=test_datagen.flow(X_val, y_val), callbacks=None)\n",
    "#     parallel_model.save_weights(CHECKPOINT_FOLDER + '/besh_%d.h5' % i)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (8518, 256, 431, 1) (8518, 41)\n",
      "val shape:  (955, 256, 431, 1) (955, 41)\n",
      "Fold:  0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "59/59 [==============================] - 31s 530ms/step - loss: 3.3210 - acc: 0.1466 - val_loss: 5.5953 - val_acc: 0.0524\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.59528, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 2/300\n",
      "59/59 [==============================] - 26s 442ms/step - loss: 2.9153 - acc: 0.2487 - val_loss: 3.6543 - val_acc: 0.1550\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.59528 to 3.65429, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 3/300\n",
      "59/59 [==============================] - 27s 457ms/step - loss: 2.6254 - acc: 0.3533 - val_loss: 3.4967 - val_acc: 0.1874\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.65429 to 3.49675, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 4/300\n",
      "59/59 [==============================] - 26s 448ms/step - loss: 2.4308 - acc: 0.4206 - val_loss: 2.6301 - val_acc: 0.3257\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.49675 to 2.63007, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 5/300\n",
      "59/59 [==============================] - 26s 442ms/step - loss: 2.2517 - acc: 0.4957 - val_loss: 2.1167 - val_acc: 0.4157\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.63007 to 2.11668, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 6/300\n",
      "59/59 [==============================] - 26s 445ms/step - loss: 2.1535 - acc: 0.5345 - val_loss: 2.3567 - val_acc: 0.4094\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "59/59 [==============================] - 26s 446ms/step - loss: 2.0166 - acc: 0.5804 - val_loss: 1.7506 - val_acc: 0.5277\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.11668 to 1.75063, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 8/300\n",
      "59/59 [==============================] - 26s 446ms/step - loss: 1.9266 - acc: 0.6123 - val_loss: 1.6095 - val_acc: 0.5990\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.75063 to 1.60951, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 9/300\n",
      "59/59 [==============================] - 26s 439ms/step - loss: 1.8309 - acc: 0.6439 - val_loss: 1.4037 - val_acc: 0.6272\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.60951 to 1.40370, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 10/300\n",
      "59/59 [==============================] - 26s 438ms/step - loss: 1.7794 - acc: 0.6586 - val_loss: 1.3273 - val_acc: 0.6461\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.40370 to 1.32733, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 11/300\n",
      "59/59 [==============================] - 26s 446ms/step - loss: 1.7012 - acc: 0.6858 - val_loss: 1.3879 - val_acc: 0.6513\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "59/59 [==============================] - 26s 441ms/step - loss: 1.6451 - acc: 0.7035 - val_loss: 1.1306 - val_acc: 0.6974\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.32733 to 1.13065, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 13/300\n",
      "59/59 [==============================] - 27s 450ms/step - loss: 1.5961 - acc: 0.7222 - val_loss: 1.1558 - val_acc: 0.7047\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "59/59 [==============================] - 26s 446ms/step - loss: 1.5973 - acc: 0.7196 - val_loss: 2.0665 - val_acc: 0.4398\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "59/59 [==============================] - 27s 454ms/step - loss: 1.5339 - acc: 0.7490 - val_loss: 1.2106 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "59/59 [==============================] - 26s 448ms/step - loss: 1.4870 - acc: 0.7622 - val_loss: 1.0705 - val_acc: 0.7225\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.13065 to 1.07045, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 17/300\n",
      "59/59 [==============================] - 27s 453ms/step - loss: 1.4645 - acc: 0.7616 - val_loss: 1.3475 - val_acc: 0.6482\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "59/59 [==============================] - 26s 444ms/step - loss: 1.4276 - acc: 0.7743 - val_loss: 1.3317 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "59/59 [==============================] - 26s 448ms/step - loss: 1.3901 - acc: 0.7892 - val_loss: 1.0925 - val_acc: 0.7215\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "59/59 [==============================] - 26s 446ms/step - loss: 1.3692 - acc: 0.7988 - val_loss: 1.2273 - val_acc: 0.6859\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "59/59 [==============================] - 26s 446ms/step - loss: 1.3360 - acc: 0.8097 - val_loss: 1.1481 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "59/59 [==============================] - 27s 451ms/step - loss: 1.3051 - acc: 0.8170 - val_loss: 1.1816 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "59/59 [==============================] - 26s 442ms/step - loss: 1.2333 - acc: 0.8477 - val_loss: 0.8120 - val_acc: 0.8084\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.07045 to 0.81201, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 24/300\n",
      "59/59 [==============================] - 28s 473ms/step - loss: 1.1736 - acc: 0.8601 - val_loss: 0.7930 - val_acc: 0.8115\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.81201 to 0.79302, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 25/300\n",
      "59/59 [==============================] - 28s 468ms/step - loss: 1.1623 - acc: 0.8680 - val_loss: 0.7715 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.79302 to 0.77153, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 26/300\n",
      "59/59 [==============================] - 28s 478ms/step - loss: 1.1497 - acc: 0.8714 - val_loss: 0.7642 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.77153 to 0.76419, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 27/300\n",
      "59/59 [==============================] - 29s 485ms/step - loss: 1.1515 - acc: 0.8699 - val_loss: 0.7574 - val_acc: 0.8136\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.76419 to 0.75742, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 28/300\n",
      "59/59 [==============================] - 28s 476ms/step - loss: 1.1475 - acc: 0.8673 - val_loss: 0.7563 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.75742 to 0.75629, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 29/300\n",
      "59/59 [==============================] - 28s 469ms/step - loss: 1.1306 - acc: 0.8685 - val_loss: 0.7521 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.75629 to 0.75215, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 30/300\n",
      "59/59 [==============================] - 28s 475ms/step - loss: 1.1218 - acc: 0.8804 - val_loss: 0.7526 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "59/59 [==============================] - 29s 489ms/step - loss: 1.1100 - acc: 0.8775 - val_loss: 0.7484 - val_acc: 0.8209\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.75215 to 0.74842, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 32/300\n",
      "59/59 [==============================] - 28s 476ms/step - loss: 1.1111 - acc: 0.8783 - val_loss: 0.7640 - val_acc: 0.8126\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "59/59 [==============================] - 28s 477ms/step - loss: 1.1014 - acc: 0.8815 - val_loss: 0.7645 - val_acc: 0.8126\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "59/59 [==============================] - 28s 478ms/step - loss: 1.1204 - acc: 0.8773 - val_loss: 0.7584 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "59/59 [==============================] - 28s 477ms/step - loss: 1.1007 - acc: 0.8793 - val_loss: 0.7615 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "59/59 [==============================] - 28s 473ms/step - loss: 1.1038 - acc: 0.8852 - val_loss: 0.7662 - val_acc: 0.8136\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "59/59 [==============================] - 28s 483ms/step - loss: 1.0869 - acc: 0.8861 - val_loss: 0.7605 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "59/59 [==============================] - 28s 477ms/step - loss: 1.0772 - acc: 0.8886 - val_loss: 0.7387 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.74842 to 0.73868, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 39/300\n",
      "59/59 [==============================] - 28s 472ms/step - loss: 1.0878 - acc: 0.8811 - val_loss: 0.7356 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.73868 to 0.73563, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 40/300\n",
      "59/59 [==============================] - 27s 458ms/step - loss: 1.0708 - acc: 0.8835 - val_loss: 0.7352 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.73563 to 0.73517, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 41/300\n",
      "59/59 [==============================] - 27s 451ms/step - loss: 1.0766 - acc: 0.8898 - val_loss: 0.7348 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.73517 to 0.73480, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 42/300\n",
      "59/59 [==============================] - 26s 443ms/step - loss: 1.0727 - acc: 0.8917 - val_loss: 0.7346 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.73480 to 0.73462, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 43/300\n",
      "59/59 [==============================] - 26s 443ms/step - loss: 1.0676 - acc: 0.8868 - val_loss: 0.7390 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "59/59 [==============================] - 26s 436ms/step - loss: 1.0728 - acc: 0.8876 - val_loss: 0.7373 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "59/59 [==============================] - 26s 445ms/step - loss: 1.0706 - acc: 0.8886 - val_loss: 0.7374 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "59/59 [==============================] - 26s 445ms/step - loss: 1.0675 - acc: 0.8868 - val_loss: 0.7350 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "59/59 [==============================] - 26s 445ms/step - loss: 1.0768 - acc: 0.8860 - val_loss: 0.7349 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "59/59 [==============================] - 26s 446ms/step - loss: 1.0686 - acc: 0.8825 - val_loss: 0.7320 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.73462 to 0.73201, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_0.h5\n",
      "Epoch 49/300\n",
      "59/59 [==============================] - 26s 442ms/step - loss: 1.0763 - acc: 0.8769 - val_loss: 0.7363 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "59/59 [==============================] - 26s 445ms/step - loss: 1.0547 - acc: 0.8987 - val_loss: 0.7321 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "59/59 [==============================] - 26s 448ms/step - loss: 1.0854 - acc: 0.8901 - val_loss: 0.7335 - val_acc: 0.8199\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "59/59 [==============================] - 26s 440ms/step - loss: 1.0617 - acc: 0.8890 - val_loss: 0.7324 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "59/59 [==============================] - 27s 459ms/step - loss: 1.0831 - acc: 0.8806 - val_loss: 0.7325 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "59/59 [==============================] - 28s 481ms/step - loss: 1.0769 - acc: 0.8903 - val_loss: 0.7352 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "59/59 [==============================] - 27s 455ms/step - loss: 1.0752 - acc: 0.8902 - val_loss: 0.7333 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "59/59 [==============================] - 27s 455ms/step - loss: 1.0631 - acc: 0.8901 - val_loss: 0.7327 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "59/59 [==============================] - 27s 451ms/step - loss: 1.0647 - acc: 0.8882 - val_loss: 0.7324 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "59/59 [==============================] - 27s 449ms/step - loss: 1.0704 - acc: 0.8867 - val_loss: 0.7326 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9473/9473 [==============================] - 23s 2ms/step\n",
      "9400/9400 [==============================] - 23s 2ms/step\n",
      "train shape:  (8520, 256, 431, 1) (8520, 41)\n",
      "val shape:  (953, 256, 431, 1) (953, 41)\n",
      "Fold:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "60/60 [==============================] - 38s 631ms/step - loss: 3.2484 - acc: 0.1657 - val_loss: 8.1213 - val_acc: 0.0651\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.12129, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 2.8558 - acc: 0.2742 - val_loss: 3.1004 - val_acc: 0.2707\n",
      "\n",
      "Epoch 00002: val_loss improved from 8.12129 to 3.10044, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 2.5858 - acc: 0.3719 - val_loss: 3.1614 - val_acc: 0.2718\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 2.3479 - acc: 0.4536 - val_loss: 3.3557 - val_acc: 0.2172\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 2.2039 - acc: 0.5086 - val_loss: 2.2711 - val_acc: 0.4292\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.10044 to 2.27109, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 2.0873 - acc: 0.5528 - val_loss: 1.7571 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.27109 to 1.75706, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.9535 - acc: 0.5945 - val_loss: 1.6560 - val_acc: 0.5362\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.75706 to 1.65596, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.8811 - acc: 0.6204 - val_loss: 1.7083 - val_acc: 0.5320\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 33s 550ms/step - loss: 1.7875 - acc: 0.6567 - val_loss: 1.7364 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 33s 550ms/step - loss: 1.7433 - acc: 0.6681 - val_loss: 1.5256 - val_acc: 0.6139\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.65596 to 1.52564, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.6640 - acc: 0.6965 - val_loss: 1.4839 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.52564 to 1.48395, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.6094 - acc: 0.7094 - val_loss: 2.2866 - val_acc: 0.4858\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.5621 - acc: 0.7243 - val_loss: 1.2152 - val_acc: 0.6821\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.48395 to 1.21517, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.5688 - acc: 0.7350 - val_loss: 1.4010 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.4789 - acc: 0.7653 - val_loss: 1.4354 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.4720 - acc: 0.7689 - val_loss: 1.1691 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.21517 to 1.16912, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.4205 - acc: 0.7800 - val_loss: 1.5520 - val_acc: 0.6201\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.4031 - acc: 0.7935 - val_loss: 1.3509 - val_acc: 0.6611\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.3713 - acc: 0.7920 - val_loss: 1.3959 - val_acc: 0.6474\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 32s 540ms/step - loss: 1.3517 - acc: 0.8058 - val_loss: 1.1623 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.16912 to 1.16226, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.2986 - acc: 0.8227 - val_loss: 0.9992 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.16226 to 0.99920, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.2931 - acc: 0.8224 - val_loss: 1.2378 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.2591 - acc: 0.8281 - val_loss: 1.1579 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.2527 - acc: 0.8327 - val_loss: 1.1432 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.2312 - acc: 0.8439 - val_loss: 1.2369 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 1.2040 - acc: 0.8513 - val_loss: 1.3665 - val_acc: 0.6747\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.2084 - acc: 0.8447 - val_loss: 1.4436 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 31s 524ms/step - loss: 1.0981 - acc: 0.8837 - val_loss: 0.9439 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.99920 to 0.94388, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0742 - acc: 0.8900 - val_loss: 0.8568 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.94388 to 0.85678, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.0652 - acc: 0.8904 - val_loss: 0.8379 - val_acc: 0.8006\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.85678 to 0.83786, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0462 - acc: 0.8957 - val_loss: 0.8129 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.83786 to 0.81295, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.0410 - acc: 0.8952 - val_loss: 0.7859 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.81295 to 0.78592, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 33s 551ms/step - loss: 1.0395 - acc: 0.8952 - val_loss: 0.7944 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.0375 - acc: 0.8913 - val_loss: 0.7820 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.78592 to 0.78198, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0268 - acc: 0.8927 - val_loss: 0.7976 - val_acc: 0.7985\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0337 - acc: 0.8972 - val_loss: 0.7968 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0375 - acc: 0.8947 - val_loss: 0.7914 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 1.0211 - acc: 0.9005 - val_loss: 0.8216 - val_acc: 0.7964\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0096 - acc: 0.8999 - val_loss: 0.7930 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.0058 - acc: 0.9019 - val_loss: 0.8041 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.0037 - acc: 0.8988 - val_loss: 0.7858 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9973 - acc: 0.8987 - val_loss: 0.7811 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.78198 to 0.78106, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_1.h5\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 33s 550ms/step - loss: 1.0019 - acc: 0.9036 - val_loss: 0.7845 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 33s 546ms/step - loss: 1.0022 - acc: 0.9022 - val_loss: 0.7848 - val_acc: 0.8038\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0111 - acc: 0.8995 - val_loss: 0.7859 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9981 - acc: 0.8994 - val_loss: 0.7825 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.0134 - acc: 0.8957 - val_loss: 0.7832 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 33s 550ms/step - loss: 0.9982 - acc: 0.9092 - val_loss: 0.7834 - val_acc: 0.8038\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.0125 - acc: 0.9006 - val_loss: 0.7830 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 33s 552ms/step - loss: 1.0036 - acc: 0.9049 - val_loss: 0.7835 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.0024 - acc: 0.9035 - val_loss: 0.7826 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0078 - acc: 0.8989 - val_loss: 0.7824 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "9473/9473 [==============================] - 23s 2ms/step\n",
      "9400/9400 [==============================] - 23s 2ms/step\n",
      "train shape:  (8521, 256, 431, 1) (8521, 41)\n",
      "val shape:  (952, 256, 431, 1) (952, 41)\n",
      "Fold:  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "60/60 [==============================] - 41s 685ms/step - loss: 3.2779 - acc: 0.1575 - val_loss: 3.7622 - val_acc: 0.1313\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.76224, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 30s 506ms/step - loss: 2.9184 - acc: 0.2568 - val_loss: 3.5631 - val_acc: 0.1975\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.76224 to 3.56314, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 2.6295 - acc: 0.3553 - val_loss: 4.3686 - val_acc: 0.1418\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 2.3819 - acc: 0.4465 - val_loss: 3.4032 - val_acc: 0.2521\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.56314 to 3.40319, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 2.2496 - acc: 0.4958 - val_loss: 2.2635 - val_acc: 0.4128\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.40319 to 2.26351, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 33s 547ms/step - loss: 2.0728 - acc: 0.5665 - val_loss: 1.6065 - val_acc: 0.5683\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.26351 to 1.60646, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 1.9716 - acc: 0.5946 - val_loss: 2.5085 - val_acc: 0.3540\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.8719 - acc: 0.6338 - val_loss: 1.4918 - val_acc: 0.5861\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.60646 to 1.49183, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.7905 - acc: 0.6523 - val_loss: 1.6038 - val_acc: 0.5651\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.7245 - acc: 0.6772 - val_loss: 1.3947 - val_acc: 0.6218\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.49183 to 1.39466, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.6699 - acc: 0.6913 - val_loss: 1.4500 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.6401 - acc: 0.7115 - val_loss: 1.5529 - val_acc: 0.5809\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.5861 - acc: 0.7329 - val_loss: 1.2685 - val_acc: 0.6607\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.39466 to 1.26851, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.5161 - acc: 0.7526 - val_loss: 1.1542 - val_acc: 0.6796\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.26851 to 1.15420, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 33s 546ms/step - loss: 1.4981 - acc: 0.7577 - val_loss: 1.3656 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.4360 - acc: 0.7724 - val_loss: 1.1006 - val_acc: 0.7216\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.15420 to 1.10062, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.4218 - acc: 0.7793 - val_loss: 1.2422 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.3965 - acc: 0.7879 - val_loss: 1.2704 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.3710 - acc: 0.7992 - val_loss: 1.2117 - val_acc: 0.6691\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.3201 - acc: 0.8108 - val_loss: 1.0628 - val_acc: 0.7195\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.10062 to 1.06278, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.3212 - acc: 0.8110 - val_loss: 1.3061 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.2850 - acc: 0.8165 - val_loss: 1.0109 - val_acc: 0.7374\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.06278 to 1.01088, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.2661 - acc: 0.8302 - val_loss: 0.9885 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.01088 to 0.98846, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.2463 - acc: 0.8347 - val_loss: 1.0043 - val_acc: 0.7384\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.2334 - acc: 0.8351 - val_loss: 0.8816 - val_acc: 0.7584\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.98846 to 0.88159, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.1942 - acc: 0.8475 - val_loss: 1.0612 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.1999 - acc: 0.8459 - val_loss: 1.2850 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 1.1783 - acc: 0.8532 - val_loss: 0.9114 - val_acc: 0.7553\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.1517 - acc: 0.8569 - val_loss: 1.0775 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.1531 - acc: 0.8646 - val_loss: 1.0967 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.1341 - acc: 0.8714 - val_loss: 0.9698 - val_acc: 0.7553\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 32s 527ms/step - loss: 1.0672 - acc: 0.8941 - val_loss: 0.7912 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.88159 to 0.79116, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.0273 - acc: 0.8987 - val_loss: 0.7567 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.79116 to 0.75669, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0041 - acc: 0.9049 - val_loss: 0.7640 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 32s 531ms/step - loss: 1.0095 - acc: 0.9022 - val_loss: 0.7439 - val_acc: 0.8183\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.75669 to 0.74386, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 33s 549ms/step - loss: 1.0017 - acc: 0.9021 - val_loss: 0.7347 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.74386 to 0.73471, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.0114 - acc: 0.8991 - val_loss: 0.7444 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9978 - acc: 0.9053 - val_loss: 0.7511 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9985 - acc: 0.8984 - val_loss: 0.7395 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9785 - acc: 0.9025 - val_loss: 0.7367 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 0.9977 - acc: 0.8968 - val_loss: 0.7509 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 0.9830 - acc: 0.9027 - val_loss: 0.7366 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 0.9756 - acc: 0.9055 - val_loss: 0.7356 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 0.9734 - acc: 0.9014 - val_loss: 0.7358 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 0.9812 - acc: 0.9083 - val_loss: 0.7341 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.73471 to 0.73406, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 0.9637 - acc: 0.9056 - val_loss: 0.7308 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.73406 to 0.73084, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 0.9740 - acc: 0.8996 - val_loss: 0.7287 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.73084 to 0.72868, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9684 - acc: 0.9038 - val_loss: 0.7315 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9593 - acc: 0.9075 - val_loss: 0.7309 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 0.9738 - acc: 0.9082 - val_loss: 0.7281 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.72868 to 0.72809, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_2.h5\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 0.9767 - acc: 0.9012 - val_loss: 0.7290 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 0.9733 - acc: 0.9029 - val_loss: 0.7304 - val_acc: 0.8183\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9718 - acc: 0.9036 - val_loss: 0.7287 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 0.9750 - acc: 0.9029 - val_loss: 0.7294 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 0.9561 - acc: 0.9059 - val_loss: 0.7299 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 0.9691 - acc: 0.9060 - val_loss: 0.7307 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "60/60 [==============================] - 33s 552ms/step - loss: 0.9657 - acc: 0.9054 - val_loss: 0.7296 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 0.9587 - acc: 0.9103 - val_loss: 0.7301 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 0.9637 - acc: 0.9049 - val_loss: 0.7296 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 0.9745 - acc: 0.9087 - val_loss: 0.7295 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "9473/9473 [==============================] - 24s 3ms/step\n",
      "9400/9400 [==============================] - 23s 2ms/step\n",
      "train shape:  (8522, 256, 431, 1) (8522, 41)\n",
      "val shape:  (951, 256, 431, 1) (951, 41)\n",
      "Fold:  3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "60/60 [==============================] - 41s 682ms/step - loss: 3.2711 - acc: 0.1643 - val_loss: 7.0428 - val_acc: 0.1062\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.04285, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 29s 483ms/step - loss: 2.8152 - acc: 0.2910 - val_loss: 3.7773 - val_acc: 0.1598\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.04285 to 3.77731, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 29s 489ms/step - loss: 2.5028 - acc: 0.4033 - val_loss: 2.6103 - val_acc: 0.3060\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.77731 to 2.61033, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 30s 494ms/step - loss: 2.3301 - acc: 0.4635 - val_loss: 2.5643 - val_acc: 0.3502\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.61033 to 2.56431, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 30s 495ms/step - loss: 2.1498 - acc: 0.5297 - val_loss: 2.5775 - val_acc: 0.3649\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 30s 498ms/step - loss: 2.0462 - acc: 0.5711 - val_loss: 1.9752 - val_acc: 0.5068\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.56431 to 1.97523, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 30s 502ms/step - loss: 1.9332 - acc: 0.6033 - val_loss: 1.9506 - val_acc: 0.4995\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.97523 to 1.95057, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 31s 509ms/step - loss: 1.8454 - acc: 0.6310 - val_loss: 1.5920 - val_acc: 0.5678\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.95057 to 1.59201, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 31s 512ms/step - loss: 1.7678 - acc: 0.6691 - val_loss: 1.5975 - val_acc: 0.5878\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 30s 507ms/step - loss: 1.7394 - acc: 0.6766 - val_loss: 1.3692 - val_acc: 0.6288\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.59201 to 1.36917, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 30s 494ms/step - loss: 1.6708 - acc: 0.6939 - val_loss: 1.2875 - val_acc: 0.6519\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.36917 to 1.28746, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 29s 491ms/step - loss: 1.6290 - acc: 0.7056 - val_loss: 1.3311 - val_acc: 0.6498\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 29s 492ms/step - loss: 1.5411 - acc: 0.7435 - val_loss: 1.2390 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.28746 to 1.23903, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 30s 496ms/step - loss: 1.5308 - acc: 0.7486 - val_loss: 1.4343 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 30s 494ms/step - loss: 1.5030 - acc: 0.7539 - val_loss: 1.8128 - val_acc: 0.5247\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 30s 494ms/step - loss: 1.4419 - acc: 0.7734 - val_loss: 1.9184 - val_acc: 0.5258\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 29s 486ms/step - loss: 1.4020 - acc: 0.7871 - val_loss: 1.3764 - val_acc: 0.6540\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 29s 490ms/step - loss: 1.3835 - acc: 0.7951 - val_loss: 1.2709 - val_acc: 0.6709\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 29s 489ms/step - loss: 1.3835 - acc: 0.7931 - val_loss: 1.2286 - val_acc: 0.6951\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.23903 to 1.22856, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 30s 499ms/step - loss: 1.3445 - acc: 0.8124 - val_loss: 1.1211 - val_acc: 0.7140\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.22856 to 1.12106, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 30s 495ms/step - loss: 1.3213 - acc: 0.8106 - val_loss: 1.0923 - val_acc: 0.7234\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.12106 to 1.09227, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 30s 498ms/step - loss: 1.2819 - acc: 0.8285 - val_loss: 1.0353 - val_acc: 0.7424\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.09227 to 1.03526, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 30s 493ms/step - loss: 1.2707 - acc: 0.8300 - val_loss: 1.3088 - val_acc: 0.6751\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 30s 497ms/step - loss: 1.2376 - acc: 0.8394 - val_loss: 1.1931 - val_acc: 0.6972\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 30s 501ms/step - loss: 1.2166 - acc: 0.8391 - val_loss: 0.9483 - val_acc: 0.7497\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.03526 to 0.94834, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 30s 501ms/step - loss: 1.2149 - acc: 0.8491 - val_loss: 1.0029 - val_acc: 0.7371\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 30s 499ms/step - loss: 1.2132 - acc: 0.8539 - val_loss: 1.1404 - val_acc: 0.7077\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 30s 499ms/step - loss: 1.1623 - acc: 0.8668 - val_loss: 1.3428 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 30s 507ms/step - loss: 1.1608 - acc: 0.8620 - val_loss: 1.0645 - val_acc: 0.7361\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 1.1638 - acc: 0.8568 - val_loss: 1.0760 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 30s 501ms/step - loss: 1.1409 - acc: 0.8692 - val_loss: 1.0377 - val_acc: 0.7308\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 29s 481ms/step - loss: 1.0685 - acc: 0.8911 - val_loss: 0.7732 - val_acc: 0.8023\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.94834 to 0.77324, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 30s 507ms/step - loss: 1.0232 - acc: 0.8985 - val_loss: 0.7305 - val_acc: 0.8202\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.77324 to 0.73051, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 30s 500ms/step - loss: 1.0133 - acc: 0.8968 - val_loss: 0.7310 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 1.0048 - acc: 0.9031 - val_loss: 0.7324 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 30s 499ms/step - loss: 1.0051 - acc: 0.9014 - val_loss: 0.7366 - val_acc: 0.8181\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 0.9767 - acc: 0.9055 - val_loss: 0.7288 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.73051 to 0.72883, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 30s 502ms/step - loss: 0.9812 - acc: 0.9043 - val_loss: 0.7325 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 30s 505ms/step - loss: 0.9786 - acc: 0.9043 - val_loss: 0.7187 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.72883 to 0.71870, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 30s 501ms/step - loss: 0.9939 - acc: 0.9025 - val_loss: 0.7189 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 30s 504ms/step - loss: 0.9713 - acc: 0.9070 - val_loss: 0.7158 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.71870 to 0.71585, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 0.9858 - acc: 0.9000 - val_loss: 0.7230 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 0.9919 - acc: 0.9016 - val_loss: 0.7274 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 30s 504ms/step - loss: 0.9782 - acc: 0.9001 - val_loss: 0.7224 - val_acc: 0.8181\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 30s 499ms/step - loss: 0.9770 - acc: 0.9063 - val_loss: 0.7387 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 31s 509ms/step - loss: 0.9766 - acc: 0.9050 - val_loss: 0.7354 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 30s 502ms/step - loss: 0.9759 - acc: 0.9059 - val_loss: 0.7228 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 0.9738 - acc: 0.9008 - val_loss: 0.7140 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.71585 to 0.71399, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 30s 507ms/step - loss: 0.9612 - acc: 0.9043 - val_loss: 0.7094 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.71399 to 0.70938, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 30s 504ms/step - loss: 0.9557 - acc: 0.9094 - val_loss: 0.7051 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.70938 to 0.70514, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 30s 500ms/step - loss: 0.9682 - acc: 0.9025 - val_loss: 0.7074 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "60/60 [==============================] - 30s 507ms/step - loss: 0.9581 - acc: 0.9069 - val_loss: 0.7070 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "60/60 [==============================] - 30s 502ms/step - loss: 0.9662 - acc: 0.9061 - val_loss: 0.7072 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "60/60 [==============================] - 30s 506ms/step - loss: 0.9507 - acc: 0.9121 - val_loss: 0.7083 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "60/60 [==============================] - 30s 504ms/step - loss: 0.9772 - acc: 0.9029 - val_loss: 0.7069 - val_acc: 0.8202\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 0.9458 - acc: 0.9066 - val_loss: 0.7045 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.70514 to 0.70451, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_3.h5\n",
      "Epoch 57/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 0.9547 - acc: 0.9046 - val_loss: 0.7065 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "60/60 [==============================] - 30s 506ms/step - loss: 0.9414 - acc: 0.9114 - val_loss: 0.7086 - val_acc: 0.8202\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "60/60 [==============================] - 30s 505ms/step - loss: 0.9611 - acc: 0.9006 - val_loss: 0.7060 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "60/60 [==============================] - 30s 504ms/step - loss: 0.9595 - acc: 0.9045 - val_loss: 0.7097 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "60/60 [==============================] - 30s 501ms/step - loss: 0.9607 - acc: 0.9083 - val_loss: 0.7089 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/300\n",
      "60/60 [==============================] - 30s 505ms/step - loss: 0.9608 - acc: 0.9007 - val_loss: 0.7062 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/300\n",
      "60/60 [==============================] - 30s 506ms/step - loss: 0.9506 - acc: 0.9049 - val_loss: 0.7064 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 0.9412 - acc: 0.9106 - val_loss: 0.7067 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "60/60 [==============================] - 30s 504ms/step - loss: 0.9514 - acc: 0.9074 - val_loss: 0.7071 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/300\n",
      "60/60 [==============================] - 30s 506ms/step - loss: 0.9569 - acc: 0.9069 - val_loss: 0.7062 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "9473/9473 [==============================] - 25s 3ms/step\n",
      "9400/9400 [==============================] - 22s 2ms/step\n",
      "train shape:  (8523, 256, 431, 1) (8523, 41)\n",
      "val shape:  (950, 256, 431, 1) (950, 41)\n",
      "Fold:  4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_8 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "60/60 [==============================] - 42s 693ms/step - loss: 3.2289 - acc: 0.1734 - val_loss: 7.2313 - val_acc: 0.0926\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.23132, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 30s 506ms/step - loss: 2.8307 - acc: 0.2782 - val_loss: 4.5855 - val_acc: 0.1326\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.23132 to 4.58552, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 2.5860 - acc: 0.3636 - val_loss: 6.7452 - val_acc: 0.0832\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 2.3772 - acc: 0.4467 - val_loss: 2.8282 - val_acc: 0.3263\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.58552 to 2.82824, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 2.2360 - acc: 0.5004 - val_loss: 2.7127 - val_acc: 0.3495\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.82824 to 2.71271, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 2.1015 - acc: 0.5469 - val_loss: 2.0823 - val_acc: 0.4568\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.71271 to 2.08233, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.9803 - acc: 0.5940 - val_loss: 2.4288 - val_acc: 0.3842\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.8834 - acc: 0.6242 - val_loss: 2.3569 - val_acc: 0.3874\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.8002 - acc: 0.6508 - val_loss: 2.5279 - val_acc: 0.3526\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.7375 - acc: 0.6749 - val_loss: 2.5395 - val_acc: 0.3579\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.6991 - acc: 0.6911 - val_loss: 1.2994 - val_acc: 0.6579\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.08233 to 1.29941, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.6495 - acc: 0.7062 - val_loss: 1.3704 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.5744 - acc: 0.7302 - val_loss: 1.5268 - val_acc: 0.5989\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 32s 526ms/step - loss: 1.5388 - acc: 0.7414 - val_loss: 1.1853 - val_acc: 0.6726\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.29941 to 1.18530, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.5030 - acc: 0.7550 - val_loss: 1.1668 - val_acc: 0.7011\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.18530 to 1.16681, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.4532 - acc: 0.7710 - val_loss: 1.2155 - val_acc: 0.6863\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.4032 - acc: 0.7859 - val_loss: 1.3910 - val_acc: 0.6316\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.4026 - acc: 0.7820 - val_loss: 1.3585 - val_acc: 0.6495\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.3699 - acc: 0.7977 - val_loss: 1.2253 - val_acc: 0.6821\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.3519 - acc: 0.8097 - val_loss: 1.4354 - val_acc: 0.6126\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.3121 - acc: 0.8176 - val_loss: 1.1620 - val_acc: 0.6989\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.16681 to 1.16198, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.3047 - acc: 0.8198 - val_loss: 1.3692 - val_acc: 0.6505\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 32s 531ms/step - loss: 1.2712 - acc: 0.8282 - val_loss: 1.1366 - val_acc: 0.7158\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.16198 to 1.13658, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.2624 - acc: 0.8316 - val_loss: 1.3469 - val_acc: 0.6611\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.2429 - acc: 0.8464 - val_loss: 1.2265 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.2112 - acc: 0.8492 - val_loss: 1.4977 - val_acc: 0.6211\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.2180 - acc: 0.8487 - val_loss: 1.0924 - val_acc: 0.7442\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.13658 to 1.09237, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.1630 - acc: 0.8583 - val_loss: 1.3027 - val_acc: 0.6663\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.1603 - acc: 0.8615 - val_loss: 1.0492 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.09237 to 1.04923, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.1419 - acc: 0.8695 - val_loss: 1.2081 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.1414 - acc: 0.8701 - val_loss: 1.1205 - val_acc: 0.7232\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.1192 - acc: 0.8731 - val_loss: 1.3329 - val_acc: 0.6811\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.1052 - acc: 0.8763 - val_loss: 1.7178 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.1080 - acc: 0.8775 - val_loss: 1.0475 - val_acc: 0.7432\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.04923 to 1.04753, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0989 - acc: 0.8781 - val_loss: 1.0195 - val_acc: 0.7442\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.04753 to 1.01947, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.0782 - acc: 0.8824 - val_loss: 1.1445 - val_acc: 0.7242\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0647 - acc: 0.8880 - val_loss: 0.9568 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.01947 to 0.95681, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.0558 - acc: 0.8926 - val_loss: 1.0803 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.0630 - acc: 0.8888 - val_loss: 1.0226 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.0468 - acc: 0.8907 - val_loss: 1.1068 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0461 - acc: 0.8878 - val_loss: 1.1009 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0292 - acc: 0.8918 - val_loss: 1.0404 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 32s 531ms/step - loss: 1.0224 - acc: 0.8939 - val_loss: 1.1143 - val_acc: 0.7232\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 31s 516ms/step - loss: 0.9773 - acc: 0.9049 - val_loss: 0.8980 - val_acc: 0.7747\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.95681 to 0.89798, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9476 - acc: 0.9063 - val_loss: 0.8299 - val_acc: 0.7926\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.89798 to 0.82990, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9443 - acc: 0.9096 - val_loss: 0.8184 - val_acc: 0.7874\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.82990 to 0.81841, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 0.9331 - acc: 0.9136 - val_loss: 0.8199 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9348 - acc: 0.9110 - val_loss: 0.8202 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 0.9243 - acc: 0.9115 - val_loss: 0.8080 - val_acc: 0.7979\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.81841 to 0.80798, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 0.9207 - acc: 0.9050 - val_loss: 0.8221 - val_acc: 0.7884\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 0.9224 - acc: 0.9076 - val_loss: 0.8107 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 0.9204 - acc: 0.9114 - val_loss: 0.8058 - val_acc: 0.7979\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.80798 to 0.80582, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 53/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 0.9160 - acc: 0.9093 - val_loss: 0.8036 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.80582 to 0.80363, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 54/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9233 - acc: 0.9104 - val_loss: 0.8131 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 0.9101 - acc: 0.9174 - val_loss: 0.8016 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.80363 to 0.80163, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_4.h5\n",
      "Epoch 56/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 0.9107 - acc: 0.9124 - val_loss: 0.8129 - val_acc: 0.7979\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 0.9113 - acc: 0.9110 - val_loss: 0.8051 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 0.9155 - acc: 0.9087 - val_loss: 0.8150 - val_acc: 0.7895\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9128 - acc: 0.9065 - val_loss: 0.8171 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 0.9130 - acc: 0.9087 - val_loss: 0.8246 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 0.9104 - acc: 0.9101 - val_loss: 0.8190 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 0.9115 - acc: 0.9121 - val_loss: 0.8054 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 0.9016 - acc: 0.9049 - val_loss: 0.8048 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 0.9036 - acc: 0.9113 - val_loss: 0.8086 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 0.9063 - acc: 0.9116 - val_loss: 0.8101 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "9473/9473 [==============================] - 23s 2ms/step\n",
      "9400/9400 [==============================] - 21s 2ms/step\n",
      "train shape:  (8527, 256, 431, 1) (8527, 41)\n",
      "val shape:  (946, 256, 431, 1) (946, 41)\n",
      "Fold:  5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_9 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "60/60 [==============================] - 40s 674ms/step - loss: 3.3644 - acc: 0.1317 - val_loss: 5.8455 - val_acc: 0.1068\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.84551, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 29s 480ms/step - loss: 2.9772 - acc: 0.2358 - val_loss: 5.1645 - val_acc: 0.1374\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.84551 to 5.16449, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 30s 503ms/step - loss: 2.7169 - acc: 0.3272 - val_loss: 4.4952 - val_acc: 0.1469\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.16449 to 4.49520, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 31s 512ms/step - loss: 2.4772 - acc: 0.4068 - val_loss: 3.1242 - val_acc: 0.2178\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.49520 to 3.12423, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 31s 511ms/step - loss: 2.3192 - acc: 0.4620 - val_loss: 2.2273 - val_acc: 0.4059\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.12423 to 2.22730, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 31s 523ms/step - loss: 2.1637 - acc: 0.5246 - val_loss: 2.9995 - val_acc: 0.2833\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 32s 528ms/step - loss: 2.0318 - acc: 0.5685 - val_loss: 2.4102 - val_acc: 0.3985\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 31s 523ms/step - loss: 1.9350 - acc: 0.6090 - val_loss: 2.2045 - val_acc: 0.4123\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.22730 to 2.20445, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 32s 528ms/step - loss: 1.8423 - acc: 0.6390 - val_loss: 1.6826 - val_acc: 0.5433\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.20445 to 1.68258, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 31s 525ms/step - loss: 1.7596 - acc: 0.6683 - val_loss: 1.6215 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.68258 to 1.62145, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 31s 522ms/step - loss: 1.6951 - acc: 0.6837 - val_loss: 2.4058 - val_acc: 0.4080\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.6500 - acc: 0.7070 - val_loss: 1.4765 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.62145 to 1.47654, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 32s 527ms/step - loss: 1.5819 - acc: 0.7182 - val_loss: 1.5250 - val_acc: 0.5962\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.5262 - acc: 0.7481 - val_loss: 1.4134 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.47654 to 1.41337, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.5081 - acc: 0.7595 - val_loss: 1.3417 - val_acc: 0.6522\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.41337 to 1.34170, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.4767 - acc: 0.7681 - val_loss: 1.3528 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.4328 - acc: 0.7721 - val_loss: 1.3525 - val_acc: 0.6353\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.3959 - acc: 0.7906 - val_loss: 0.9811 - val_acc: 0.7357\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.34170 to 0.98108, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.3668 - acc: 0.7931 - val_loss: 1.1182 - val_acc: 0.7051\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.3410 - acc: 0.8067 - val_loss: 0.9667 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.98108 to 0.96672, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.3145 - acc: 0.8163 - val_loss: 1.1260 - val_acc: 0.7104\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.2967 - acc: 0.8187 - val_loss: 1.0514 - val_acc: 0.7220\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.2700 - acc: 0.8255 - val_loss: 0.9628 - val_acc: 0.7389\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.96672 to 0.96284, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.2651 - acc: 0.8338 - val_loss: 1.0980 - val_acc: 0.7230\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.2267 - acc: 0.8440 - val_loss: 1.0857 - val_acc: 0.7347\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.2210 - acc: 0.8518 - val_loss: 1.0736 - val_acc: 0.7230\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.2065 - acc: 0.8468 - val_loss: 1.1205 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.1821 - acc: 0.8597 - val_loss: 0.9883 - val_acc: 0.7474\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.1579 - acc: 0.8643 - val_loss: 0.9744 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 30s 506ms/step - loss: 1.0890 - acc: 0.8831 - val_loss: 0.7497 - val_acc: 0.8076\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.96284 to 0.74974, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 32s 531ms/step - loss: 1.0479 - acc: 0.8966 - val_loss: 0.7483 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.74974 to 0.74830, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0393 - acc: 0.9005 - val_loss: 0.7403 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.74830 to 0.74026, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0234 - acc: 0.9021 - val_loss: 0.7230 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.74026 to 0.72300, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_5.h5\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0230 - acc: 0.8908 - val_loss: 0.7251 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.0163 - acc: 0.8971 - val_loss: 0.7241 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0141 - acc: 0.9055 - val_loss: 0.7340 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0057 - acc: 0.9019 - val_loss: 0.7474 - val_acc: 0.8150\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0159 - acc: 0.8985 - val_loss: 0.7495 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 32s 531ms/step - loss: 0.9938 - acc: 0.9012 - val_loss: 0.7482 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9872 - acc: 0.9014 - val_loss: 0.7293 - val_acc: 0.8171\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 0.9896 - acc: 0.9061 - val_loss: 0.7262 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9890 - acc: 0.9025 - val_loss: 0.7234 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9875 - acc: 0.9040 - val_loss: 0.7242 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "9473/9473 [==============================] - 24s 3ms/step\n",
      "9400/9400 [==============================] - 21s 2ms/step\n",
      "train shape:  (8528, 256, 431, 1) (8528, 41)\n",
      "val shape:  (945, 256, 431, 1) (945, 41)\n",
      "Fold:  6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_55 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_56 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_10  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "60/60 [==============================] - 42s 699ms/step - loss: 3.2817 - acc: 0.1486 - val_loss: 3.4791 - val_acc: 0.1101\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.47914, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 30s 496ms/step - loss: 2.8923 - acc: 0.2619 - val_loss: 3.3497 - val_acc: 0.2116\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.47914 to 3.34974, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 2.5942 - acc: 0.3619 - val_loss: 6.1113 - val_acc: 0.1280\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 32s 529ms/step - loss: 2.4287 - acc: 0.4312 - val_loss: 4.2409 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 2.2156 - acc: 0.4989 - val_loss: 2.5360 - val_acc: 0.3894\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.34974 to 2.53601, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 33s 554ms/step - loss: 2.0986 - acc: 0.5496 - val_loss: 2.2073 - val_acc: 0.4212\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.53601 to 2.20733, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.9649 - acc: 0.5912 - val_loss: 1.5048 - val_acc: 0.5862\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.20733 to 1.50476, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 33s 552ms/step - loss: 1.9022 - acc: 0.6138 - val_loss: 1.8928 - val_acc: 0.5069\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 33s 547ms/step - loss: 1.8135 - acc: 0.6512 - val_loss: 1.5049 - val_acc: 0.6116\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 33s 546ms/step - loss: 1.7445 - acc: 0.6697 - val_loss: 1.4876 - val_acc: 0.5884\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.50476 to 1.48757, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 33s 551ms/step - loss: 1.6784 - acc: 0.6942 - val_loss: 1.4535 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.48757 to 1.45347, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.6405 - acc: 0.7067 - val_loss: 1.2531 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.45347 to 1.25313, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 33s 557ms/step - loss: 1.5828 - acc: 0.7188 - val_loss: 1.4230 - val_acc: 0.6074\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 33s 551ms/step - loss: 1.5510 - acc: 0.7366 - val_loss: 1.3086 - val_acc: 0.6529\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.4875 - acc: 0.7523 - val_loss: 1.0943 - val_acc: 0.7069\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.25313 to 1.09428, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.4647 - acc: 0.7671 - val_loss: 1.0888 - val_acc: 0.7153\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.09428 to 1.08882, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 33s 549ms/step - loss: 1.4401 - acc: 0.7771 - val_loss: 1.2502 - val_acc: 0.6741\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 33s 549ms/step - loss: 1.4090 - acc: 0.7842 - val_loss: 1.1249 - val_acc: 0.7132\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 33s 546ms/step - loss: 1.3742 - acc: 0.7899 - val_loss: 1.2645 - val_acc: 0.6794\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 33s 553ms/step - loss: 1.3526 - acc: 0.7980 - val_loss: 1.4082 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.3377 - acc: 0.8080 - val_loss: 0.8971 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.08882 to 0.89706, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.2992 - acc: 0.8141 - val_loss: 1.0169 - val_acc: 0.7397\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.2918 - acc: 0.8223 - val_loss: 0.9546 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.2489 - acc: 0.8347 - val_loss: 0.9066 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.2323 - acc: 0.8427 - val_loss: 1.2665 - val_acc: 0.6677\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.2319 - acc: 0.8384 - val_loss: 1.0311 - val_acc: 0.7185\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.1876 - acc: 0.8588 - val_loss: 1.0352 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 30s 502ms/step - loss: 1.1069 - acc: 0.8776 - val_loss: 0.7381 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.89706 to 0.73813, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.0879 - acc: 0.8851 - val_loss: 0.7135 - val_acc: 0.8339\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.73813 to 0.71349, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.0724 - acc: 0.8874 - val_loss: 0.6850 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.71349 to 0.68500, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.0599 - acc: 0.8867 - val_loss: 0.6976 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0467 - acc: 0.8917 - val_loss: 0.6887 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 32s 531ms/step - loss: 1.0493 - acc: 0.8885 - val_loss: 0.6824 - val_acc: 0.8444\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.68500 to 0.68235, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0385 - acc: 0.8891 - val_loss: 0.6898 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.0254 - acc: 0.8908 - val_loss: 0.6769 - val_acc: 0.8434\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.68235 to 0.67687, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.0408 - acc: 0.8910 - val_loss: 0.6719 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.67687 to 0.67190, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0302 - acc: 0.8981 - val_loss: 0.6828 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.0285 - acc: 0.8994 - val_loss: 0.6797 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.0348 - acc: 0.8947 - val_loss: 0.6887 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0217 - acc: 0.8977 - val_loss: 0.6872 - val_acc: 0.8349\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.0212 - acc: 0.8899 - val_loss: 0.6773 - val_acc: 0.8466\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.0272 - acc: 0.8921 - val_loss: 0.6876 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0028 - acc: 0.8992 - val_loss: 0.6728 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0102 - acc: 0.8965 - val_loss: 0.6692 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.67190 to 0.66916, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0063 - acc: 0.9025 - val_loss: 0.6691 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.66916 to 0.66908, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 0.9921 - acc: 0.9001 - val_loss: 0.6698 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0135 - acc: 0.8954 - val_loss: 0.6703 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.0116 - acc: 0.9032 - val_loss: 0.6723 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.0021 - acc: 0.9004 - val_loss: 0.6714 - val_acc: 0.8402\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0000 - acc: 0.8950 - val_loss: 0.6689 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.66908 to 0.66887, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_6.h5\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.0088 - acc: 0.8967 - val_loss: 0.6721 - val_acc: 0.8434\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 0.9947 - acc: 0.9033 - val_loss: 0.6700 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "60/60 [==============================] - 32s 529ms/step - loss: 0.9970 - acc: 0.9027 - val_loss: 0.6694 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.0062 - acc: 0.9015 - val_loss: 0.6709 - val_acc: 0.8444\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9996 - acc: 0.8966 - val_loss: 0.6740 - val_acc: 0.8455\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 32s 533ms/step - loss: 0.9982 - acc: 0.8994 - val_loss: 0.6682 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0032 - acc: 0.8934 - val_loss: 0.6684 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0017 - acc: 0.8987 - val_loss: 0.6687 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/300\n",
      "60/60 [==============================] - 32s 531ms/step - loss: 0.9974 - acc: 0.9020 - val_loss: 0.6682 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 0.9884 - acc: 0.8962 - val_loss: 0.6688 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 0.9958 - acc: 0.9031 - val_loss: 0.6690 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0060 - acc: 0.8965 - val_loss: 0.6693 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.0024 - acc: 0.8991 - val_loss: 0.6691 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.0029 - acc: 0.9014 - val_loss: 0.6691 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9925 - acc: 0.8999 - val_loss: 0.6698 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "9473/9473 [==============================] - 24s 3ms/step\n",
      "9400/9400 [==============================] - 21s 2ms/step\n",
      "train shape:  (8530, 256, 431, 1) (8530, 41)\n",
      "val shape:  (943, 256, 431, 1) (943, 41)\n",
      "Fold:  7\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_62 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_11  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "60/60 [==============================] - 37s 623ms/step - loss: 3.2616 - acc: 0.1630 - val_loss: 5.7703 - val_acc: 0.1124\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.77030, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 33s 549ms/step - loss: 2.8216 - acc: 0.2867 - val_loss: 2.8985 - val_acc: 0.2322\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.77030 to 2.89850, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 33s 555ms/step - loss: 2.5452 - acc: 0.3864 - val_loss: 4.3103 - val_acc: 0.1654\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 2.3351 - acc: 0.4619 - val_loss: 2.4128 - val_acc: 0.4252\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.89850 to 2.41280, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 2.1864 - acc: 0.5202 - val_loss: 2.4785 - val_acc: 0.3584\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 2.0605 - acc: 0.5610 - val_loss: 2.1459 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.41280 to 2.14590, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.9372 - acc: 0.6029 - val_loss: 2.2075 - val_acc: 0.4284\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.8522 - acc: 0.6360 - val_loss: 1.4242 - val_acc: 0.6288\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.14590 to 1.42418, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.7713 - acc: 0.6561 - val_loss: 2.0703 - val_acc: 0.4517\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.7146 - acc: 0.6777 - val_loss: 1.8718 - val_acc: 0.5069\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.6793 - acc: 0.6952 - val_loss: 1.6190 - val_acc: 0.5514\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 32s 535ms/step - loss: 1.6231 - acc: 0.7089 - val_loss: 1.8342 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.5443 - acc: 0.7418 - val_loss: 1.1996 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.42418 to 1.19956, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.5210 - acc: 0.7455 - val_loss: 1.1123 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.19956 to 1.11226, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.4724 - acc: 0.7665 - val_loss: 1.9904 - val_acc: 0.4867\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.4395 - acc: 0.7696 - val_loss: 1.2420 - val_acc: 0.6713\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.4222 - acc: 0.7751 - val_loss: 1.2299 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.3814 - acc: 0.7959 - val_loss: 1.7516 - val_acc: 0.5323\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.3588 - acc: 0.7971 - val_loss: 1.5643 - val_acc: 0.6416\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.3271 - acc: 0.8080 - val_loss: 1.0825 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.11226 to 1.08255, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.2979 - acc: 0.8191 - val_loss: 1.2829 - val_acc: 0.6649\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.2681 - acc: 0.8246 - val_loss: 1.1458 - val_acc: 0.7010\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.2472 - acc: 0.8378 - val_loss: 1.0731 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.08255 to 1.07312, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.2399 - acc: 0.8349 - val_loss: 1.2391 - val_acc: 0.6734\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.2111 - acc: 0.8401 - val_loss: 1.0011 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.07312 to 1.00115, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.1925 - acc: 0.8539 - val_loss: 1.0912 - val_acc: 0.7137\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.1723 - acc: 0.8623 - val_loss: 1.4398 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.1763 - acc: 0.8627 - val_loss: 1.2315 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.1477 - acc: 0.8612 - val_loss: 1.1988 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.1325 - acc: 0.8708 - val_loss: 1.0486 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.1361 - acc: 0.8674 - val_loss: 1.1838 - val_acc: 0.7169\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 30s 502ms/step - loss: 1.0621 - acc: 0.8908 - val_loss: 0.7442 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.00115 to 0.74421, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0209 - acc: 0.8958 - val_loss: 0.7412 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.74421 to 0.74117, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0129 - acc: 0.8969 - val_loss: 0.7294 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.74117 to 0.72939, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9851 - acc: 0.9032 - val_loss: 0.7449 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0023 - acc: 0.9023 - val_loss: 0.7532 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0071 - acc: 0.9004 - val_loss: 0.7464 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9830 - acc: 0.9092 - val_loss: 0.7471 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 0.9803 - acc: 0.9036 - val_loss: 0.7475 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 0.9825 - acc: 0.9038 - val_loss: 0.7518 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9797 - acc: 0.9008 - val_loss: 0.7312 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 0.9704 - acc: 0.9074 - val_loss: 0.7264 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.72939 to 0.72645, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_7.h5\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 0.9726 - acc: 0.9038 - val_loss: 0.7304 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9764 - acc: 0.9096 - val_loss: 0.7287 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 0.9758 - acc: 0.9060 - val_loss: 0.7290 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 0.9762 - acc: 0.9048 - val_loss: 0.7302 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9781 - acc: 0.9067 - val_loss: 0.7282 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 0.9897 - acc: 0.9025 - val_loss: 0.7309 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9690 - acc: 0.9101 - val_loss: 0.7312 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 0.9781 - acc: 0.9056 - val_loss: 0.7305 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 0.9705 - acc: 0.9047 - val_loss: 0.7300 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 0.9685 - acc: 0.9072 - val_loss: 0.7306 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "9473/9473 [==============================] - 24s 3ms/step\n",
      "9400/9400 [==============================] - 21s 2ms/step\n",
      "train shape:  (8531, 256, 431, 1) (8531, 41)\n",
      "val shape:  (942, 256, 431, 1) (942, 41)\n",
      "Fold:  8\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_67 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_68 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_12  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "60/60 [==============================] - 41s 691ms/step - loss: 3.2476 - acc: 0.1601 - val_loss: 4.5841 - val_acc: 0.1348\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.58406, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 31s 510ms/step - loss: 2.8303 - acc: 0.2791 - val_loss: 4.1424 - val_acc: 0.2017\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.58406 to 4.14242, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 2.5150 - acc: 0.3938 - val_loss: 3.1363 - val_acc: 0.3079\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.14242 to 3.13627, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 32s 526ms/step - loss: 2.3542 - acc: 0.4494 - val_loss: 2.1012 - val_acc: 0.4586\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.13627 to 2.10118, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 32s 529ms/step - loss: 2.1810 - acc: 0.5230 - val_loss: 2.0577 - val_acc: 0.4745\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.10118 to 2.05771, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 2.0727 - acc: 0.5633 - val_loss: 2.1108 - val_acc: 0.4660\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 33s 547ms/step - loss: 1.9866 - acc: 0.5830 - val_loss: 1.7093 - val_acc: 0.5329\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.05771 to 1.70933, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.8795 - acc: 0.6279 - val_loss: 1.4769 - val_acc: 0.5987\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.70933 to 1.47688, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.7737 - acc: 0.6587 - val_loss: 1.9202 - val_acc: 0.5340\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.7320 - acc: 0.6795 - val_loss: 1.9783 - val_acc: 0.5085\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.6461 - acc: 0.7026 - val_loss: 2.0121 - val_acc: 0.4915\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.6047 - acc: 0.7224 - val_loss: 2.1486 - val_acc: 0.5191\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.5636 - acc: 0.7313 - val_loss: 1.5611 - val_acc: 0.5987\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.5189 - acc: 0.7454 - val_loss: 1.1613 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.47688 to 1.16128, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.4868 - acc: 0.7573 - val_loss: 1.5074 - val_acc: 0.6476\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 33s 547ms/step - loss: 1.4483 - acc: 0.7648 - val_loss: 1.3740 - val_acc: 0.6635\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.4067 - acc: 0.7823 - val_loss: 1.2458 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.3940 - acc: 0.7906 - val_loss: 1.6803 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.3459 - acc: 0.8028 - val_loss: 1.1134 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.16128 to 1.11340, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 1.3316 - acc: 0.8130 - val_loss: 1.6625 - val_acc: 0.5870\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.2963 - acc: 0.8178 - val_loss: 1.2365 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 33s 549ms/step - loss: 1.2927 - acc: 0.8258 - val_loss: 1.2466 - val_acc: 0.6985\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.2532 - acc: 0.8354 - val_loss: 1.2321 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.2171 - acc: 0.8461 - val_loss: 1.4411 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.2307 - acc: 0.8459 - val_loss: 1.2670 - val_acc: 0.6837\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 30s 501ms/step - loss: 1.1269 - acc: 0.8711 - val_loss: 0.8507 - val_acc: 0.7845\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.11340 to 0.85069, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0986 - acc: 0.8837 - val_loss: 0.8358 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.85069 to 0.83577, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0796 - acc: 0.8890 - val_loss: 0.8385 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.0604 - acc: 0.8961 - val_loss: 0.8273 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.83577 to 0.82733, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0723 - acc: 0.8863 - val_loss: 0.8146 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.82733 to 0.81458, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0509 - acc: 0.8933 - val_loss: 0.8273 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.0638 - acc: 0.8883 - val_loss: 0.8259 - val_acc: 0.8015\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 33s 547ms/step - loss: 1.0511 - acc: 0.8884 - val_loss: 0.8603 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.0568 - acc: 0.8871 - val_loss: 0.8173 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 33s 547ms/step - loss: 1.0492 - acc: 0.8946 - val_loss: 0.8200 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.0473 - acc: 0.8921 - val_loss: 0.8169 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 1.0288 - acc: 0.8998 - val_loss: 0.8064 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.81458 to 0.80644, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0290 - acc: 0.8944 - val_loss: 0.8050 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.80644 to 0.80504, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 39/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.0274 - acc: 0.8910 - val_loss: 0.8012 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.80504 to 0.80124, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 40/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 33s 543ms/step - loss: 1.0281 - acc: 0.9000 - val_loss: 0.8006 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.80124 to 0.80057, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.0097 - acc: 0.8937 - val_loss: 0.7950 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.80057 to 0.79503, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_8.h5\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.0193 - acc: 0.8989 - val_loss: 0.7981 - val_acc: 0.8068\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0320 - acc: 0.8904 - val_loss: 0.7980 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.0107 - acc: 0.8986 - val_loss: 0.7964 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 33s 544ms/step - loss: 1.0316 - acc: 0.8957 - val_loss: 0.7968 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0191 - acc: 0.8992 - val_loss: 0.7969 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0186 - acc: 0.8994 - val_loss: 0.8033 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.0327 - acc: 0.9011 - val_loss: 0.8000 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0279 - acc: 0.8973 - val_loss: 0.7988 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "60/60 [==============================] - 33s 545ms/step - loss: 1.0202 - acc: 0.8932 - val_loss: 0.7980 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.0098 - acc: 0.8974 - val_loss: 0.7976 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "9473/9473 [==============================] - 27s 3ms/step\n",
      "9400/9400 [==============================] - 23s 2ms/step\n",
      "train shape:  (8537, 256, 431, 1) (8537, 41)\n",
      "val shape:  (936, 256, 431, 1) (936, 41)\n",
      "Fold:  9\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_76 (MaxPooling (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_77 (MaxPooling (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_78 (MaxPooling (None, 4, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_13  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "27\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "60/60 [==============================] - 43s 716ms/step - loss: 3.2111 - acc: 0.1779 - val_loss: 6.3866 - val_acc: 0.0769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.38663, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 2/300\n",
      "60/60 [==============================] - 31s 511ms/step - loss: 2.7807 - acc: 0.3006 - val_loss: 3.1140 - val_acc: 0.2650\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.38663 to 3.11401, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 3/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 2.5005 - acc: 0.4023 - val_loss: 3.3100 - val_acc: 0.2660\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 2.3118 - acc: 0.4670 - val_loss: 2.1763 - val_acc: 0.4306\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.11401 to 2.17625, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 5/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 2.1555 - acc: 0.5345 - val_loss: 1.9496 - val_acc: 0.4712\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.17625 to 1.94964, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 6/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 2.0429 - acc: 0.5676 - val_loss: 1.8204 - val_acc: 0.5011\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.94964 to 1.82045, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 7/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.9323 - acc: 0.6178 - val_loss: 1.7747 - val_acc: 0.5075\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.82045 to 1.77474, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 8/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.8677 - acc: 0.6336 - val_loss: 1.7372 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.77474 to 1.73723, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 9/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.7885 - acc: 0.6566 - val_loss: 1.5470 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.73723 to 1.54697, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 10/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.7228 - acc: 0.6833 - val_loss: 1.4460 - val_acc: 0.6143\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.54697 to 1.44601, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 11/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.6577 - acc: 0.7014 - val_loss: 1.5417 - val_acc: 0.6079\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.5894 - acc: 0.7279 - val_loss: 1.7606 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.5460 - acc: 0.7396 - val_loss: 1.3075 - val_acc: 0.6560\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.44601 to 1.30750, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 14/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.5142 - acc: 0.7466 - val_loss: 1.2910 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.30750 to 1.29102, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 15/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.4865 - acc: 0.7624 - val_loss: 1.1196 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.29102 to 1.11958, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 16/300\n",
      "60/60 [==============================] - 32s 532ms/step - loss: 1.4438 - acc: 0.7737 - val_loss: 1.5231 - val_acc: 0.5962\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.3968 - acc: 0.7890 - val_loss: 1.0149 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.11958 to 1.01491, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 18/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.3826 - acc: 0.7903 - val_loss: 1.1997 - val_acc: 0.6955\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.3423 - acc: 0.8108 - val_loss: 1.3766 - val_acc: 0.6592\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.3221 - acc: 0.8102 - val_loss: 1.0742 - val_acc: 0.7276\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.2972 - acc: 0.8275 - val_loss: 0.9088 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.01491 to 0.90880, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 22/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.2775 - acc: 0.8284 - val_loss: 1.1144 - val_acc: 0.7169\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.2550 - acc: 0.8329 - val_loss: 1.2135 - val_acc: 0.6870\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "60/60 [==============================] - 32s 538ms/step - loss: 1.2317 - acc: 0.8418 - val_loss: 1.2381 - val_acc: 0.6827\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.2243 - acc: 0.8427 - val_loss: 1.1401 - val_acc: 0.7318\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "60/60 [==============================] - 32s 528ms/step - loss: 1.1944 - acc: 0.8488 - val_loss: 1.2460 - val_acc: 0.7073\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "60/60 [==============================] - 33s 543ms/step - loss: 1.1788 - acc: 0.8506 - val_loss: 1.0617 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "60/60 [==============================] - 30s 499ms/step - loss: 1.0969 - acc: 0.8809 - val_loss: 0.8178 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.90880 to 0.81779, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 29/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0669 - acc: 0.8918 - val_loss: 0.7695 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.81779 to 0.76951, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 30/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.0559 - acc: 0.8917 - val_loss: 0.7805 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 1.0319 - acc: 0.8978 - val_loss: 0.7808 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "60/60 [==============================] - 32s 539ms/step - loss: 1.0346 - acc: 0.9012 - val_loss: 0.7765 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0294 - acc: 0.8962 - val_loss: 0.7732 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 1.0296 - acc: 0.8955 - val_loss: 0.7815 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "60/60 [==============================] - 32s 536ms/step - loss: 1.0148 - acc: 0.8995 - val_loss: 0.7754 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "60/60 [==============================] - 32s 535ms/step - loss: 1.0190 - acc: 0.8978 - val_loss: 0.7542 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.76951 to 0.75417, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 37/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 1.0124 - acc: 0.8946 - val_loss: 0.7524 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.75417 to 0.75238, saving model to checkpoints_log_mel_sp_44000x5_image_aug_10f/best_9.h5\n",
      "Epoch 38/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0169 - acc: 0.8979 - val_loss: 0.7541 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.0039 - acc: 0.8972 - val_loss: 0.7546 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "60/60 [==============================] - 32s 530ms/step - loss: 0.9999 - acc: 0.9022 - val_loss: 0.7554 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "60/60 [==============================] - 32s 540ms/step - loss: 0.9940 - acc: 0.9007 - val_loss: 0.7563 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "60/60 [==============================] - 32s 533ms/step - loss: 1.0038 - acc: 0.8968 - val_loss: 0.7590 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "60/60 [==============================] - 33s 542ms/step - loss: 1.0130 - acc: 0.8988 - val_loss: 0.7576 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "60/60 [==============================] - 32s 534ms/step - loss: 1.0084 - acc: 0.9008 - val_loss: 0.7561 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "60/60 [==============================] - 32s 541ms/step - loss: 0.9935 - acc: 0.8985 - val_loss: 0.7571 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0150 - acc: 0.8960 - val_loss: 0.7567 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "60/60 [==============================] - 32s 537ms/step - loss: 1.0036 - acc: 0.8962 - val_loss: 0.7557 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "9473/9473 [==============================] - 28s 3ms/step\n",
      "9400/9400 [==============================] - 24s 3ms/step\n",
      "CPU times: user 5h 3min 35s, sys: 4h 3min 58s, total: 9h 7min 33s\n",
      "Wall time: 5h 11min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# log mel sp 44100x5 image_aug\n",
    "config = Config(sampling_rate=44100, audio_duration=5, n_classes=41, use_log_mel_sp=True, n_folds=10, max_epochs=300, n_mfcc=128*2)\n",
    "PREDICTION_FOLDER = \"predictions_log_mel_sp_44000x5_image_aug_10f\"\n",
    "CHECKPOINT_FOLDER = 'checkpoints_log_mel_sp_44000x5_image_aug_10f'\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 142\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "#     y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "\n",
    "    X = X_train[train_split]\n",
    "    y = y_train[train_split]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model(config)\n",
    "    parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#     model.load_weights('checkpoint_1d_24000x5' + '/best_99.h5')\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-7, amsgrad=True)\n",
    "    parallel_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=adam,\n",
    "                         metrics=['acc'])\n",
    "\n",
    "#     sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#     parallel_model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=sgd,\n",
    "#                   metrics=['acc'])\n",
    "\n",
    "    checkpoint = ParallelModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    preprocessing_function=get_random_eraser(v_l=np.min(X), v_h=np.max(X)) # RANDOM ERASER\n",
    "    )\n",
    "    \n",
    "    datagen.fit(X)\n",
    "    training_generator = MixupGenerator(X, y, batch_size=batch_size, alpha=0.5, datagen=datagen)()\n",
    "    \n",
    "    parallel_model.fit_generator(generator=training_generator,\n",
    "                        steps_per_epoch=X.shape[0] // batch_size,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=config.max_epochs, verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "# Fine tune\n",
    "#     model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5' % i)\n",
    "#     K.set_value(model.optimizer.lr, 0.00001)\n",
    "#     parallel_model.fit_generator(mixupgen(),\n",
    "#                         steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "#                         epochs=10,\n",
    "#                         validation_data=test_datagen.flow(X_val, y_val), callbacks=None)\n",
    "#     parallel_model.save_weights(CHECKPOINT_FOLDER + '/besh_%d.h5' % i)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = parallel_model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = parallel_model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# log mel sp 44100x5 image_aug + class_weights\n",
    "config = Config(sampling_rate=44100, audio_duration=5, n_classes=41, use_log_mel_sp=True, n_folds=7, max_epochs=300, n_mfcc=128*2)\n",
    "PREDICTION_FOLDER = \"predictions_log_mel_sp_44000x5_image_aug_10f\"\n",
    "CHECKPOINT_FOLDER = 'checkpoints_log_mel_sp_44000x5_image_aug_10f'\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 142\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "#     y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "\n",
    "    X = X_train[train_split]\n",
    "    y = y_train[train_split]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model(config)\n",
    "    parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#     model.load_weights('checkpoint_1d_24000x5' + '/best_99.h5')\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-7, amsgrad=True)\n",
    "    parallel_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=adam,\n",
    "                         metrics=['acc'])\n",
    "\n",
    "#     sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#     parallel_model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=sgd,\n",
    "#                   metrics=['acc'])\n",
    "\n",
    "    checkpoint = ParallelModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    preprocessing_function=get_random_eraser(v_l=np.min(X), v_h=np.max(X)) # RANDOM ERASER\n",
    "    )\n",
    "    \n",
    "    datagen.fit(X)\n",
    "    training_generator = MixupGenerator(X, y, batch_size=batch_size, alpha=0.5, datagen=datagen)()\n",
    "    \n",
    "    parallel_model.fit_generator(generator=training_generator,\n",
    "                        steps_per_epoch=X.shape[0] // batch_size,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=config.max_epochs, verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "# Fine tune\n",
    "#     model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5' % i)\n",
    "#     K.set_value(model.optimizer.lr, 0.00001)\n",
    "#     parallel_model.fit_generator(mixupgen(),\n",
    "#                         steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "#                         epochs=10,\n",
    "#                         validation_data=test_datagen.flow(X_val, y_val), callbacks=None)\n",
    "#     parallel_model.save_weights(CHECKPOINT_FOLDER + '/besh_%d.h5' % i)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = parallel_model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = parallel_model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    nclass = config.n_classes\n",
    "    \n",
    "    inp = Input(shape=(config.dim[0], config.dim[1], 1))\n",
    "    x = Convolution2D(32, (3,3), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(32, (3,3), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "    x = Convolution2D(64, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(64, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "    x = Convolution2D(128, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(128, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(128, (1,1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "    x = Convolution2D(256, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(256, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(256, (1,1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D(strides=(2, 2))(x)\n",
    "\n",
    "    x = Convolution2D(512, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(512, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Convolution2D(512, (1,1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "#     x = MaxPool2D(strides=(2, 2))(x)\n",
    "    \n",
    "\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "    out = Dense(nclass, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.summary()\n",
    "    print(len(model.layers))\n",
    "    print(config.dim)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (8108, 256, 431, 1) (8108, 41)\n",
      "val shape:  (1365, 256, 431, 1) (1365, 41)\n",
      "Fold:  0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 128, 215, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 128, 215, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 64, 107, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_90 (Conv2D)           (None, 64, 107, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 64, 107, 128)      16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling (None, 32, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 32, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 32, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 32, 53, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 1,327,337\n",
      "Trainable params: 1,324,201\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "35\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 3.7551 - acc: 0.1520 - val_loss: 12.2328 - val_acc: 0.0454\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 12.23277, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 2/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 2.8420 - acc: 0.3218 - val_loss: 2.5014 - val_acc: 0.2791\n",
      "\n",
      "Epoch 00002: val_loss improved from 12.23277 to 2.50143, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 3/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 2.3541 - acc: 0.4521 - val_loss: 2.6280 - val_acc: 0.3612\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 2.1682 - acc: 0.5172 - val_loss: 2.3280 - val_acc: 0.3949\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.50143 to 2.32797, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 5/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 2.0799 - acc: 0.5457 - val_loss: 1.9539 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.32797 to 1.95385, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 6/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.9525 - acc: 0.5899 - val_loss: 2.4745 - val_acc: 0.3839\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.8572 - acc: 0.6250 - val_loss: 1.8500 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.95385 to 1.85001, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 8/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.8143 - acc: 0.6392 - val_loss: 1.9269 - val_acc: 0.5165\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.7620 - acc: 0.6554 - val_loss: 2.7334 - val_acc: 0.3663\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.6997 - acc: 0.6802 - val_loss: 1.8675 - val_acc: 0.5275\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.6488 - acc: 0.6980 - val_loss: 1.7803 - val_acc: 0.5582\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.85001 to 1.78027, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 12/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.6291 - acc: 0.6995 - val_loss: 1.8678 - val_acc: 0.5143\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.5820 - acc: 0.7160 - val_loss: 1.9299 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.5511 - acc: 0.7203 - val_loss: 1.5309 - val_acc: 0.5993\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.78027 to 1.53088, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 15/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.5174 - acc: 0.7358 - val_loss: 1.4958 - val_acc: 0.6117\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.53088 to 1.49584, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 16/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.4750 - acc: 0.7521 - val_loss: 1.2077 - val_acc: 0.6886\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.49584 to 1.20772, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 17/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.4528 - acc: 0.7608 - val_loss: 1.3250 - val_acc: 0.6674\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.4430 - acc: 0.7673 - val_loss: 1.2230 - val_acc: 0.6799\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.3953 - acc: 0.7719 - val_loss: 1.2580 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.3601 - acc: 0.7860 - val_loss: 1.5313 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.3526 - acc: 0.7875 - val_loss: 1.4101 - val_acc: 0.6322\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.3140 - acc: 0.7959 - val_loss: 1.3715 - val_acc: 0.6337\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.2124 - acc: 0.8363 - val_loss: 0.8093 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.20772 to 0.80935, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 24/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.1452 - acc: 0.8545 - val_loss: 0.7808 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.80935 to 0.78077, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 25/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1388 - acc: 0.8556 - val_loss: 0.7598 - val_acc: 0.8154\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.78077 to 0.75978, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 26/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.1264 - acc: 0.8597 - val_loss: 0.7604 - val_acc: 0.8161\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.1224 - acc: 0.8639 - val_loss: 0.7627 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1189 - acc: 0.8604 - val_loss: 0.7541 - val_acc: 0.8110\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.75978 to 0.75411, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 29/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1066 - acc: 0.8650 - val_loss: 0.7698 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1025 - acc: 0.8661 - val_loss: 0.7653 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.0835 - acc: 0.8714 - val_loss: 0.7568 - val_acc: 0.8183\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.0959 - acc: 0.8699 - val_loss: 0.7483 - val_acc: 0.8117\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.75411 to 0.74830, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_0.h5\n",
      "Epoch 33/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0822 - acc: 0.8738 - val_loss: 0.7564 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0721 - acc: 0.8772 - val_loss: 0.7675 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0754 - acc: 0.8811 - val_loss: 0.7619 - val_acc: 0.8117\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0691 - acc: 0.8712 - val_loss: 0.7702 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0845 - acc: 0.8715 - val_loss: 0.7588 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.0605 - acc: 0.8726 - val_loss: 0.7741 - val_acc: 0.8029\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0660 - acc: 0.8764 - val_loss: 0.7601 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0412 - acc: 0.8814 - val_loss: 0.7624 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0524 - acc: 0.8804 - val_loss: 0.7604 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0518 - acc: 0.8812 - val_loss: 0.7615 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0263 - acc: 0.8813 - val_loss: 0.7574 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0459 - acc: 0.8825 - val_loss: 0.7573 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "202/202 [==============================] - 78s 389ms/step - loss: 1.0325 - acc: 0.8829 - val_loss: 0.7572 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "202/202 [==============================] - 78s 388ms/step - loss: 1.0292 - acc: 0.8863 - val_loss: 0.7572 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0432 - acc: 0.8781 - val_loss: 0.7576 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "9473/9473 [==============================] - 41s 4ms/step\n",
      "9400/9400 [==============================] - 39s 4ms/step\n",
      "train shape:  (8111, 256, 431, 1) (8111, 41)\n",
      "val shape:  (1362, 256, 431, 1) (1362, 41)\n",
      "Fold:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 128, 215, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 128, 215, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 64, 107, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 64, 107, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 64, 107, 128)      16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_102 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_76 (MaxPooling (None, 32, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 32, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_103 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 32, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 32, 53, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_105 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_106 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 1,327,337\n",
      "Trainable params: 1,324,201\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "35\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "202/202 [==============================] - 80s 398ms/step - loss: 3.4728 - acc: 0.1661 - val_loss: 7.3321 - val_acc: 0.0844\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.33205, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 2/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 2.6859 - acc: 0.3486 - val_loss: 2.4343 - val_acc: 0.3436\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.33205 to 2.43428, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 3/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 2.3532 - acc: 0.4509 - val_loss: 2.6182 - val_acc: 0.3524\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 2.1541 - acc: 0.5235 - val_loss: 2.0386 - val_acc: 0.4295\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.43428 to 2.03855, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 5/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 2.0491 - acc: 0.5605 - val_loss: 3.0184 - val_acc: 0.3253\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.9746 - acc: 0.5864 - val_loss: 1.9287 - val_acc: 0.5029\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.03855 to 1.92870, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 7/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.8961 - acc: 0.6064 - val_loss: 2.1398 - val_acc: 0.4104\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.8011 - acc: 0.6377 - val_loss: 2.0437 - val_acc: 0.4523\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.7473 - acc: 0.6554 - val_loss: 2.3544 - val_acc: 0.4258\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.7141 - acc: 0.6736 - val_loss: 2.0250 - val_acc: 0.5117\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.6618 - acc: 0.6946 - val_loss: 1.3235 - val_acc: 0.6549\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.92870 to 1.32345, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 12/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.6454 - acc: 0.6975 - val_loss: 1.5101 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.5431 - acc: 0.7245 - val_loss: 1.5850 - val_acc: 0.5962\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.5485 - acc: 0.7261 - val_loss: 1.4000 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.5149 - acc: 0.7355 - val_loss: 1.3193 - val_acc: 0.6571\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.32345 to 1.31934, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 16/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.4625 - acc: 0.7582 - val_loss: 1.5260 - val_acc: 0.6101\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 1.4221 - acc: 0.7660 - val_loss: 1.5475 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.4040 - acc: 0.7723 - val_loss: 1.4765 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.3684 - acc: 0.7812 - val_loss: 1.4411 - val_acc: 0.6189\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.3435 - acc: 0.7865 - val_loss: 1.9359 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.3207 - acc: 0.7965 - val_loss: 1.6336 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2143 - acc: 0.8321 - val_loss: 0.8396 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.31934 to 0.83958, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 23/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1684 - acc: 0.8502 - val_loss: 0.8265 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.83958 to 0.82654, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 24/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1378 - acc: 0.8531 - val_loss: 0.8227 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.82654 to 0.82272, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 25/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1322 - acc: 0.8579 - val_loss: 0.8107 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.82272 to 0.81067, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 26/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1097 - acc: 0.8660 - val_loss: 0.8123 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1069 - acc: 0.8663 - val_loss: 0.8258 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.1059 - acc: 0.8652 - val_loss: 0.8046 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.81067 to 0.80462, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 29/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0922 - acc: 0.8713 - val_loss: 0.8084 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0902 - acc: 0.8693 - val_loss: 0.8285 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0703 - acc: 0.8813 - val_loss: 0.8089 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0737 - acc: 0.8764 - val_loss: 0.8169 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0733 - acc: 0.8806 - val_loss: 0.8198 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0614 - acc: 0.8772 - val_loss: 0.8261 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0438 - acc: 0.8770 - val_loss: 0.8066 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0386 - acc: 0.8818 - val_loss: 0.8078 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0432 - acc: 0.8829 - val_loss: 0.8029 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.80462 to 0.80289, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 38/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0417 - acc: 0.8835 - val_loss: 0.7988 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.80289 to 0.79880, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_1.h5\n",
      "Epoch 39/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0424 - acc: 0.8863 - val_loss: 0.8009 - val_acc: 0.7849\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.0469 - acc: 0.8787 - val_loss: 0.8013 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0526 - acc: 0.8807 - val_loss: 0.8016 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0471 - acc: 0.8822 - val_loss: 0.8004 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0470 - acc: 0.8821 - val_loss: 0.8000 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "202/202 [==============================] - 78s 389ms/step - loss: 1.0405 - acc: 0.8821 - val_loss: 0.7996 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0486 - acc: 0.8842 - val_loss: 0.7997 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0587 - acc: 0.8834 - val_loss: 0.7999 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0545 - acc: 0.8809 - val_loss: 0.7992 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0436 - acc: 0.8807 - val_loss: 0.7993 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0533 - acc: 0.8787 - val_loss: 0.7990 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0486 - acc: 0.8827 - val_loss: 0.8000 - val_acc: 0.7849\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0412 - acc: 0.8863 - val_loss: 0.7992 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0481 - acc: 0.8856 - val_loss: 0.7996 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.0387 - acc: 0.8884 - val_loss: 0.8007 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "9473/9473 [==============================] - 40s 4ms/step\n",
      "9400/9400 [==============================] - 39s 4ms/step\n",
      "train shape:  (8112, 256, 431, 1) (8112, 41)\n",
      "val shape:  (1361, 256, 431, 1) (1361, 41)\n",
      "Fold:  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_106 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_108 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_77 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_107 (Conv2D)          (None, 128, 215, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, 128, 215, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_78 (MaxPooling (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_109 (Conv2D)          (None, 64, 107, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_110 (Conv2D)          (None, 64, 107, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_111 (Conv2D)          (None, 64, 107, 128)      16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_79 (MaxPooling (None, 32, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_112 (Conv2D)          (None, 32, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_113 (Conv2D)          (None, 32, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_114 (Conv2D)          (None, 32, 53, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 1,327,337\n",
      "Trainable params: 1,324,201\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "35\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 3.5299 - acc: 0.1335 - val_loss: 3.9685 - val_acc: 0.1359\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.96853, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 2/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 2.7660 - acc: 0.3127 - val_loss: 3.0842 - val_acc: 0.2535\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.96853 to 3.08419, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 3/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 2.4058 - acc: 0.4290 - val_loss: 3.6389 - val_acc: 0.1859\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 79s 390ms/step - loss: 2.1788 - acc: 0.5113 - val_loss: 2.4282 - val_acc: 0.3630\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.08419 to 2.42817, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 5/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 2.0391 - acc: 0.5610 - val_loss: 1.8387 - val_acc: 0.5349\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.42817 to 1.83870, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 6/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.9391 - acc: 0.5954 - val_loss: 1.7955 - val_acc: 0.5033\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.83870 to 1.79552, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 7/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.8456 - acc: 0.6223 - val_loss: 1.7652 - val_acc: 0.5298\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.79552 to 1.76521, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 8/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.7449 - acc: 0.6610 - val_loss: 2.5751 - val_acc: 0.3747\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.6887 - acc: 0.6785 - val_loss: 2.1548 - val_acc: 0.4930\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.6730 - acc: 0.6877 - val_loss: 1.7797 - val_acc: 0.5305\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.6127 - acc: 0.7007 - val_loss: 1.8487 - val_acc: 0.5246\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.5672 - acc: 0.7203 - val_loss: 1.4771 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.76521 to 1.47708, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 13/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.5304 - acc: 0.7262 - val_loss: 1.3252 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.47708 to 1.32525, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 14/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.4708 - acc: 0.7488 - val_loss: 1.5630 - val_acc: 0.5805\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.4371 - acc: 0.7626 - val_loss: 1.6977 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.4207 - acc: 0.7568 - val_loss: 1.2745 - val_acc: 0.6613\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.32525 to 1.27452, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 17/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.4000 - acc: 0.7694 - val_loss: 1.3649 - val_acc: 0.6370\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.3748 - acc: 0.7718 - val_loss: 1.1631 - val_acc: 0.7010\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.27452 to 1.16311, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 19/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.3360 - acc: 0.7948 - val_loss: 1.4916 - val_acc: 0.6165\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.3255 - acc: 0.7944 - val_loss: 1.4718 - val_acc: 0.6319\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2995 - acc: 0.7993 - val_loss: 1.2240 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2696 - acc: 0.8077 - val_loss: 1.1148 - val_acc: 0.7193\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.16311 to 1.11484, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 23/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2251 - acc: 0.8208 - val_loss: 1.2824 - val_acc: 0.6708\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2087 - acc: 0.8204 - val_loss: 1.4166 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.2045 - acc: 0.8353 - val_loss: 1.2522 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1881 - acc: 0.8353 - val_loss: 1.1526 - val_acc: 0.7142\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1709 - acc: 0.8335 - val_loss: 0.9499 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.11484 to 0.94991, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 28/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1667 - acc: 0.8447 - val_loss: 1.2009 - val_acc: 0.7149\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1366 - acc: 0.8550 - val_loss: 1.1515 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1151 - acc: 0.8585 - val_loss: 1.1998 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1123 - acc: 0.8579 - val_loss: 1.1067 - val_acc: 0.7186\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 1.1086 - acc: 0.8546 - val_loss: 1.1849 - val_acc: 0.6738\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0907 - acc: 0.8639 - val_loss: 0.9778 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.0258 - acc: 0.8840 - val_loss: 0.7668 - val_acc: 0.8046\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.94991 to 0.76682, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 35/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9913 - acc: 0.8963 - val_loss: 0.7547 - val_acc: 0.8163\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.76682 to 0.75468, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 36/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9673 - acc: 0.8910 - val_loss: 0.7545 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.75468 to 0.75453, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 37/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9613 - acc: 0.8911 - val_loss: 0.7451 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.75453 to 0.74507, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 38/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 0.9452 - acc: 0.8964 - val_loss: 0.7444 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.74507 to 0.74443, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 39/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9319 - acc: 0.8970 - val_loss: 0.7518 - val_acc: 0.8126\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 0.9467 - acc: 0.9006 - val_loss: 0.7343 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.74443 to 0.73431, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_2.h5\n",
      "Epoch 41/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9460 - acc: 0.8972 - val_loss: 0.7459 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9306 - acc: 0.8983 - val_loss: 0.7452 - val_acc: 0.8126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9297 - acc: 0.8996 - val_loss: 0.7493 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9357 - acc: 0.8978 - val_loss: 0.7369 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9268 - acc: 0.8989 - val_loss: 0.7395 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 0.9194 - acc: 0.9012 - val_loss: 0.7469 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 0.9276 - acc: 0.9001 - val_loss: 0.7394 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9194 - acc: 0.8991 - val_loss: 0.7367 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "202/202 [==============================] - 78s 389ms/step - loss: 0.9231 - acc: 0.8990 - val_loss: 0.7365 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9264 - acc: 0.8947 - val_loss: 0.7366 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9118 - acc: 0.9043 - val_loss: 0.7384 - val_acc: 0.8104\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 0.9127 - acc: 0.8970 - val_loss: 0.7361 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9168 - acc: 0.9012 - val_loss: 0.7368 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "202/202 [==============================] - 79s 389ms/step - loss: 0.9004 - acc: 0.9019 - val_loss: 0.7371 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9158 - acc: 0.9040 - val_loss: 0.7372 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "9473/9473 [==============================] - 40s 4ms/step\n",
      "9400/9400 [==============================] - 39s 4ms/step\n",
      "train shape:  (8115, 256, 431, 1) (8115, 41)\n",
      "val shape:  (1358, 256, 431, 1) (1358, 41)\n",
      "Fold:  3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_80 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 128, 215, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 128, 215, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_81 (MaxPooling (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 64, 107, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 64, 107, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 64, 107, 128)      16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_82 (MaxPooling (None, 32, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 32, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 32, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_126 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 32, 53, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_127 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_4 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_128 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 1,327,337\n",
      "Trainable params: 1,324,201\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "35\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 3.3018 - acc: 0.2140 - val_loss: 4.1671 - val_acc: 0.1613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.16709, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 2/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 2.5194 - acc: 0.3996 - val_loss: 2.9988 - val_acc: 0.2636\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.16709 to 2.99879, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 3/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 2.2246 - acc: 0.4958 - val_loss: 3.4617 - val_acc: 0.2371\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 79s 390ms/step - loss: 2.0869 - acc: 0.5521 - val_loss: 2.2563 - val_acc: 0.4212\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.99879 to 2.25631, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 5/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.9642 - acc: 0.5900 - val_loss: 2.8949 - val_acc: 0.3041\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.8650 - acc: 0.6220 - val_loss: 1.8633 - val_acc: 0.4897\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.25631 to 1.86334, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 7/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.8021 - acc: 0.6449 - val_loss: 2.4676 - val_acc: 0.3962\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.7068 - acc: 0.6720 - val_loss: 2.1780 - val_acc: 0.4588\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.6760 - acc: 0.6838 - val_loss: 1.6636 - val_acc: 0.5751\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.86334 to 1.66361, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 10/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.6217 - acc: 0.7099 - val_loss: 1.5905 - val_acc: 0.5648\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.66361 to 1.59046, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 11/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 1.5868 - acc: 0.7135 - val_loss: 1.9826 - val_acc: 0.5125\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.5511 - acc: 0.7255 - val_loss: 1.6666 - val_acc: 0.5943\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.4938 - acc: 0.7403 - val_loss: 1.6048 - val_acc: 0.5788\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.4669 - acc: 0.7538 - val_loss: 1.5397 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.59046 to 1.53969, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 15/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.4440 - acc: 0.7573 - val_loss: 1.7649 - val_acc: 0.5589\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.4273 - acc: 0.7658 - val_loss: 1.3494 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.53969 to 1.34944, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 17/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.3933 - acc: 0.7776 - val_loss: 1.2829 - val_acc: 0.6502\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.34944 to 1.28291, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 18/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.3509 - acc: 0.7827 - val_loss: 1.3158 - val_acc: 0.6613\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.3262 - acc: 0.7969 - val_loss: 1.5310 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2839 - acc: 0.8062 - val_loss: 1.6075 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 1.2809 - acc: 0.8074 - val_loss: 2.1338 - val_acc: 0.5037\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2603 - acc: 0.8179 - val_loss: 1.2202 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.28291 to 1.22025, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 23/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.2414 - acc: 0.8235 - val_loss: 1.4818 - val_acc: 0.5979\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.2059 - acc: 0.8329 - val_loss: 1.2369 - val_acc: 0.6716\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1999 - acc: 0.8366 - val_loss: 1.6837 - val_acc: 0.5619\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1932 - acc: 0.8379 - val_loss: 1.3165 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1729 - acc: 0.8448 - val_loss: 1.1959 - val_acc: 0.6951\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.22025 to 1.19592, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 28/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 1.1459 - acc: 0.8483 - val_loss: 1.7616 - val_acc: 0.5501\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1292 - acc: 0.8504 - val_loss: 1.1664 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.19592 to 1.16637, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 30/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 1.1281 - acc: 0.8535 - val_loss: 1.2531 - val_acc: 0.6841\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.1172 - acc: 0.8538 - val_loss: 1.2705 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.0857 - acc: 0.8627 - val_loss: 1.4910 - val_acc: 0.6112\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.0732 - acc: 0.8676 - val_loss: 1.2246 - val_acc: 0.6892\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.0779 - acc: 0.8681 - val_loss: 1.2507 - val_acc: 0.6664\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.0674 - acc: 0.8689 - val_loss: 1.1856 - val_acc: 0.7003\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 1.0029 - acc: 0.8866 - val_loss: 0.8089 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.16637 to 0.80886, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 37/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9564 - acc: 0.9004 - val_loss: 0.7982 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.80886 to 0.79823, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 38/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9433 - acc: 0.9028 - val_loss: 0.8084 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9542 - acc: 0.8999 - val_loss: 0.7934 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.79823 to 0.79340, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_3.h5\n",
      "Epoch 40/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 0.9427 - acc: 0.9010 - val_loss: 0.8057 - val_acc: 0.7931\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9508 - acc: 0.8934 - val_loss: 0.8025 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9198 - acc: 0.9075 - val_loss: 0.8040 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 0.9300 - acc: 0.8989 - val_loss: 0.8033 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9316 - acc: 0.9006 - val_loss: 0.7976 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9204 - acc: 0.9028 - val_loss: 0.8117 - val_acc: 0.7931\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 0.9293 - acc: 0.9010 - val_loss: 0.7999 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9150 - acc: 0.9035 - val_loss: 0.7978 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9136 - acc: 0.9068 - val_loss: 0.7967 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9151 - acc: 0.9020 - val_loss: 0.7981 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "202/202 [==============================] - 79s 391ms/step - loss: 0.9037 - acc: 0.9041 - val_loss: 0.7972 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "202/202 [==============================] - 79s 392ms/step - loss: 0.9043 - acc: 0.9072 - val_loss: 0.7977 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9182 - acc: 0.9026 - val_loss: 0.7976 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9053 - acc: 0.9011 - val_loss: 0.7971 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "202/202 [==============================] - 79s 390ms/step - loss: 0.9274 - acc: 0.9087 - val_loss: 0.7960 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "9473/9473 [==============================] - 41s 4ms/step\n",
      "9400/9400 [==============================] - 39s 4ms/step\n",
      "train shape:  (8121, 256, 431, 1) (8121, 41)\n",
      "val shape:  (1352, 256, 431, 1) (1352, 41)\n",
      "Fold:  4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_130 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_83 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 128, 215, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_131 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 128, 215, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_132 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_84 (MaxPooling (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 64, 107, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_133 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_130 (Conv2D)          (None, 64, 107, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_131 (Conv2D)          (None, 64, 107, 128)      16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_85 (MaxPooling (None, 32, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_132 (Conv2D)          (None, 32, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_133 (Conv2D)          (None, 32, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 32, 53, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_5 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 1,327,337\n",
      "Trainable params: 1,324,201\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "35\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "203/203 [==============================] - 82s 402ms/step - loss: 3.7225 - acc: 0.1299 - val_loss: 5.6738 - val_acc: 0.0629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.67380, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 2/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 2.8486 - acc: 0.3156 - val_loss: 2.9639 - val_acc: 0.2389\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.67380 to 2.96388, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 3/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 2.4257 - acc: 0.4224 - val_loss: 1.9508 - val_acc: 0.4556\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.96388 to 1.95076, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 4/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 2.2110 - acc: 0.4950 - val_loss: 2.7038 - val_acc: 0.3129\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 2.0863 - acc: 0.5469 - val_loss: 2.8178 - val_acc: 0.3646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.9969 - acc: 0.5800 - val_loss: 2.0162 - val_acc: 0.4608\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.9128 - acc: 0.6025 - val_loss: 1.4171 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.95076 to 1.41707, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 8/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.8106 - acc: 0.6414 - val_loss: 1.8780 - val_acc: 0.4926\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.8062 - acc: 0.6361 - val_loss: 1.7678 - val_acc: 0.5237\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.7191 - acc: 0.6712 - val_loss: 1.3293 - val_acc: 0.6206\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.41707 to 1.32927, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 11/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.6806 - acc: 0.6820 - val_loss: 1.7155 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.6378 - acc: 0.6963 - val_loss: 1.5927 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.6026 - acc: 0.7119 - val_loss: 1.4803 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.5590 - acc: 0.7191 - val_loss: 1.6860 - val_acc: 0.5459\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.5450 - acc: 0.7272 - val_loss: 1.6997 - val_acc: 0.5325\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.4939 - acc: 0.7383 - val_loss: 1.1321 - val_acc: 0.6886\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.32927 to 1.13210, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 17/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.4610 - acc: 0.7549 - val_loss: 1.6313 - val_acc: 0.5688\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.4562 - acc: 0.7509 - val_loss: 1.6326 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.4178 - acc: 0.7687 - val_loss: 1.3546 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.4084 - acc: 0.7750 - val_loss: 1.4015 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.3855 - acc: 0.7791 - val_loss: 1.5316 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.3460 - acc: 0.7898 - val_loss: 1.2508 - val_acc: 0.6805\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.2291 - acc: 0.8340 - val_loss: 0.7861 - val_acc: 0.7922\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.13210 to 0.78606, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 24/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.1956 - acc: 0.8477 - val_loss: 0.7769 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.78606 to 0.77694, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 25/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.1467 - acc: 0.8553 - val_loss: 0.7809 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.1496 - acc: 0.8605 - val_loss: 0.7793 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.1512 - acc: 0.8557 - val_loss: 0.7740 - val_acc: 0.7959\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.77694 to 0.77405, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 28/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.1351 - acc: 0.8639 - val_loss: 0.7806 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1211 - acc: 0.8621 - val_loss: 0.7748 - val_acc: 0.7929\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1310 - acc: 0.8667 - val_loss: 0.7711 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.77405 to 0.77114, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 31/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.1213 - acc: 0.8672 - val_loss: 0.7628 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.77114 to 0.76284, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 32/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0958 - acc: 0.8717 - val_loss: 0.7742 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0980 - acc: 0.8702 - val_loss: 0.7738 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0980 - acc: 0.8746 - val_loss: 0.7863 - val_acc: 0.7944\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0907 - acc: 0.8702 - val_loss: 0.7786 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0736 - acc: 0.8729 - val_loss: 0.7749 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0941 - acc: 0.8677 - val_loss: 0.7887 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0774 - acc: 0.8781 - val_loss: 0.7654 - val_acc: 0.7899\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0847 - acc: 0.8784 - val_loss: 0.7628 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.76284 to 0.76279, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 40/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0678 - acc: 0.8746 - val_loss: 0.7632 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0725 - acc: 0.8826 - val_loss: 0.7633 - val_acc: 0.8010\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0831 - acc: 0.8775 - val_loss: 0.7627 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.76279 to 0.76267, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 43/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0764 - acc: 0.8771 - val_loss: 0.7627 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0653 - acc: 0.8745 - val_loss: 0.7624 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.76267 to 0.76237, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 45/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0710 - acc: 0.8808 - val_loss: 0.7623 - val_acc: 0.8010\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.76237 to 0.76231, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 46/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0617 - acc: 0.8791 - val_loss: 0.7640 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0592 - acc: 0.8841 - val_loss: 0.7628 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.0779 - acc: 0.8794 - val_loss: 0.7623 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.76231 to 0.76226, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 49/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0505 - acc: 0.8803 - val_loss: 0.7611 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.76226 to 0.76110, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 50/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.0671 - acc: 0.8749 - val_loss: 0.7610 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.76110 to 0.76101, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 51/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0544 - acc: 0.8852 - val_loss: 0.7603 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.76101 to 0.76027, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 52/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0618 - acc: 0.8837 - val_loss: 0.7628 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0655 - acc: 0.8819 - val_loss: 0.7605 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0581 - acc: 0.8744 - val_loss: 0.7594 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.76027 to 0.75936, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_4.h5\n",
      "Epoch 55/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0510 - acc: 0.8768 - val_loss: 0.7616 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0617 - acc: 0.8844 - val_loss: 0.7635 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0549 - acc: 0.8782 - val_loss: 0.7641 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0568 - acc: 0.8776 - val_loss: 0.7649 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0554 - acc: 0.8803 - val_loss: 0.7651 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0621 - acc: 0.8784 - val_loss: 0.7652 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0567 - acc: 0.8781 - val_loss: 0.7667 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0703 - acc: 0.8746 - val_loss: 0.7659 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0563 - acc: 0.8813 - val_loss: 0.7651 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0504 - acc: 0.8781 - val_loss: 0.7643 - val_acc: 0.8070\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0524 - acc: 0.8796 - val_loss: 0.7637 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0512 - acc: 0.8861 - val_loss: 0.7645 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.0671 - acc: 0.8775 - val_loss: 0.7649 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0640 - acc: 0.8755 - val_loss: 0.7647 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.0532 - acc: 0.8861 - val_loss: 0.7642 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "9473/9473 [==============================] - 41s 4ms/step\n",
      "9400/9400 [==============================] - 39s 4ms/step\n",
      "train shape:  (8125, 256, 431, 1) (8125, 41)\n",
      "val shape:  (1348, 256, 431, 1) (1348, 41)\n",
      "Fold:  5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_86 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_137 (Conv2D)          (None, 128, 215, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_142 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_138 (Conv2D)          (None, 128, 215, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_143 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_87 (MaxPooling (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_139 (Conv2D)          (None, 64, 107, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_144 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_140 (Conv2D)          (None, 64, 107, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_145 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_141 (Conv2D)          (None, 64, 107, 128)      16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_88 (MaxPooling (None, 32, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_142 (Conv2D)          (None, 32, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_143 (Conv2D)          (None, 32, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_144 (Conv2D)          (None, 32, 53, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_6 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 1,327,337\n",
      "Trainable params: 1,324,201\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "35\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "203/203 [==============================] - 82s 402ms/step - loss: 3.2850 - acc: 0.2117 - val_loss: 3.1896 - val_acc: 0.2411\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.18958, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 2/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 2.5230 - acc: 0.4012 - val_loss: 2.8600 - val_acc: 0.2663\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.18958 to 2.85997, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 3/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 2.2125 - acc: 0.4982 - val_loss: 2.8941 - val_acc: 0.3798\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 2.0643 - acc: 0.5459 - val_loss: 2.1640 - val_acc: 0.4585\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.85997 to 2.16405, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 5/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.9694 - acc: 0.5881 - val_loss: 2.4711 - val_acc: 0.4355\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.8781 - acc: 0.6160 - val_loss: 1.9620 - val_acc: 0.4963\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.16405 to 1.96202, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 7/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.7942 - acc: 0.6410 - val_loss: 1.8178 - val_acc: 0.5141\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.96202 to 1.81781, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 8/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.7482 - acc: 0.6635 - val_loss: 1.4686 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.81781 to 1.46862, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 9/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.6724 - acc: 0.6826 - val_loss: 2.1097 - val_acc: 0.5074\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.6298 - acc: 0.6984 - val_loss: 1.6387 - val_acc: 0.5482\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.5652 - acc: 0.7179 - val_loss: 1.6657 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.5295 - acc: 0.7267 - val_loss: 1.3331 - val_acc: 0.6476\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.46862 to 1.33308, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 13/300\n",
      "203/203 [==============================] - 80s 392ms/step - loss: 1.4832 - acc: 0.7401 - val_loss: 1.6704 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.4488 - acc: 0.7557 - val_loss: 1.4993 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.4215 - acc: 0.7554 - val_loss: 2.4519 - val_acc: 0.4733\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "203/203 [==============================] - 80s 392ms/step - loss: 1.4011 - acc: 0.7682 - val_loss: 1.4505 - val_acc: 0.6269\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.3550 - acc: 0.7786 - val_loss: 1.4179 - val_acc: 0.6298\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.3391 - acc: 0.7888 - val_loss: 1.4094 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.2394 - acc: 0.8320 - val_loss: 0.8516 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.33308 to 0.85160, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 20/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.1676 - acc: 0.8502 - val_loss: 0.8369 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.85160 to 0.83693, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 21/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1449 - acc: 0.8560 - val_loss: 0.8324 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.83693 to 0.83240, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 22/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.1405 - acc: 0.8473 - val_loss: 0.8242 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.83240 to 0.82416, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 23/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.1184 - acc: 0.8624 - val_loss: 0.8265 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1152 - acc: 0.8676 - val_loss: 0.8306 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1108 - acc: 0.8663 - val_loss: 0.8245 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "203/203 [==============================] - 79s 392ms/step - loss: 1.1061 - acc: 0.8596 - val_loss: 0.8455 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0872 - acc: 0.8711 - val_loss: 0.8466 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.1039 - acc: 0.8696 - val_loss: 0.8582 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0739 - acc: 0.8767 - val_loss: 0.8222 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.82416 to 0.82221, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 30/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0802 - acc: 0.8752 - val_loss: 0.8216 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.82221 to 0.82164, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 31/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0668 - acc: 0.8752 - val_loss: 0.8195 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.82164 to 0.81949, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 32/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0646 - acc: 0.8688 - val_loss: 0.8197 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0654 - acc: 0.8776 - val_loss: 0.8162 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.81949 to 0.81618, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 34/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0575 - acc: 0.8820 - val_loss: 0.8189 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0586 - acc: 0.8765 - val_loss: 0.8193 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0688 - acc: 0.8672 - val_loss: 0.8160 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.81618 to 0.81600, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 37/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0767 - acc: 0.8696 - val_loss: 0.8156 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.81600 to 0.81562, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 38/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0643 - acc: 0.8727 - val_loss: 0.8140 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.81562 to 0.81403, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_5.h5\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0466 - acc: 0.8725 - val_loss: 0.8175 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0642 - acc: 0.8724 - val_loss: 0.8178 - val_acc: 0.8019\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0312 - acc: 0.8828 - val_loss: 0.8184 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0648 - acc: 0.8728 - val_loss: 0.8157 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0675 - acc: 0.8735 - val_loss: 0.8163 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0715 - acc: 0.8762 - val_loss: 0.8164 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0531 - acc: 0.8819 - val_loss: 0.8171 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0503 - acc: 0.8714 - val_loss: 0.8166 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0600 - acc: 0.8786 - val_loss: 0.8168 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0551 - acc: 0.8762 - val_loss: 0.8158 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.0593 - acc: 0.8714 - val_loss: 0.8158 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.0438 - acc: 0.8762 - val_loss: 0.8168 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0442 - acc: 0.8766 - val_loss: 0.8167 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "203/203 [==============================] - 79s 392ms/step - loss: 1.0645 - acc: 0.8709 - val_loss: 0.8167 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.0733 - acc: 0.8768 - val_loss: 0.8163 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "9473/9473 [==============================] - 40s 4ms/step\n",
      "9400/9400 [==============================] - 39s 4ms/step\n",
      "train shape:  (8146, 256, 431, 1) (8146, 41)\n",
      "val shape:  (1327, 256, 431, 1) (1327, 41)\n",
      "Fold:  6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_146 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_89 (MaxPooling (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_147 (Conv2D)          (None, 128, 215, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_148 (Conv2D)          (None, 128, 215, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_154 (Bat (None, 128, 215, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 128, 215, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_90 (MaxPooling (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_149 (Conv2D)          (None, 64, 107, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_155 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          (None, 64, 107, 128)      147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_156 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_151 (Conv2D)          (None, 64, 107, 128)      16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_157 (Bat (None, 64, 107, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 64, 107, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_91 (MaxPooling (None, 32, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_152 (Conv2D)          (None, 32, 53, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_158 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_153 (Conv2D)          (None, 32, 53, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_159 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_154 (Conv2D)          (None, 32, 53, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_160 (Bat (None, 32, 53, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 32, 53, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_7 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_161 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 1,327,337\n",
      "Trainable params: 1,324,201\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "35\n",
      "(256, 431, 1)\n",
      "Epoch 1/300\n",
      "203/203 [==============================] - 82s 402ms/step - loss: 3.4059 - acc: 0.1748 - val_loss: 4.0839 - val_acc: 0.1530\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.08391, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 2/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 2.5857 - acc: 0.3863 - val_loss: 3.0894 - val_acc: 0.2645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_loss improved from 4.08391 to 3.08936, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 3/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 2.2317 - acc: 0.4957 - val_loss: 2.1913 - val_acc: 0.3821\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.08936 to 2.19133, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 4/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 2.0728 - acc: 0.5502 - val_loss: 3.0257 - val_acc: 0.3157\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.9892 - acc: 0.5840 - val_loss: 3.2135 - val_acc: 0.3060\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.8658 - acc: 0.6186 - val_loss: 2.5581 - val_acc: 0.4265\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.7999 - acc: 0.6414 - val_loss: 1.5842 - val_acc: 0.5742\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.19133 to 1.58421, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 8/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.7525 - acc: 0.6539 - val_loss: 1.8387 - val_acc: 0.5072\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.6906 - acc: 0.6748 - val_loss: 1.6490 - val_acc: 0.5433\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.6307 - acc: 0.7000 - val_loss: 1.5777 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.58421 to 1.57772, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 11/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.5736 - acc: 0.7165 - val_loss: 1.4415 - val_acc: 0.6300\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.57772 to 1.44151, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 12/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.5484 - acc: 0.7196 - val_loss: 1.5059 - val_acc: 0.6066\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.5214 - acc: 0.7331 - val_loss: 1.5858 - val_acc: 0.6059\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.4583 - acc: 0.7456 - val_loss: 1.7623 - val_acc: 0.5290\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.4374 - acc: 0.7584 - val_loss: 1.8051 - val_acc: 0.5569\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.3893 - acc: 0.7695 - val_loss: 1.2863 - val_acc: 0.6639\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.44151 to 1.28628, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 17/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.3658 - acc: 0.7852 - val_loss: 1.5957 - val_acc: 0.6059\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.3361 - acc: 0.7840 - val_loss: 1.3217 - val_acc: 0.6684\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.3223 - acc: 0.7975 - val_loss: 1.4727 - val_acc: 0.6255\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.2903 - acc: 0.8044 - val_loss: 1.4565 - val_acc: 0.6428\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 1.2878 - acc: 0.8064 - val_loss: 1.2789 - val_acc: 0.6662\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.28628 to 1.27891, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 22/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.2289 - acc: 0.8233 - val_loss: 1.4634 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 1.2126 - acc: 0.8188 - val_loss: 1.2522 - val_acc: 0.6609\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.27891 to 1.25218, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 24/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.2171 - acc: 0.8344 - val_loss: 1.1193 - val_acc: 0.6963\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.25218 to 1.11932, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 25/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.1708 - acc: 0.8399 - val_loss: 1.6106 - val_acc: 0.5885\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1592 - acc: 0.8394 - val_loss: 1.3381 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 1.1629 - acc: 0.8416 - val_loss: 1.1526 - val_acc: 0.7061\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1484 - acc: 0.8467 - val_loss: 1.2811 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1167 - acc: 0.8564 - val_loss: 1.1850 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 1.1052 - acc: 0.8569 - val_loss: 1.2039 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "203/203 [==============================] - 79s 392ms/step - loss: 1.0362 - acc: 0.8697 - val_loss: 0.7921 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.11932 to 0.79215, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 32/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 0.9937 - acc: 0.8959 - val_loss: 0.7807 - val_acc: 0.8026\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.79215 to 0.78066, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 33/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9786 - acc: 0.8925 - val_loss: 0.7743 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.78066 to 0.77434, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 34/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9497 - acc: 0.8991 - val_loss: 0.7739 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.77434 to 0.77391, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 35/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9708 - acc: 0.8932 - val_loss: 0.7662 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.77391 to 0.76616, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 36/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9618 - acc: 0.9027 - val_loss: 0.7682 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 0.9423 - acc: 0.8957 - val_loss: 0.7744 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 0.9472 - acc: 0.8926 - val_loss: 0.7699 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 0.9349 - acc: 0.8924 - val_loss: 0.7692 - val_acc: 0.8139\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9386 - acc: 0.8986 - val_loss: 0.7765 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9174 - acc: 0.9044 - val_loss: 0.7739 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9276 - acc: 0.9059 - val_loss: 0.7679 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "203/203 [==============================] - 80s 392ms/step - loss: 0.9341 - acc: 0.9025 - val_loss: 0.7668 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 0.9260 - acc: 0.9015 - val_loss: 0.7654 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.76616 to 0.76540, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 45/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9223 - acc: 0.9079 - val_loss: 0.7645 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.76540 to 0.76452, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 46/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9199 - acc: 0.9053 - val_loss: 0.7640 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.76452 to 0.76398, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 47/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9109 - acc: 0.9068 - val_loss: 0.7635 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.76398 to 0.76349, saving model to checkpoints_log_mel_sp_44000x5_image_aug_m2/best_6.h5\n",
      "Epoch 48/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 0.9481 - acc: 0.9028 - val_loss: 0.7648 - val_acc: 0.8071\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9346 - acc: 0.9007 - val_loss: 0.7635 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9228 - acc: 0.9023 - val_loss: 0.7647 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 0.9356 - acc: 0.9025 - val_loss: 0.7651 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9163 - acc: 0.9036 - val_loss: 0.7678 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9226 - acc: 0.9047 - val_loss: 0.7678 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 0.9450 - acc: 0.9001 - val_loss: 0.7680 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 0.9246 - acc: 0.9002 - val_loss: 0.7667 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9167 - acc: 0.9011 - val_loss: 0.7665 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "203/203 [==============================] - 79s 388ms/step - loss: 0.9178 - acc: 0.9057 - val_loss: 0.7664 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9151 - acc: 0.8996 - val_loss: 0.7670 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/300\n",
      "203/203 [==============================] - 79s 391ms/step - loss: 0.9263 - acc: 0.8996 - val_loss: 0.7674 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9260 - acc: 0.9030 - val_loss: 0.7668 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "203/203 [==============================] - 79s 390ms/step - loss: 0.9282 - acc: 0.9059 - val_loss: 0.7670 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/300\n",
      "203/203 [==============================] - 79s 389ms/step - loss: 0.9190 - acc: 0.9059 - val_loss: 0.7666 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "9473/9473 [==============================] - 41s 4ms/step\n",
      "9400/9400 [==============================] - 39s 4ms/step\n",
      "CPU times: user 7h 1min 31s, sys: 4h 13min 20s, total: 11h 14min 51s\n",
      "Wall time: 8h 52min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# log mel sp 44100x5 image_aug_m2\n",
    "config = Config(sampling_rate=44100, audio_duration=5, n_classes=41, use_log_mel_sp=True, n_folds=7, max_epochs=300, n_mfcc=128*2)\n",
    "PREDICTION_FOLDER = \"predictions_log_mel_sp_44000x5_image_aug_m2\"\n",
    "CHECKPOINT_FOLDER = 'checkpoints_log_mel_sp_44000x5_image_aug_m2'\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "#     y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "\n",
    "    X = X_train[train_split]\n",
    "    y = y_train[train_split]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model(config)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#     model.load_weights('checkpoint_1d_24000x5' + '/best_99.h5')\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-7, amsgrad=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=adam,\n",
    "                         metrics=['acc'])\n",
    "\n",
    "#     sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#     parallel_model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=sgd,\n",
    "#                   metrics=['acc'])\n",
    "\n",
    "    checkpoint = ParallelModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    preprocessing_function=get_random_eraser(v_l=np.min(X), v_h=np.max(X)) # RANDOM ERASER\n",
    "    )\n",
    "    \n",
    "    datagen.fit(X)\n",
    "    training_generator = MixupGenerator(X, y, batch_size=batch_size, alpha=0.5, datagen=datagen)()\n",
    "    \n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        steps_per_epoch=X.shape[0] // batch_size,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=config.max_epochs, verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "# Fine tune\n",
    "#     model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5' % i)\n",
    "#     K.set_value(model.optimizer.lr, 0.00001)\n",
    "#     parallel_model.fit_generator(mixupgen(),\n",
    "#                         steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "#                         epochs=10,\n",
    "#                         validation_data=test_datagen.flow(X_val, y_val), callbacks=None)\n",
    "#     parallel_model.save_weights(CHECKPOINT_FOLDER + '/besh_%d.h5' % i)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (15144, 256, 431, 1) (15144, 41)\n",
      "val shape:  (1901, 256, 431, 1) (1901, 41)\n",
      "Fold:  0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_180 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_188 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_106 (MaxPoolin (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_181 (Conv2D)          (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_189 (Bat (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_107 (MaxPoolin (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_182 (Conv2D)          (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_190 (Bat (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_108 (MaxPoolin (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_183 (Conv2D)          (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_191 (Bat (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_109 (MaxPoolin (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_184 (Conv2D)          (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_192 (Bat (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_110 (MaxPoolin (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_185 (Conv2D)          (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_193 (Bat (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_17  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 15144 samples, validate on 1901 samples\n",
      "Epoch 1/300\n",
      "15144/15144 [==============================] - 78s 5ms/step - loss: 2.9845 - acc: 0.2179 - val_loss: 4.9471 - val_acc: 0.1652\n",
      "\n",
      "Validation loss decreased from inf to 4.947076729007422, saving model\n",
      "Epoch 2/300\n",
      "15144/15144 [==============================] - 76s 5ms/step - loss: 2.3782 - acc: 0.3965 - val_loss: 2.8465 - val_acc: 0.3335\n",
      "\n",
      "Validation loss decreased from 4.947076729007422 to 2.8465399733347243, saving model\n",
      "Epoch 3/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 2.0416 - acc: 0.5164 - val_loss: 1.8576 - val_acc: 0.5013\n",
      "\n",
      "Validation loss decreased from 2.8465399733347243 to 1.8575544065077139, saving model\n",
      "Epoch 4/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 1.8294 - acc: 0.5895 - val_loss: 1.5488 - val_acc: 0.5744\n",
      "\n",
      "Validation loss decreased from 1.8575544065077139 to 1.5488133180900225, saving model\n",
      "Epoch 5/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 1.6651 - acc: 0.6373 - val_loss: 1.5211 - val_acc: 0.5855\n",
      "\n",
      "Validation loss decreased from 1.5488133180900225 to 1.5210937585785538, saving model\n",
      "Epoch 6/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 1.5334 - acc: 0.6790 - val_loss: 1.3820 - val_acc: 0.6407\n",
      "\n",
      "Validation loss decreased from 1.5210937585785538 to 1.3819760069729214, saving model\n",
      "Epoch 7/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 1.4358 - acc: 0.7135 - val_loss: 1.1672 - val_acc: 0.6865\n",
      "\n",
      "Validation loss decreased from 1.3819760069729214 to 1.1671844564695724, saving model\n",
      "Epoch 8/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 1.3421 - acc: 0.7402 - val_loss: 1.6958 - val_acc: 0.5655\n",
      "Epoch 9/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 1.2680 - acc: 0.7653 - val_loss: 1.0990 - val_acc: 0.6986\n",
      "\n",
      "Validation loss decreased from 1.1671844564695724 to 1.099030316220905, saving model\n",
      "Epoch 10/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 1.1951 - acc: 0.7824 - val_loss: 0.9277 - val_acc: 0.7501\n",
      "\n",
      "Validation loss decreased from 1.099030316220905 to 0.9277120951537644, saving model\n",
      "Epoch 11/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 1.1431 - acc: 0.8012 - val_loss: 1.2218 - val_acc: 0.7007\n",
      "Epoch 12/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 1.0183 - acc: 0.8348 - val_loss: 1.1540 - val_acc: 0.7070\n",
      "Epoch 14/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.9671 - acc: 0.8413 - val_loss: 1.1300 - val_acc: 0.7138\n",
      "Epoch 15/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 0.9210 - acc: 0.8584 - val_loss: 0.9033 - val_acc: 0.7649\n",
      "\n",
      "Validation loss decreased from 0.9277120951537644 to 0.9032500078024707, saving model\n",
      "Epoch 16/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.8794 - acc: 0.8658 - val_loss: 1.0285 - val_acc: 0.7386\n",
      "Epoch 17/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.8302 - acc: 0.8778 - val_loss: 1.2457 - val_acc: 0.6970\n",
      "Epoch 18/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.7991 - acc: 0.8866 - val_loss: 0.8892 - val_acc: 0.7743\n",
      "\n",
      "Validation loss decreased from 0.9032500078024707 to 0.8891671045023413, saving model\n",
      "Epoch 19/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.7623 - acc: 0.8914 - val_loss: 0.9809 - val_acc: 0.7438\n",
      "Epoch 20/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.7456 - acc: 0.8908 - val_loss: 0.7770 - val_acc: 0.7954\n",
      "\n",
      "Validation loss decreased from 0.8891671045023413 to 0.7769963417098373, saving model\n",
      "Epoch 21/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.7121 - acc: 0.8982 - val_loss: 0.8735 - val_acc: 0.7680\n",
      "Epoch 22/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.6905 - acc: 0.8995 - val_loss: 0.9267 - val_acc: 0.7827\n",
      "Epoch 23/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.6774 - acc: 0.8992 - val_loss: 0.8225 - val_acc: 0.8080\n",
      "Epoch 24/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.6533 - acc: 0.9054 - val_loss: 1.0298 - val_acc: 0.7643\n",
      "Epoch 25/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.6415 - acc: 0.9031 - val_loss: 0.9848 - val_acc: 0.7706\n",
      "Epoch 26/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.6256 - acc: 0.9025 - val_loss: 0.9036 - val_acc: 0.7743\n",
      "Epoch 27/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.5272 - acc: 0.9161 - val_loss: 0.5651 - val_acc: 0.8538\n",
      "\n",
      "Validation loss decreased from 0.7769963417098373 to 0.565061130001945, saving model\n",
      "Epoch 28/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4935 - acc: 0.9188 - val_loss: 0.5675 - val_acc: 0.8564\n",
      "Epoch 29/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4831 - acc: 0.9204 - val_loss: 0.5651 - val_acc: 0.8553\n",
      "Epoch 30/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 0.4729 - acc: 0.9251 - val_loss: 0.5562 - val_acc: 0.8564\n",
      "\n",
      "Validation loss decreased from 0.565061130001945 to 0.5561606315234534, saving model\n",
      "Epoch 31/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4673 - acc: 0.9264 - val_loss: 0.5602 - val_acc: 0.8548\n",
      "Epoch 32/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4601 - acc: 0.9257 - val_loss: 0.5611 - val_acc: 0.8553\n",
      "Epoch 33/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4555 - acc: 0.9235 - val_loss: 0.5597 - val_acc: 0.8569\n",
      "Epoch 34/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4521 - acc: 0.9260 - val_loss: 0.5706 - val_acc: 0.8527\n",
      "Epoch 35/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4479 - acc: 0.9248 - val_loss: 0.5665 - val_acc: 0.8601\n",
      "Epoch 36/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4444 - acc: 0.9264 - val_loss: 0.5661 - val_acc: 0.8564\n",
      "Epoch 37/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4365 - acc: 0.9293 - val_loss: 0.5627 - val_acc: 0.8580\n",
      "Epoch 38/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4357 - acc: 0.9278 - val_loss: 0.5623 - val_acc: 0.8585\n",
      "Epoch 39/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4338 - acc: 0.9253 - val_loss: 0.5633 - val_acc: 0.8574\n",
      "Epoch 40/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4328 - acc: 0.9301 - val_loss: 0.5627 - val_acc: 0.8580\n",
      "Epoch 41/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4338 - acc: 0.9300 - val_loss: 0.5642 - val_acc: 0.8601\n",
      "Epoch 42/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4325 - acc: 0.9281 - val_loss: 0.5633 - val_acc: 0.8580\n",
      "Epoch 43/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4313 - acc: 0.9280 - val_loss: 0.5648 - val_acc: 0.8585\n",
      "Epoch 44/300\n",
      "15144/15144 [==============================] - 73s 5ms/step - loss: 0.4316 - acc: 0.9288 - val_loss: 0.5636 - val_acc: 0.8580\n",
      "Epoch 45/300\n",
      "15144/15144 [==============================] - 74s 5ms/step - loss: 0.4323 - acc: 0.9295 - val_loss: 0.5635 - val_acc: 0.8574\n",
      "9473/9473 [==============================] - 23s 2ms/step\n",
      "9400/9400 [==============================] - 21s 2ms/step\n",
      "train shape:  (15150, 256, 431, 1) (15150, 41)\n",
      "val shape:  (1898, 256, 431, 1) (1898, 41)\n",
      "Fold:  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_186 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_194 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_111 (MaxPoolin (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_187 (Conv2D)          (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_195 (Bat (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_112 (MaxPoolin (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_188 (Conv2D)          (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_196 (Bat (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_113 (MaxPoolin (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_189 (Conv2D)          (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_197 (Bat (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_114 (MaxPoolin (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_190 (Conv2D)          (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_198 (Bat (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_115 (MaxPoolin (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_191 (Conv2D)          (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_199 (Bat (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_18  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 15150 samples, validate on 1898 samples\n",
      "Epoch 1/300\n",
      "15150/15150 [==============================] - 81s 5ms/step - loss: 3.0566 - acc: 0.1949 - val_loss: 4.2344 - val_acc: 0.1175\n",
      "\n",
      "Validation loss decreased from inf to 4.23439820071543, saving model\n",
      "Epoch 2/300\n",
      "15150/15150 [==============================] - 74s 5ms/step - loss: 2.4647 - acc: 0.3758 - val_loss: 2.6110 - val_acc: 0.3567\n",
      "\n",
      "Validation loss decreased from 4.23439820071543 to 2.610990391641824, saving model\n",
      "Epoch 3/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 2.0945 - acc: 0.4989 - val_loss: 1.8196 - val_acc: 0.5137\n",
      "\n",
      "Validation loss decreased from 2.610990391641824 to 1.8196010399919917, saving model\n",
      "Epoch 4/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 1.8545 - acc: 0.5787 - val_loss: 1.9704 - val_acc: 0.5074\n",
      "Epoch 5/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 1.6867 - acc: 0.6314 - val_loss: 1.5492 - val_acc: 0.6222\n",
      "\n",
      "Validation loss decreased from 1.8196010399919917 to 1.5492449226319098, saving model\n",
      "Epoch 6/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15150/15150 [==============================] - 73s 5ms/step - loss: 1.5504 - acc: 0.6754 - val_loss: 1.5835 - val_acc: 0.5769\n",
      "Epoch 7/300\n",
      "15150/15150 [==============================] - 74s 5ms/step - loss: 1.4452 - acc: 0.7081 - val_loss: 1.3217 - val_acc: 0.6607\n",
      "\n",
      "Validation loss decreased from 1.5492449226319098 to 1.3216732721559867, saving model\n",
      "Epoch 8/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 1.3543 - acc: 0.7369 - val_loss: 1.2418 - val_acc: 0.6812\n",
      "\n",
      "Validation loss decreased from 1.3216732721559867 to 1.2418281610572302, saving model\n",
      "Epoch 9/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 1.2686 - acc: 0.7626 - val_loss: 1.1657 - val_acc: 0.6791\n",
      "\n",
      "Validation loss decreased from 1.2418281610572302 to 1.165733650311027, saving model\n",
      "Epoch 10/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 1.1985 - acc: 0.7848 - val_loss: 1.0782 - val_acc: 0.7165\n",
      "\n",
      "Validation loss decreased from 1.165733650311027 to 1.0781811972563837, saving model\n",
      "Epoch 11/300\n",
      "15150/15150 [==============================] - 74s 5ms/step - loss: 1.1404 - acc: 0.7989 - val_loss: 1.3656 - val_acc: 0.6612\n",
      "Epoch 12/300\n",
      "15150/15150 [==============================] - 74s 5ms/step - loss: 1.0757 - acc: 0.8167 - val_loss: 1.2697 - val_acc: 0.6791\n",
      "Epoch 13/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 1.0158 - acc: 0.8337 - val_loss: 1.3775 - val_acc: 0.6644\n",
      "Epoch 14/300\n",
      "15150/15150 [==============================] - 74s 5ms/step - loss: 0.9651 - acc: 0.8500 - val_loss: 1.1312 - val_acc: 0.7345\n",
      "Epoch 15/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.9269 - acc: 0.8570 - val_loss: 1.0976 - val_acc: 0.7329\n",
      "Epoch 16/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.8919 - acc: 0.8673 - val_loss: 0.8927 - val_acc: 0.7682\n",
      "\n",
      "Validation loss decreased from 1.0781811972563837 to 0.8926779144179582, saving model\n",
      "Epoch 17/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.8412 - acc: 0.8780 - val_loss: 1.3211 - val_acc: 0.6934\n",
      "Epoch 18/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.7961 - acc: 0.8861 - val_loss: 1.0057 - val_acc: 0.7534\n",
      "Epoch 19/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.7678 - acc: 0.8917 - val_loss: 1.1041 - val_acc: 0.7197\n",
      "Epoch 20/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.7496 - acc: 0.8888 - val_loss: 1.0145 - val_acc: 0.7487\n",
      "Epoch 21/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.7095 - acc: 0.8989 - val_loss: 0.8975 - val_acc: 0.7777\n",
      "Epoch 22/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.6878 - acc: 0.8988 - val_loss: 0.9601 - val_acc: 0.7677\n",
      "Epoch 23/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5816 - acc: 0.9219 - val_loss: 0.6165 - val_acc: 0.8440\n",
      "\n",
      "Validation loss decreased from 0.8926779144179582 to 0.6165361692078875, saving model\n",
      "Epoch 24/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5506 - acc: 0.9246 - val_loss: 0.6142 - val_acc: 0.8430\n",
      "\n",
      "Validation loss decreased from 0.6165361692078875 to 0.6141870581437966, saving model\n",
      "Epoch 25/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5362 - acc: 0.9272 - val_loss: 0.6123 - val_acc: 0.8446\n",
      "\n",
      "Validation loss decreased from 0.6141870581437966 to 0.6123100093971188, saving model\n",
      "Epoch 26/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5281 - acc: 0.9266 - val_loss: 0.6169 - val_acc: 0.8446\n",
      "Epoch 27/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5209 - acc: 0.9248 - val_loss: 0.6102 - val_acc: 0.8472\n",
      "\n",
      "Validation loss decreased from 0.6123100093971188 to 0.6101505822954489, saving model\n",
      "Epoch 28/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5146 - acc: 0.9282 - val_loss: 0.6112 - val_acc: 0.8477\n",
      "Epoch 29/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5075 - acc: 0.9290 - val_loss: 0.6141 - val_acc: 0.8398\n",
      "Epoch 30/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.5032 - acc: 0.9285 - val_loss: 0.6199 - val_acc: 0.8435\n",
      "Epoch 31/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4987 - acc: 0.9271 - val_loss: 0.6110 - val_acc: 0.8456\n",
      "Epoch 32/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4943 - acc: 0.9267 - val_loss: 0.6233 - val_acc: 0.8440\n",
      "Epoch 33/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4920 - acc: 0.9269 - val_loss: 0.6137 - val_acc: 0.8498\n",
      "Epoch 34/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4805 - acc: 0.9314 - val_loss: 0.6114 - val_acc: 0.8462\n",
      "Epoch 35/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4777 - acc: 0.9331 - val_loss: 0.6148 - val_acc: 0.8446\n",
      "Epoch 36/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4787 - acc: 0.9277 - val_loss: 0.6134 - val_acc: 0.8446\n",
      "Epoch 37/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4773 - acc: 0.9318 - val_loss: 0.6128 - val_acc: 0.8435\n",
      "Epoch 38/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4767 - acc: 0.9297 - val_loss: 0.6135 - val_acc: 0.8456\n",
      "Epoch 39/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4762 - acc: 0.9269 - val_loss: 0.6130 - val_acc: 0.8440\n",
      "Epoch 40/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4749 - acc: 0.9295 - val_loss: 0.6133 - val_acc: 0.8440\n",
      "Epoch 41/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4744 - acc: 0.9319 - val_loss: 0.6140 - val_acc: 0.8451\n",
      "Epoch 42/300\n",
      "15150/15150 [==============================] - 73s 5ms/step - loss: 0.4744 - acc: 0.9314 - val_loss: 0.6135 - val_acc: 0.8451\n",
      "9473/9473 [==============================] - 41s 4ms/step\n",
      "9400/9400 [==============================] - 21s 2ms/step\n",
      "train shape:  (15156, 256, 431, 1) (15156, 41)\n",
      "val shape:  (1895, 256, 431, 1) (1895, 41)\n",
      "Fold:  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_192 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_200 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_116 (MaxPoolin (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_193 (Conv2D)          (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_201 (Bat (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_117 (MaxPoolin (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_194 (Conv2D)          (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_202 (Bat (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_118 (MaxPoolin (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_195 (Conv2D)          (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_203 (Bat (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_119 (MaxPoolin (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_196 (Conv2D)          (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_204 (Bat (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_120 (MaxPoolin (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_197 (Conv2D)          (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_205 (Bat (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_19  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15156 samples, validate on 1895 samples\n",
      "Epoch 1/300\n",
      "15156/15156 [==============================] - 83s 5ms/step - loss: 3.0668 - acc: 0.1929 - val_loss: 2.9440 - val_acc: 0.2354\n",
      "\n",
      "Validation loss decreased from inf to 2.9440285369399986, saving model\n",
      "Epoch 2/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 2.4704 - acc: 0.3715 - val_loss: 2.2410 - val_acc: 0.3873\n",
      "\n",
      "Validation loss decreased from 2.9440285369399986 to 2.2410008869573748, saving model\n",
      "Epoch 3/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 2.0996 - acc: 0.4951 - val_loss: 2.1552 - val_acc: 0.4354\n",
      "\n",
      "Validation loss decreased from 2.2410008869573748 to 2.1552185105145134, saving model\n",
      "Epoch 4/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.8502 - acc: 0.5788 - val_loss: 1.8200 - val_acc: 0.5193\n",
      "\n",
      "Validation loss decreased from 2.1552185105145134 to 1.8199748279551402, saving model\n",
      "Epoch 5/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.6743 - acc: 0.6355 - val_loss: 1.3132 - val_acc: 0.6512\n",
      "\n",
      "Validation loss decreased from 1.8199748279551402 to 1.3131829325945208, saving model\n",
      "Epoch 6/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.5464 - acc: 0.6786 - val_loss: 1.3954 - val_acc: 0.6116\n",
      "Epoch 7/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.4428 - acc: 0.7074 - val_loss: 1.2693 - val_acc: 0.6575\n",
      "\n",
      "Validation loss decreased from 1.3131829325945208 to 1.2693160547115558, saving model\n",
      "Epoch 8/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.3602 - acc: 0.7340 - val_loss: 1.4170 - val_acc: 0.6364\n",
      "Epoch 9/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.2808 - acc: 0.7571 - val_loss: 1.4295 - val_acc: 0.6274\n",
      "Epoch 10/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.2096 - acc: 0.7790 - val_loss: 1.3075 - val_acc: 0.6633\n",
      "Epoch 11/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 1.1488 - acc: 0.7955 - val_loss: 0.9642 - val_acc: 0.7430\n",
      "\n",
      "Validation loss decreased from 1.2693160547115558 to 0.9642103831812071, saving model\n",
      "Epoch 12/300\n",
      "15156/15156 [==============================] - 74s 5ms/step - loss: 1.1003 - acc: 0.8104 - val_loss: 1.0149 - val_acc: 0.7282\n",
      "Epoch 13/300\n",
      "15156/15156 [==============================] - 74s 5ms/step - loss: 1.0519 - acc: 0.8214 - val_loss: 1.2176 - val_acc: 0.7103\n",
      "Epoch 14/300\n",
      "15156/15156 [==============================] - 74s 5ms/step - loss: 0.9966 - acc: 0.8370 - val_loss: 0.9693 - val_acc: 0.7446\n",
      "Epoch 15/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.9482 - acc: 0.8470 - val_loss: 1.0819 - val_acc: 0.7261\n",
      "Epoch 16/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.9186 - acc: 0.8535 - val_loss: 1.0607 - val_acc: 0.7272\n",
      "Epoch 17/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.8637 - acc: 0.8672 - val_loss: 1.0757 - val_acc: 0.7319\n",
      "Epoch 18/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.7300 - acc: 0.9049 - val_loss: 0.5869 - val_acc: 0.8485\n",
      "\n",
      "Validation loss decreased from 0.9642103831812071 to 0.5868891879719605, saving model\n",
      "Epoch 19/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6927 - acc: 0.9128 - val_loss: 0.5806 - val_acc: 0.8443\n",
      "\n",
      "Validation loss decreased from 0.5868891879719605 to 0.5805586301556992, saving model\n",
      "Epoch 20/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6765 - acc: 0.9134 - val_loss: 0.5703 - val_acc: 0.8522\n",
      "\n",
      "Validation loss decreased from 0.5805586301556992 to 0.570265021361902, saving model\n",
      "Epoch 21/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6661 - acc: 0.9138 - val_loss: 0.5780 - val_acc: 0.8459\n",
      "Epoch 22/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6567 - acc: 0.9171 - val_loss: 0.5757 - val_acc: 0.8544\n",
      "Epoch 23/300\n",
      "15156/15156 [==============================] - 74s 5ms/step - loss: 0.6484 - acc: 0.9176 - val_loss: 0.5733 - val_acc: 0.8507\n",
      "Epoch 24/300\n",
      "15156/15156 [==============================] - 74s 5ms/step - loss: 0.6394 - acc: 0.9185 - val_loss: 0.5737 - val_acc: 0.8544\n",
      "Epoch 25/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6335 - acc: 0.9196 - val_loss: 0.5691 - val_acc: 0.8575\n",
      "\n",
      "Validation loss decreased from 0.570265021361902 to 0.5691074289086626, saving model\n",
      "Epoch 26/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6269 - acc: 0.9188 - val_loss: 0.5655 - val_acc: 0.8544\n",
      "\n",
      "Validation loss decreased from 0.5691074289086626 to 0.5654706847541879, saving model\n",
      "Epoch 27/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6212 - acc: 0.9192 - val_loss: 0.5774 - val_acc: 0.8491\n",
      "Epoch 28/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6146 - acc: 0.9177 - val_loss: 0.5728 - val_acc: 0.8575\n",
      "Epoch 29/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6097 - acc: 0.9212 - val_loss: 0.5715 - val_acc: 0.8554\n",
      "Epoch 30/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.6017 - acc: 0.9202 - val_loss: 0.5771 - val_acc: 0.8522\n",
      "Epoch 31/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5956 - acc: 0.9218 - val_loss: 0.5778 - val_acc: 0.8517\n",
      "Epoch 32/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5910 - acc: 0.9220 - val_loss: 0.5750 - val_acc: 0.8586\n",
      "Epoch 33/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5753 - acc: 0.9233 - val_loss: 0.5649 - val_acc: 0.8623\n",
      "\n",
      "Validation loss decreased from 0.5654706847541879 to 0.5649181710541405, saving model\n",
      "Epoch 34/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5734 - acc: 0.9248 - val_loss: 0.5640 - val_acc: 0.8654\n",
      "\n",
      "Validation loss decreased from 0.5649181710541405 to 0.5640012373395834, saving model\n",
      "Epoch 35/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5713 - acc: 0.9256 - val_loss: 0.5637 - val_acc: 0.8628\n",
      "\n",
      "Validation loss decreased from 0.5640012373395834 to 0.5637011246976877, saving model\n",
      "Epoch 36/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5699 - acc: 0.9224 - val_loss: 0.5634 - val_acc: 0.8607\n",
      "\n",
      "Validation loss decreased from 0.5637011246976877 to 0.5633938536323155, saving model\n",
      "Epoch 37/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5709 - acc: 0.9244 - val_loss: 0.5630 - val_acc: 0.8591\n",
      "\n",
      "Validation loss decreased from 0.5633938536323155 to 0.5629919703214339, saving model\n",
      "Epoch 38/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5667 - acc: 0.9247 - val_loss: 0.5653 - val_acc: 0.8602\n",
      "Epoch 39/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5683 - acc: 0.9232 - val_loss: 0.5643 - val_acc: 0.8586\n",
      "Epoch 40/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5670 - acc: 0.9255 - val_loss: 0.5652 - val_acc: 0.8591\n",
      "Epoch 41/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5684 - acc: 0.9244 - val_loss: 0.5647 - val_acc: 0.8612\n",
      "Epoch 42/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5665 - acc: 0.9248 - val_loss: 0.5645 - val_acc: 0.8607\n",
      "Epoch 43/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5661 - acc: 0.9252 - val_loss: 0.5639 - val_acc: 0.8580\n",
      "Epoch 44/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5639 - acc: 0.9248 - val_loss: 0.5638 - val_acc: 0.8580\n",
      "Epoch 45/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5642 - acc: 0.9227 - val_loss: 0.5642 - val_acc: 0.8586\n",
      "Epoch 46/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5634 - acc: 0.9268 - val_loss: 0.5645 - val_acc: 0.8580\n",
      "Epoch 47/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5656 - acc: 0.9248 - val_loss: 0.5642 - val_acc: 0.8591\n",
      "Epoch 48/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5622 - acc: 0.9258 - val_loss: 0.5647 - val_acc: 0.8586\n",
      "Epoch 49/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5644 - acc: 0.9249 - val_loss: 0.5646 - val_acc: 0.8586\n",
      "Epoch 50/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5636 - acc: 0.9238 - val_loss: 0.5649 - val_acc: 0.8586\n",
      "Epoch 51/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5638 - acc: 0.9242 - val_loss: 0.5646 - val_acc: 0.8580\n",
      "Epoch 52/300\n",
      "15156/15156 [==============================] - 73s 5ms/step - loss: 0.5626 - acc: 0.9247 - val_loss: 0.5648 - val_acc: 0.8596\n",
      "9473/9473 [==============================] - 26s 3ms/step\n",
      "9400/9400 [==============================] - 30s 3ms/step\n",
      "train shape:  (15160, 256, 431, 1) (15160, 41)\n",
      "val shape:  (1893, 256, 431, 1) (1893, 41)\n",
      "Fold:  3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_198 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_206 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_121 (MaxPoolin (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_199 (Conv2D)          (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_207 (Bat (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_122 (MaxPoolin (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_200 (Conv2D)          (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_208 (Bat (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_123 (MaxPoolin (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_201 (Conv2D)          (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_209 (Bat (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_124 (MaxPoolin (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_202 (Conv2D)          (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_210 (Bat (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_125 (MaxPoolin (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_203 (Conv2D)          (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_211 (Bat (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_20  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 15160 samples, validate on 1893 samples\n",
      "Epoch 1/300\n",
      "15160/15160 [==============================] - 78s 5ms/step - loss: 3.0699 - acc: 0.1951 - val_loss: 2.9394 - val_acc: 0.2166\n",
      "\n",
      "Validation loss decreased from inf to 2.9393865191867223, saving model\n",
      "Epoch 2/300\n",
      "15160/15160 [==============================] - 80s 5ms/step - loss: 2.4949 - acc: 0.3619 - val_loss: 2.0523 - val_acc: 0.4432\n",
      "\n",
      "Validation loss decreased from 2.9393865191867223 to 2.0522834550622777, saving model\n",
      "Epoch 3/300\n",
      "15160/15160 [==============================] - 74s 5ms/step - loss: 2.1316 - acc: 0.4789 - val_loss: 2.1703 - val_acc: 0.4036\n",
      "Epoch 4/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.8831 - acc: 0.5619 - val_loss: 1.8356 - val_acc: 0.4691\n",
      "\n",
      "Validation loss decreased from 2.0522834550622777 to 1.8355544385491889, saving model\n",
      "Epoch 5/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.7149 - acc: 0.6199 - val_loss: 1.5349 - val_acc: 0.6006\n",
      "\n",
      "Validation loss decreased from 1.8355544385491889 to 1.5348551248786189, saving model\n",
      "Epoch 6/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.5691 - acc: 0.6685 - val_loss: 1.4811 - val_acc: 0.6096\n",
      "\n",
      "Validation loss decreased from 1.5348551248786189 to 1.4810886563161536, saving model\n",
      "Epoch 7/300\n",
      "15160/15160 [==============================] - 74s 5ms/step - loss: 1.4607 - acc: 0.7040 - val_loss: 1.5849 - val_acc: 0.5880\n",
      "Epoch 8/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.3664 - acc: 0.7309 - val_loss: 1.6117 - val_acc: 0.5869\n",
      "Epoch 9/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.2850 - acc: 0.7554 - val_loss: 1.6495 - val_acc: 0.6059\n",
      "Epoch 10/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.2155 - acc: 0.7745 - val_loss: 1.2883 - val_acc: 0.6756\n",
      "\n",
      "Validation loss decreased from 1.4810886563161536 to 1.288329841406712, saving model\n",
      "Epoch 11/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.1454 - acc: 0.7957 - val_loss: 1.0704 - val_acc: 0.7264\n",
      "\n",
      "Validation loss decreased from 1.288329841406712 to 1.0703874046816375, saving model\n",
      "Epoch 12/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.0796 - acc: 0.8117 - val_loss: 1.2209 - val_acc: 0.6910\n",
      "Epoch 13/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 1.0331 - acc: 0.8250 - val_loss: 0.9576 - val_acc: 0.7538\n",
      "\n",
      "Validation loss decreased from 1.0703874046816375 to 0.9576300649307924, saving model\n",
      "Epoch 14/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.9853 - acc: 0.8352 - val_loss: 1.2222 - val_acc: 0.7137\n",
      "Epoch 15/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.9281 - acc: 0.8541 - val_loss: 1.0339 - val_acc: 0.7406\n",
      "Epoch 16/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.8893 - acc: 0.8618 - val_loss: 0.8790 - val_acc: 0.7845\n",
      "\n",
      "Validation loss decreased from 0.9576300649307924 to 0.8790385199802235, saving model\n",
      "Epoch 17/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.8515 - acc: 0.8708 - val_loss: 1.3233 - val_acc: 0.6609\n",
      "Epoch 18/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.8298 - acc: 0.8708 - val_loss: 1.1720 - val_acc: 0.7295\n",
      "Epoch 19/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.7184 - acc: 0.8949 - val_loss: 0.9383 - val_acc: 0.7723\n",
      "Epoch 22/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.6996 - acc: 0.8973 - val_loss: 1.0215 - val_acc: 0.7538\n",
      "Epoch 23/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5856 - acc: 0.9163 - val_loss: 0.5486 - val_acc: 0.8590\n",
      "\n",
      "Validation loss decreased from 0.8790385199802235 to 0.5486323537488998, saving model\n",
      "Epoch 24/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5517 - acc: 0.9227 - val_loss: 0.5506 - val_acc: 0.8590\n",
      "Epoch 25/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5387 - acc: 0.9255 - val_loss: 0.5586 - val_acc: 0.8637\n",
      "Epoch 26/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5324 - acc: 0.9250 - val_loss: 0.5513 - val_acc: 0.8621\n",
      "Epoch 27/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5249 - acc: 0.9246 - val_loss: 0.5512 - val_acc: 0.8611\n",
      "Epoch 28/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5172 - acc: 0.9255 - val_loss: 0.5543 - val_acc: 0.8627\n",
      "Epoch 29/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5121 - acc: 0.9262 - val_loss: 0.5554 - val_acc: 0.8621\n",
      "Epoch 30/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.5019 - acc: 0.9287 - val_loss: 0.5493 - val_acc: 0.8627\n",
      "Epoch 31/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4995 - acc: 0.9284 - val_loss: 0.5494 - val_acc: 0.8632\n",
      "Epoch 32/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4968 - acc: 0.9309 - val_loss: 0.5479 - val_acc: 0.8621\n",
      "\n",
      "Validation loss decreased from 0.5486323537488998 to 0.5479361142075635, saving model\n",
      "Epoch 33/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4999 - acc: 0.9270 - val_loss: 0.5499 - val_acc: 0.8627\n",
      "Epoch 34/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4967 - acc: 0.9293 - val_loss: 0.5494 - val_acc: 0.8627\n",
      "Epoch 35/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4963 - acc: 0.9307 - val_loss: 0.5510 - val_acc: 0.8637\n",
      "Epoch 36/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4968 - acc: 0.9289 - val_loss: 0.5486 - val_acc: 0.8632\n",
      "Epoch 37/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4949 - acc: 0.9286 - val_loss: 0.5493 - val_acc: 0.8642\n",
      "Epoch 38/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4955 - acc: 0.9297 - val_loss: 0.5501 - val_acc: 0.8621\n",
      "Epoch 39/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.4938 - acc: 0.9293 - val_loss: 0.5492 - val_acc: 0.8637\n",
      "Epoch 40/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.4936 - acc: 0.9290 - val_loss: 0.5493 - val_acc: 0.8632\n",
      "Epoch 41/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4925 - acc: 0.9268 - val_loss: 0.5491 - val_acc: 0.8627\n",
      "Epoch 42/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4946 - acc: 0.9265 - val_loss: 0.5492 - val_acc: 0.8627\n",
      "Epoch 43/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.4944 - acc: 0.9268 - val_loss: 0.5497 - val_acc: 0.8621\n",
      "Epoch 44/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.4932 - acc: 0.9305 - val_loss: 0.5495 - val_acc: 0.8621\n",
      "Epoch 45/300\n",
      "15160/15160 [==============================] - 72s 5ms/step - loss: 0.4943 - acc: 0.9271 - val_loss: 0.5496 - val_acc: 0.8616\n",
      "Epoch 46/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.4952 - acc: 0.9279 - val_loss: 0.5494 - val_acc: 0.8621\n",
      "Epoch 47/300\n",
      "15160/15160 [==============================] - 73s 5ms/step - loss: 0.4927 - acc: 0.9280 - val_loss: 0.5496 - val_acc: 0.8621\n",
      "9473/9473 [==============================] - 24s 3ms/step\n",
      "9400/9400 [==============================] - 26s 3ms/step\n",
      "train shape:  (15174, 256, 431, 1) (15174, 41)\n",
      "val shape:  (1886, 256, 431, 1) (1886, 41)\n",
      "Fold:  4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 256, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_204 (Conv2D)          (None, 256, 431, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_212 (Bat (None, 256, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 256, 431, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_126 (MaxPoolin (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_205 (Conv2D)          (None, 128, 215, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_213 (Bat (None, 128, 215, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 128, 215, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_127 (MaxPoolin (None, 64, 107, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_206 (Conv2D)          (None, 64, 107, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_214 (Bat (None, 64, 107, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 64, 107, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_128 (MaxPoolin (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_207 (Conv2D)          (None, 32, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_215 (Bat (None, 32, 53, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 32, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_129 (MaxPoolin (None, 16, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_208 (Conv2D)          (None, 16, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_216 (Bat (None, 16, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 16, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_130 (MaxPoolin (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_209 (Conv2D)          (None, 8, 13, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_217 (Bat (None, 8, 13, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 8, 13, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_21  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 293,513\n",
      "Trainable params: 292,617\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "26\n",
      "(256, 431, 1)\n",
      "Train on 15174 samples, validate on 1886 samples\n",
      "Epoch 1/300\n",
      "15174/15174 [==============================] - 79s 5ms/step - loss: 3.0243 - acc: 0.1999 - val_loss: 3.3194 - val_acc: 0.2116\n",
      "\n",
      "Validation loss decreased from inf to 3.319423122092497, saving model\n",
      "Epoch 2/300\n",
      "15174/15174 [==============================] - 78s 5ms/step - loss: 2.3757 - acc: 0.4050 - val_loss: 1.8565 - val_acc: 0.4846\n",
      "\n",
      "Validation loss decreased from 3.319423122092497 to 1.8565010686947205, saving model\n",
      "Epoch 3/300\n",
      "15174/15174 [==============================] - 77s 5ms/step - loss: 2.0299 - acc: 0.5146 - val_loss: 1.7023 - val_acc: 0.5345\n",
      "\n",
      "Validation loss decreased from 1.8565010686947205 to 1.70234156135284, saving model\n",
      "Epoch 4/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 1.8175 - acc: 0.5892 - val_loss: 1.9255 - val_acc: 0.5191\n",
      "Epoch 5/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.6710 - acc: 0.6394 - val_loss: 1.7637 - val_acc: 0.5477\n",
      "Epoch 6/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.5381 - acc: 0.6807 - val_loss: 1.6560 - val_acc: 0.5536\n",
      "\n",
      "Validation loss decreased from 1.70234156135284 to 1.655966705507337, saving model\n",
      "Epoch 7/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.4435 - acc: 0.7115 - val_loss: 1.3386 - val_acc: 0.6538\n",
      "\n",
      "Validation loss decreased from 1.655966705507337 to 1.338567156427612, saving model\n",
      "Epoch 8/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.3502 - acc: 0.7396 - val_loss: 1.3887 - val_acc: 0.6527\n",
      "Epoch 9/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.2871 - acc: 0.7536 - val_loss: 1.3338 - val_acc: 0.6633\n",
      "\n",
      "Validation loss decreased from 1.338567156427612 to 1.3337515674744784, saving model\n",
      "Epoch 10/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.2194 - acc: 0.7763 - val_loss: 1.2757 - val_acc: 0.6622\n",
      "\n",
      "Validation loss decreased from 1.3337515674744784 to 1.2757368080183658, saving model\n",
      "Epoch 11/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.1499 - acc: 0.7950 - val_loss: 1.1249 - val_acc: 0.7306\n",
      "\n",
      "Validation loss decreased from 1.2757368080183658 to 1.1248715140028192, saving model\n",
      "Epoch 12/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.0695 - acc: 0.8238 - val_loss: 1.5310 - val_acc: 0.6485\n",
      "Epoch 13/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 1.0455 - acc: 0.8218 - val_loss: 1.0365 - val_acc: 0.7492\n",
      "\n",
      "Validation loss decreased from 1.1248715140028192 to 1.0365046599004832, saving model\n",
      "Epoch 14/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.9689 - acc: 0.8437 - val_loss: 1.3106 - val_acc: 0.7010\n",
      "Epoch 15/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.9335 - acc: 0.8523 - val_loss: 1.2195 - val_acc: 0.7063\n",
      "Epoch 16/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.8839 - acc: 0.8656 - val_loss: 0.9531 - val_acc: 0.7487\n",
      "\n",
      "Validation loss decreased from 1.0365046599004832 to 0.9530621607761242, saving model\n",
      "Epoch 17/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.8525 - acc: 0.8676 - val_loss: 1.1670 - val_acc: 0.7126\n",
      "Epoch 18/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.8180 - acc: 0.8760 - val_loss: 1.3395 - val_acc: 0.6903\n",
      "Epoch 19/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.8211 - acc: 0.8727 - val_loss: 1.1715 - val_acc: 0.7020\n",
      "Epoch 20/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.7663 - acc: 0.8884 - val_loss: 1.0610 - val_acc: 0.7317\n",
      "Epoch 21/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.7357 - acc: 0.8910 - val_loss: 1.1912 - val_acc: 0.7105\n",
      "Epoch 22/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.7369 - acc: 0.8865 - val_loss: 1.6498 - val_acc: 0.6299\n",
      "Epoch 23/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.5989 - acc: 0.9162 - val_loss: 0.6046 - val_acc: 0.8574\n",
      "\n",
      "Validation loss decreased from 0.9530621607761242 to 0.6045974254102494, saving model\n",
      "Epoch 24/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.5642 - acc: 0.9214 - val_loss: 0.6021 - val_acc: 0.8584\n",
      "\n",
      "Validation loss decreased from 0.6045974254102494 to 0.6020616368802456, saving model\n",
      "Epoch 25/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.5534 - acc: 0.9216 - val_loss: 0.6065 - val_acc: 0.8616\n",
      "Epoch 26/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.5461 - acc: 0.9236 - val_loss: 0.5967 - val_acc: 0.8611\n",
      "\n",
      "Validation loss decreased from 0.6020616368802456 to 0.5966538209171841, saving model\n",
      "Epoch 27/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.5403 - acc: 0.9230 - val_loss: 0.6037 - val_acc: 0.8531\n",
      "Epoch 28/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.5321 - acc: 0.9234 - val_loss: 0.6071 - val_acc: 0.8579\n",
      "Epoch 29/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.5261 - acc: 0.9257 - val_loss: 0.6050 - val_acc: 0.8590\n",
      "Epoch 30/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.5195 - acc: 0.9258 - val_loss: 0.6145 - val_acc: 0.8537\n",
      "Epoch 31/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.5161 - acc: 0.9234 - val_loss: 0.6071 - val_acc: 0.8542\n",
      "Epoch 32/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.5127 - acc: 0.9231 - val_loss: 0.6192 - val_acc: 0.8558\n",
      "Epoch 33/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.5010 - acc: 0.9260 - val_loss: 0.6030 - val_acc: 0.8579\n",
      "Epoch 34/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.4983 - acc: 0.9303 - val_loss: 0.6047 - val_acc: 0.8563\n",
      "Epoch 35/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.4978 - acc: 0.9302 - val_loss: 0.6053 - val_acc: 0.8547\n",
      "Epoch 36/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.4962 - acc: 0.9306 - val_loss: 0.6054 - val_acc: 0.8568\n",
      "Epoch 37/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.4973 - acc: 0.9274 - val_loss: 0.6054 - val_acc: 0.8542\n",
      "Epoch 38/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.4952 - acc: 0.9280 - val_loss: 0.6061 - val_acc: 0.8547\n",
      "Epoch 39/300\n",
      "15174/15174 [==============================] - 75s 5ms/step - loss: 0.4960 - acc: 0.9273 - val_loss: 0.6058 - val_acc: 0.8552\n",
      "Epoch 40/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.4956 - acc: 0.9273 - val_loss: 0.6050 - val_acc: 0.8552\n",
      "Epoch 41/300\n",
      "15174/15174 [==============================] - 76s 5ms/step - loss: 0.4962 - acc: 0.9284 - val_loss: 0.6053 - val_acc: 0.8547\n",
      "9473/9473 [==============================] - 26s 3ms/step\n",
      "9400/9400 [==============================] - 26s 3ms/step\n",
      "CPU times: user 3h 3min 15s, sys: 1h 10min 33s, total: 4h 13min 48s\n",
      "Wall time: 4h 48min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# log_mel_sp_44100x5 \n",
    "config = Config(sampling_rate=44100, audio_duration=5, n_classes=41, use_log_mel_sp=True, n_folds=5, max_epochs=300, n_mfcc=128*2)\n",
    "PREDICTION_FOLDER = \"predictions_log_mel_sp_44000x5_mixup_2x\"\n",
    "CHECKPOINT_FOLDER = 'checkpionts_log_mel_sp_44000x5_mixup_2x'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "    X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "    y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = X_train[train_split]\n",
    "#     y = y_train[train_split]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model(config)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#     model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6, amsgrad=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=adam,\n",
    "                         metrics=['acc'])\n",
    "#     sgd = SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=sgd,\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "    #     checkpoint = ModelCheckpoint(CHECKPOINT_FOLDER+'/best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    checkpoint = CustomModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks_list = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=callbacks_list, \n",
    "                        batch_size=batch_size, epochs=config.max_epochs, shuffle=True)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
