{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/audio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/root/anaconda3/envs/audio/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456),\n",
       " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 10968078746)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import librosa\n",
    "import shutil\n",
    "import keras\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "\n",
    "from keras.optimizers import *\n",
    "\n",
    "from keras.regularizers import *\n",
    "\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.applications import *\n",
    "from keras.utils import *\n",
    "from keras.callbacks import *\n",
    "from sklearn.model_selection import *\n",
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session)\n",
    "from keras import backend as K\n",
    "K.get_session().list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../audio-data/')\n",
    "train_path = '../audio-data/audio_train/'\n",
    "test_path = '../audio-data/audio_test/'\n",
    "train = pd.read_csv('../audio-data/train.csv')\n",
    "test = pd.read_csv('../audio-data/sample_submission.csv')\n",
    "# print('training samples: ', len(os.listdir(train_path)))\n",
    "# print('test samples: ', len(os.listdir(test_path)))\n",
    "# print('training labels: ', len(train.label.unique()))\n",
    "# print(train.head())\n",
    "LABELS = list(train.label.unique())\n",
    "label_idx = {label: i for i, label in enumerate(LABELS)}\n",
    "train.set_index('fname', inplace=True)\n",
    "test.set_index('fname', inplace=True)\n",
    "train['label_idx'] = train.label.apply(lambda x: label_idx[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                sampling_rate=44100, audio_duration=2, n_classes=41,\n",
    "                use_mfcc=False, n_folds=10, learning_rate=0.0001,\n",
    "                max_epochs=50, n_mfcc=20, use_log_sp=False, use_mixup=False, alpha=0.2, use_log_mel_sp=False):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = n_classes\n",
    "        self.use_log_sp = use_log_sp\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.use_mixup = use_mixup\n",
    "        self.use_log_mel_sp = use_log_mel_sp\n",
    "        self.alpha = alpha\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_folds = n_folds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            # np.floor 计算比每一个元素小或相等的最大的整数\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        elif self.use_log_sp:\n",
    "            self.dim = (self.audio_duration*100-1, self.sampling_rate//100+1, 3)\n",
    "        elif self.use_log_mel_sp:\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram.shape[0] = time*100 - 1\n",
    "# spectrogram.shape[1] = rate/100 + 1\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "def audio_norm(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data - min_data) / (max_data - min_data + 1e-6)\n",
    "    return data - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = np.load('X_1d_24000x5_train.npy')\n",
    "X_test = np.load('X_1d_24000x5_test.npy')\n",
    "# X_train = np.load('X_1d_train.npy')\n",
    "# X_test = np.load('X_1d_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9473, 120000, 1) (9400, 120000, 1)\n",
      "CPU times: user 22.3 s, sys: 25.5 s, total: 47.8 s\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def normalization(x):\n",
    "    mean = np.mean(x, axis=0)\n",
    "    std = np.std(x, axis=0)\n",
    "    x = (x - mean) / std\n",
    "    return x\n",
    "X_train = normalization(X_train)\n",
    "X_test = normalization(X_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9473, 120000, 1) (9473, 41)\n",
      "CPU times: user 7.13 s, sys: 15.7 s, total: 22.8 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def mixup(data, targets, alpha):\n",
    "        size = data.shape[0]\n",
    "        weight = np.random.beta(alpha, alpha, size)\n",
    "      \n",
    "#         x_weight = weight.reshape(size, 1, 1, 1)\n",
    "        x_weight = weight.reshape(size, 1, 1)\n",
    "        y_weight = weight.reshape(size, 1)\n",
    "        index = np.random.permutation(size)\n",
    "        x1, x2 = data, data[index]\n",
    "        x = x1 * x_weight + x2 * (1 - x_weight)\n",
    "        y1, y2 = targets, targets[index]\n",
    "        y = y1 * y_weight + y2 * (1 - y_weight)\n",
    "        return x, y\n",
    "X_train_mixup, y_train_mixup = mixup(X_train, y_train, 2)\n",
    "print(X_train_mixup.shape, y_train_mixup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BLOCK(seq, filters): # 定义网络的Block\n",
    "    cnn = Conv1D(filters*2, 3, padding='SAME', dilation_rate=1, activation='relu')(seq)\n",
    "    cnn = Lambda(lambda x: x[:,:,:filters] + x[:,:,filters:])(cnn)\n",
    "    cnn = Conv1D(filters*2, 3, padding='SAME', dilation_rate=2, activation='relu')(cnn)\n",
    "    cnn = Lambda(lambda x: x[:,:,:filters] + x[:,:,filters:])(cnn)\n",
    "    cnn = Conv1D(filters*2, 3, padding='SAME', dilation_rate=4, activation='relu')(cnn)\n",
    "    cnn = Lambda(lambda x: x[:,:,:filters] + x[:,:,filters:])(cnn)\n",
    "    if int(seq.shape[-1]) != filters:\n",
    "        seq = Conv1D(filters, 1, padding='SAME')(seq)\n",
    "    seq = add([seq, cnn])\n",
    "    return seq\n",
    "\n",
    "\n",
    "def get_model():\n",
    "#     with tf.device('/cpu:0'):\n",
    "        #搭建模型，就是常规的CNN加残差加池化\n",
    "#         input_tensor = Input(shape=(16000, 1))\n",
    "    x_in = Input(shape=(24000*5, 1))\n",
    "\n",
    "    seq = BLOCK(x_in, 32)\n",
    "    seq = MaxPooling1D(2)(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "    seq = BLOCK(seq, 32)\n",
    "    seq = MaxPooling1D(2)(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "    seq = BLOCK(seq, 64)\n",
    "    seq = MaxPooling1D(2)(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "    seq = BLOCK(seq, 64)\n",
    "    seq = MaxPooling1D(2)(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "    seq = BLOCK(seq, 128)\n",
    "    seq = MaxPooling1D(2)(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "    seq = BLOCK(seq, 128)\n",
    "    seq = MaxPooling1D(2)(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "    seq = BLOCK(seq, 256)\n",
    "    seq = MaxPooling1D(2)(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "    seq = BLOCK(seq, 256)\n",
    "#     seq = Dropout(0.5, (128, int(seq.shape[1]), 1))(seq)\n",
    "#     seq = Dropout(0.5)(seq)\n",
    "    seq = GlobalAveragePooling1D()(seq)\n",
    "    seq = BatchNormalization()(seq)\n",
    "#     seq = Dense(256, activation='relu')(seq)\n",
    "#     seq = Dropout(0.5)(seq)\n",
    "#     seq = Dense(256, activation='relu')(seq)\n",
    "#     seq = Dropout(0.5)(seq)\n",
    "    \n",
    "    output = Dense(41, activation='softmax')(seq)\n",
    "\n",
    "    model = Model(inputs=x_in, outputs=output)\n",
    "    print(len(model.layers))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self,model,filepath, monitor='val_loss', verbose=1,\n",
    "                 save_best_only=True, save_weights_only=True,\n",
    "                 mode='auto', period=1):\n",
    "        self.single_model = model\n",
    "        super(ParallelModelCheckpoint,self).__init__(filepath, monitor, verbose,save_best_only, save_weights_only,mode, period)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        super(ParallelModelCheckpoint,self).set_model(self.single_model)\n",
    "\n",
    "\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, path):\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        self.best_loss = np.inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs['val_loss']\n",
    "        if val_loss < self.best_loss:\n",
    "            print(\"\\nValidation loss decreased from {} to {}, saving model\".format(self.best_loss, val_loss))\n",
    "            self.model.save_weights(self.path, overwrite=True)\n",
    "            self.best_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (18946, 120000, 1) (18946, 41)\n",
      "Fold:  99\n",
      "78\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 120000, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 120000, 64)   256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 120000, 32)   0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 120000, 64)   6208        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 120000, 32)   0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 120000, 64)   6208        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 120000, 32)   64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 120000, 32)   0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 120000, 32)   0           conv1d_4[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 60000, 32)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 60000, 32)    128         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 60000, 64)    6208        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 60000, 32)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 60000, 64)    6208        lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 60000, 32)    0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 60000, 64)    6208        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 60000, 32)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 60000, 32)    0           batch_normalization_1[0][0]      \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 30000, 32)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 30000, 32)    128         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 30000, 128)   12416       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 30000, 64)    0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 30000, 128)   24704       lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 30000, 64)    0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 30000, 128)   24704       lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 30000, 64)    2112        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 30000, 64)    0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 30000, 64)    0           conv1d_11[0][0]                  \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 15000, 64)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15000, 64)    256         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 15000, 128)   24704       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 15000, 64)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 15000, 128)   24704       lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 15000, 64)    0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 15000, 128)   24704       lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 15000, 64)    0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 15000, 64)    0           batch_normalization_3[0][0]      \n",
      "                                                                 lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 7500, 64)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7500, 64)     256         max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 7500, 256)    49408       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 7500, 128)    0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 7500, 256)    98560       lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 7500, 128)    0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 7500, 256)    98560       lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 7500, 128)    8320        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 7500, 128)    0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 7500, 128)    0           conv1d_18[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 3750, 128)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 3750, 128)    512         max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 3750, 256)    98560       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 3750, 128)    0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 3750, 256)    98560       lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 3750, 128)    0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 3750, 256)    98560       lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 3750, 128)    0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 3750, 128)    0           batch_normalization_5[0][0]      \n",
      "                                                                 lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1875, 128)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1875, 128)    512         max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1875, 512)    197120      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 1875, 256)    0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1875, 512)    393728      lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1875, 256)    0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1875, 512)    393728      lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 1875, 256)    33024       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 1875, 256)    0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 1875, 256)    0           conv1d_25[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 937, 256)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 937, 256)     1024        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 937, 512)     393728      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 937, 256)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 937, 512)     393728      lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 937, 256)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 937, 512)     393728      lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 937, 256)     0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 937, 256)     0           batch_normalization_7[0][0]      \n",
      "                                                                 lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256)          1024        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 41)           10537       batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,933,097\n",
      "Trainable params: 2,931,177\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "18946/18946 [==============================] - 345s 18ms/step - loss: 3.6708 - acc: 0.0940\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.67075, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 2/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 3.1980 - acc: 0.1689\n",
      "\n",
      "Epoch 00002: loss improved from 3.67075 to 3.19799, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 3/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 3.0288 - acc: 0.2248\n",
      "\n",
      "Epoch 00003: loss improved from 3.19799 to 3.02879, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 4/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 2.9249 - acc: 0.2576\n",
      "\n",
      "Epoch 00004: loss improved from 3.02879 to 2.92488, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 5/300\n",
      "18946/18946 [==============================] - 334s 18ms/step - loss: 2.7960 - acc: 0.2968\n",
      "\n",
      "Epoch 00005: loss improved from 2.92488 to 2.79600, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 6/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 2.7091 - acc: 0.3192\n",
      "\n",
      "Epoch 00006: loss improved from 2.79600 to 2.70910, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 7/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 2.6585 - acc: 0.3367\n",
      "\n",
      "Epoch 00007: loss improved from 2.70910 to 2.65845, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 8/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.6080 - acc: 0.3501\n",
      "\n",
      "Epoch 00008: loss improved from 2.65845 to 2.60802, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 9/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 2.5642 - acc: 0.3668\n",
      "\n",
      "Epoch 00009: loss improved from 2.60802 to 2.56422, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 10/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.5174 - acc: 0.3783\n",
      "\n",
      "Epoch 00010: loss improved from 2.56422 to 2.51738, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 11/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.4773 - acc: 0.3889\n",
      "\n",
      "Epoch 00011: loss improved from 2.51738 to 2.47735, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 12/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 2.4965 - acc: 0.3776\n",
      "\n",
      "Epoch 00012: loss did not improve\n",
      "Epoch 13/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.5893 - acc: 0.3514\n",
      "\n",
      "Epoch 00013: loss did not improve\n",
      "Epoch 14/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.4342 - acc: 0.4005\n",
      "\n",
      "Epoch 00014: loss improved from 2.47735 to 2.43417, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 15/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.3851 - acc: 0.4147\n",
      "\n",
      "Epoch 00015: loss improved from 2.43417 to 2.38514, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 16/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 2.3296 - acc: 0.4278\n",
      "\n",
      "Epoch 00016: loss improved from 2.38514 to 2.32959, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 17/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 2.2947 - acc: 0.4383\n",
      "\n",
      "Epoch 00017: loss improved from 2.32959 to 2.29468, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 18/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.2687 - acc: 0.4519\n",
      "\n",
      "Epoch 00018: loss improved from 2.29468 to 2.26868, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 19/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.2561 - acc: 0.4493\n",
      "\n",
      "Epoch 00019: loss improved from 2.26868 to 2.25610, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 20/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.1984 - acc: 0.4689\n",
      "\n",
      "Epoch 00020: loss improved from 2.25610 to 2.19842, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 21/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.1708 - acc: 0.4762\n",
      "\n",
      "Epoch 00021: loss improved from 2.19842 to 2.17079, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 22/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 2.1596 - acc: 0.4774\n",
      "\n",
      "Epoch 00022: loss improved from 2.17079 to 2.15962, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 23/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 2.1359 - acc: 0.4844\n",
      "\n",
      "Epoch 00023: loss improved from 2.15962 to 2.13591, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 24/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 2.0859 - acc: 0.5007\n",
      "\n",
      "Epoch 00024: loss improved from 2.13591 to 2.08595, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 25/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.0472 - acc: 0.5098\n",
      "\n",
      "Epoch 00025: loss improved from 2.08595 to 2.04719, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 26/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.0269 - acc: 0.5178\n",
      "\n",
      "Epoch 00026: loss improved from 2.04719 to 2.02686, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 27/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 2.0167 - acc: 0.5179\n",
      "\n",
      "Epoch 00027: loss improved from 2.02686 to 2.01669, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 28/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.9687 - acc: 0.5343\n",
      "\n",
      "Epoch 00028: loss improved from 2.01669 to 1.96871, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 29/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.9571 - acc: 0.5343\n",
      "\n",
      "Epoch 00029: loss improved from 1.96871 to 1.95706, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 30/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.9066 - acc: 0.5506\n",
      "\n",
      "Epoch 00030: loss improved from 1.95706 to 1.90660, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 31/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.8888 - acc: 0.5552\n",
      "\n",
      "Epoch 00031: loss improved from 1.90660 to 1.88878, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 32/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.8796 - acc: 0.5594\n",
      "\n",
      "Epoch 00032: loss improved from 1.88878 to 1.87962, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 33/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.8745 - acc: 0.5585\n",
      "\n",
      "Epoch 00033: loss improved from 1.87962 to 1.87450, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 34/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.8437 - acc: 0.5685\n",
      "\n",
      "Epoch 00034: loss improved from 1.87450 to 1.84365, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 35/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.8049 - acc: 0.5755\n",
      "\n",
      "Epoch 00035: loss improved from 1.84365 to 1.80495, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 36/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.7844 - acc: 0.5865\n",
      "\n",
      "Epoch 00036: loss improved from 1.80495 to 1.78442, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 37/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.7620 - acc: 0.5898\n",
      "\n",
      "Epoch 00037: loss improved from 1.78442 to 1.76196, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 38/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.7449 - acc: 0.5986\n",
      "\n",
      "Epoch 00038: loss improved from 1.76196 to 1.74493, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 39/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.7366 - acc: 0.5998\n",
      "\n",
      "Epoch 00039: loss improved from 1.74493 to 1.73659, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 40/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.7447 - acc: 0.5970\n",
      "\n",
      "Epoch 00040: loss did not improve\n",
      "Epoch 41/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.7041 - acc: 0.6065\n",
      "\n",
      "Epoch 00041: loss improved from 1.73659 to 1.70415, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 42/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.6656 - acc: 0.6187\n",
      "\n",
      "Epoch 00042: loss improved from 1.70415 to 1.66565, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 43/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.6403 - acc: 0.6218\n",
      "\n",
      "Epoch 00043: loss improved from 1.66565 to 1.64027, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 44/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.6143 - acc: 0.6355\n",
      "\n",
      "Epoch 00044: loss improved from 1.64027 to 1.61431, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 45/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.6131 - acc: 0.6315\n",
      "\n",
      "Epoch 00045: loss improved from 1.61431 to 1.61315, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 46/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.5968 - acc: 0.6383\n",
      "\n",
      "Epoch 00046: loss improved from 1.61315 to 1.59680, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 47/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.5579 - acc: 0.6459\n",
      "\n",
      "Epoch 00047: loss improved from 1.59680 to 1.55787, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 48/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.5410 - acc: 0.6514\n",
      "\n",
      "Epoch 00048: loss improved from 1.55787 to 1.54104, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 49/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.5221 - acc: 0.6589\n",
      "\n",
      "Epoch 00049: loss improved from 1.54104 to 1.52205, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 50/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.4977 - acc: 0.6652\n",
      "\n",
      "Epoch 00050: loss improved from 1.52205 to 1.49767, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 51/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.5114 - acc: 0.6608\n",
      "\n",
      "Epoch 00051: loss did not improve\n",
      "Epoch 52/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.4739 - acc: 0.6693\n",
      "\n",
      "Epoch 00052: loss improved from 1.49767 to 1.47388, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 53/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.4449 - acc: 0.6814\n",
      "\n",
      "Epoch 00053: loss improved from 1.47388 to 1.44487, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 54/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 1.4473 - acc: 0.6740\n",
      "\n",
      "Epoch 00054: loss did not improve\n",
      "Epoch 55/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.4260 - acc: 0.6856\n",
      "\n",
      "Epoch 00055: loss improved from 1.44487 to 1.42596, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 56/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.3972 - acc: 0.6922\n",
      "\n",
      "Epoch 00056: loss improved from 1.42596 to 1.39725, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 57/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.3924 - acc: 0.6919\n",
      "\n",
      "Epoch 00057: loss improved from 1.39725 to 1.39236, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 58/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.3549 - acc: 0.7061\n",
      "\n",
      "Epoch 00058: loss improved from 1.39236 to 1.35495, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 59/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.3402 - acc: 0.7058\n",
      "\n",
      "Epoch 00059: loss improved from 1.35495 to 1.34020, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 60/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.3720 - acc: 0.6918\n",
      "\n",
      "Epoch 00060: loss did not improve\n",
      "Epoch 61/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 1.3087 - acc: 0.7137\n",
      "\n",
      "Epoch 00061: loss improved from 1.34020 to 1.30870, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 62/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.2782 - acc: 0.7227\n",
      "\n",
      "Epoch 00062: loss improved from 1.30870 to 1.27822, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 63/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.2854 - acc: 0.7209\n",
      "\n",
      "Epoch 00063: loss did not improve\n",
      "Epoch 64/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.2580 - acc: 0.7285\n",
      "\n",
      "Epoch 00064: loss improved from 1.27822 to 1.25801, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 65/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.2310 - acc: 0.7349\n",
      "\n",
      "Epoch 00065: loss improved from 1.25801 to 1.23099, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 66/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.2101 - acc: 0.7428\n",
      "\n",
      "Epoch 00066: loss improved from 1.23099 to 1.21013, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 67/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.2005 - acc: 0.7426\n",
      "\n",
      "Epoch 00067: loss improved from 1.21013 to 1.20049, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 68/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.1833 - acc: 0.7485\n",
      "\n",
      "Epoch 00068: loss improved from 1.20049 to 1.18325, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 69/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.1639 - acc: 0.7532\n",
      "\n",
      "Epoch 00069: loss improved from 1.18325 to 1.16393, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 70/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.1464 - acc: 0.7597\n",
      "\n",
      "Epoch 00070: loss improved from 1.16393 to 1.14636, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 71/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.1355 - acc: 0.7611\n",
      "\n",
      "Epoch 00071: loss improved from 1.14636 to 1.13545, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 72/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.1257 - acc: 0.7652\n",
      "\n",
      "Epoch 00072: loss improved from 1.13545 to 1.12570, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 73/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.1145 - acc: 0.7643\n",
      "\n",
      "Epoch 00073: loss improved from 1.12570 to 1.11454, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 74/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0896 - acc: 0.7744\n",
      "\n",
      "Epoch 00074: loss improved from 1.11454 to 1.08957, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 75/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0888 - acc: 0.7764\n",
      "\n",
      "Epoch 00075: loss improved from 1.08957 to 1.08877, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 76/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0803 - acc: 0.7730\n",
      "\n",
      "Epoch 00076: loss improved from 1.08877 to 1.08026, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 77/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0508 - acc: 0.7815\n",
      "\n",
      "Epoch 00077: loss improved from 1.08026 to 1.05079, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 78/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0429 - acc: 0.7850\n",
      "\n",
      "Epoch 00078: loss improved from 1.05079 to 1.04293, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 79/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 1.0397 - acc: 0.7846\n",
      "\n",
      "Epoch 00079: loss improved from 1.04293 to 1.03966, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 80/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0246 - acc: 0.7903\n",
      "\n",
      "Epoch 00080: loss improved from 1.03966 to 1.02457, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 81/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 1.0118 - acc: 0.7949\n",
      "\n",
      "Epoch 00081: loss improved from 1.02457 to 1.01182, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 82/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0012 - acc: 0.7959\n",
      "\n",
      "Epoch 00082: loss improved from 1.01182 to 1.00120, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 83/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 1.0100 - acc: 0.7912\n",
      "\n",
      "Epoch 00083: loss did not improve\n",
      "Epoch 84/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.9755 - acc: 0.8042\n",
      "\n",
      "Epoch 00084: loss improved from 1.00120 to 0.97548, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 85/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.9604 - acc: 0.8065\n",
      "\n",
      "Epoch 00085: loss improved from 0.97548 to 0.96044, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 86/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.9525 - acc: 0.8066\n",
      "\n",
      "Epoch 00086: loss improved from 0.96044 to 0.95251, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 87/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.9459 - acc: 0.8094\n",
      "\n",
      "Epoch 00087: loss improved from 0.95251 to 0.94586, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 88/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.9435 - acc: 0.8057\n",
      "\n",
      "Epoch 00088: loss improved from 0.94586 to 0.94346, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 89/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.9321 - acc: 0.8113\n",
      "\n",
      "Epoch 00089: loss improved from 0.94346 to 0.93207, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 90/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.9240 - acc: 0.8118\n",
      "\n",
      "Epoch 00090: loss improved from 0.93207 to 0.92403, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 91/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.9145 - acc: 0.8149\n",
      "\n",
      "Epoch 00091: loss improved from 0.92403 to 0.91453, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 92/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.9124 - acc: 0.8150\n",
      "\n",
      "Epoch 00092: loss improved from 0.91453 to 0.91241, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 93/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.9123 - acc: 0.8149\n",
      "\n",
      "Epoch 00093: loss improved from 0.91241 to 0.91229, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 94/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.9023 - acc: 0.8182\n",
      "\n",
      "Epoch 00094: loss improved from 0.91229 to 0.90228, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 95/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.8737 - acc: 0.8263\n",
      "\n",
      "Epoch 00095: loss improved from 0.90228 to 0.87373, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 96/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8653 - acc: 0.8280\n",
      "\n",
      "Epoch 00096: loss improved from 0.87373 to 0.86534, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 97/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.8555 - acc: 0.8310\n",
      "\n",
      "Epoch 00097: loss improved from 0.86534 to 0.85549, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 98/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8527 - acc: 0.8286\n",
      "\n",
      "Epoch 00098: loss improved from 0.85549 to 0.85274, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 99/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8528 - acc: 0.8288\n",
      "\n",
      "Epoch 00099: loss did not improve\n",
      "Epoch 100/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8441 - acc: 0.8309\n",
      "\n",
      "Epoch 00100: loss improved from 0.85274 to 0.84412, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 101/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8238 - acc: 0.8407\n",
      "\n",
      "Epoch 00101: loss improved from 0.84412 to 0.82384, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 102/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8234 - acc: 0.8336\n",
      "\n",
      "Epoch 00102: loss improved from 0.82384 to 0.82339, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 103/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.8318 - acc: 0.8346\n",
      "\n",
      "Epoch 00103: loss did not improve\n",
      "Epoch 104/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.8132 - acc: 0.8339\n",
      "\n",
      "Epoch 00104: loss improved from 0.82339 to 0.81325, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 105/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8026 - acc: 0.8393\n",
      "\n",
      "Epoch 00105: loss improved from 0.81325 to 0.80259, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 106/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8059 - acc: 0.8367\n",
      "\n",
      "Epoch 00106: loss did not improve\n",
      "Epoch 107/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.8009 - acc: 0.8373\n",
      "\n",
      "Epoch 00107: loss improved from 0.80259 to 0.80088, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 108/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.7867 - acc: 0.8397\n",
      "\n",
      "Epoch 00108: loss improved from 0.80088 to 0.78667, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 109/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7910 - acc: 0.8419\n",
      "\n",
      "Epoch 00109: loss did not improve\n",
      "Epoch 110/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7858 - acc: 0.8433\n",
      "\n",
      "Epoch 00110: loss improved from 0.78667 to 0.78580, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 111/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7752 - acc: 0.8432\n",
      "\n",
      "Epoch 00111: loss improved from 0.78580 to 0.77525, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 112/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7691 - acc: 0.8430\n",
      "\n",
      "Epoch 00112: loss improved from 0.77525 to 0.76907, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 113/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7633 - acc: 0.8415\n",
      "\n",
      "Epoch 00113: loss improved from 0.76907 to 0.76330, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 114/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7613 - acc: 0.8470\n",
      "\n",
      "Epoch 00114: loss improved from 0.76330 to 0.76127, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 115/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.7547 - acc: 0.8491\n",
      "\n",
      "Epoch 00115: loss improved from 0.76127 to 0.75465, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 116/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7551 - acc: 0.8487\n",
      "\n",
      "Epoch 00116: loss did not improve\n",
      "Epoch 117/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7478 - acc: 0.8492\n",
      "\n",
      "Epoch 00117: loss improved from 0.75465 to 0.74781, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 118/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7397 - acc: 0.8522\n",
      "\n",
      "Epoch 00118: loss improved from 0.74781 to 0.73971, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 119/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7373 - acc: 0.8490\n",
      "\n",
      "Epoch 00119: loss improved from 0.73971 to 0.73730, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 120/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7380 - acc: 0.8519\n",
      "\n",
      "Epoch 00120: loss did not improve\n",
      "Epoch 121/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7207 - acc: 0.8536\n",
      "\n",
      "Epoch 00121: loss improved from 0.73730 to 0.72068, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 122/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.7246 - acc: 0.8576\n",
      "\n",
      "Epoch 00122: loss did not improve\n",
      "Epoch 123/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7230 - acc: 0.8561\n",
      "\n",
      "Epoch 00123: loss did not improve\n",
      "Epoch 124/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.7133 - acc: 0.8594\n",
      "\n",
      "Epoch 00124: loss improved from 0.72068 to 0.71330, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 125/300\n",
      "18946/18946 [==============================] - 334s 18ms/step - loss: 0.7156 - acc: 0.8571\n",
      "\n",
      "Epoch 00125: loss did not improve\n",
      "Epoch 126/300\n",
      "18946/18946 [==============================] - 334s 18ms/step - loss: 0.7145 - acc: 0.8554\n",
      "\n",
      "Epoch 00126: loss did not improve\n",
      "Epoch 127/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.7125 - acc: 0.8574\n",
      "\n",
      "Epoch 00127: loss improved from 0.71330 to 0.71251, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 128/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.6961 - acc: 0.8568\n",
      "\n",
      "Epoch 00128: loss improved from 0.71251 to 0.69614, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 129/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.7002 - acc: 0.8547\n",
      "\n",
      "Epoch 00129: loss did not improve\n",
      "Epoch 130/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.6972 - acc: 0.8559\n",
      "\n",
      "Epoch 00130: loss did not improve\n",
      "Epoch 131/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.7464 - acc: 0.8453\n",
      "\n",
      "Epoch 00131: loss did not improve\n",
      "Epoch 132/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.7046 - acc: 0.8543\n",
      "\n",
      "Epoch 00132: loss did not improve\n",
      "Epoch 133/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.6956 - acc: 0.8576\n",
      "\n",
      "Epoch 00133: loss improved from 0.69614 to 0.69564, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 134/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.6905 - acc: 0.8580\n",
      "\n",
      "Epoch 00134: loss improved from 0.69564 to 0.69047, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 135/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.6888 - acc: 0.8571\n",
      "\n",
      "Epoch 00135: loss improved from 0.69047 to 0.68883, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 136/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.6847 - acc: 0.8578\n",
      "\n",
      "Epoch 00136: loss improved from 0.68883 to 0.68472, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 137/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.6743 - acc: 0.8611\n",
      "\n",
      "Epoch 00137: loss improved from 0.68472 to 0.67435, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 138/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.6794 - acc: 0.8616\n",
      "\n",
      "Epoch 00138: loss did not improve\n",
      "Epoch 139/300\n",
      "18946/18946 [==============================] - 338s 18ms/step - loss: 0.6801 - acc: 0.8616\n",
      "\n",
      "Epoch 00139: loss did not improve\n",
      "Epoch 140/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.6759 - acc: 0.8614\n",
      "\n",
      "Epoch 00140: loss did not improve\n",
      "Epoch 141/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.6678 - acc: 0.8606\n",
      "\n",
      "Epoch 00141: loss improved from 0.67435 to 0.66781, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 142/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.6620 - acc: 0.8603\n",
      "\n",
      "Epoch 00142: loss improved from 0.66781 to 0.66204, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 143/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.6654 - acc: 0.8637\n",
      "\n",
      "Epoch 00143: loss did not improve\n",
      "Epoch 144/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.6603 - acc: 0.8637\n",
      "\n",
      "Epoch 00144: loss improved from 0.66204 to 0.66025, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 145/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.6563 - acc: 0.8621\n",
      "\n",
      "Epoch 00145: loss improved from 0.66025 to 0.65634, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 146/300\n",
      "18946/18946 [==============================] - 337s 18ms/step - loss: 0.6519 - acc: 0.8671\n",
      "\n",
      "Epoch 00146: loss improved from 0.65634 to 0.65190, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 147/300\n",
      "18946/18946 [==============================] - 333s 18ms/step - loss: 0.6474 - acc: 0.8661\n",
      "\n",
      "Epoch 00147: loss improved from 0.65190 to 0.64737, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 148/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6534 - acc: 0.8632\n",
      "\n",
      "Epoch 00148: loss did not improve\n",
      "Epoch 149/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6455 - acc: 0.8662\n",
      "\n",
      "Epoch 00149: loss improved from 0.64737 to 0.64554, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 150/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6381 - acc: 0.8678\n",
      "\n",
      "Epoch 00150: loss improved from 0.64554 to 0.63809, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 151/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6483 - acc: 0.8658\n",
      "\n",
      "Epoch 00151: loss did not improve\n",
      "Epoch 152/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6411 - acc: 0.8633\n",
      "\n",
      "Epoch 00152: loss did not improve\n",
      "Epoch 153/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6421 - acc: 0.8630\n",
      "\n",
      "Epoch 00153: loss did not improve\n",
      "Epoch 154/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6396 - acc: 0.8647\n",
      "\n",
      "Epoch 00154: loss did not improve\n",
      "Epoch 155/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6343 - acc: 0.8659\n",
      "\n",
      "Epoch 00155: loss improved from 0.63809 to 0.63434, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 156/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6264 - acc: 0.8680\n",
      "\n",
      "Epoch 00156: loss improved from 0.63434 to 0.62640, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 157/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6253 - acc: 0.8663\n",
      "\n",
      "Epoch 00157: loss improved from 0.62640 to 0.62530, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 158/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6319 - acc: 0.8650\n",
      "\n",
      "Epoch 00158: loss did not improve\n",
      "Epoch 159/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6252 - acc: 0.8659\n",
      "\n",
      "Epoch 00159: loss improved from 0.62530 to 0.62518, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 160/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6192 - acc: 0.8695\n",
      "\n",
      "Epoch 00160: loss improved from 0.62518 to 0.61923, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 161/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6278 - acc: 0.8606\n",
      "\n",
      "Epoch 00161: loss did not improve\n",
      "Epoch 162/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6161 - acc: 0.8669\n",
      "\n",
      "Epoch 00162: loss improved from 0.61923 to 0.61607, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 163/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6223 - acc: 0.8665\n",
      "\n",
      "Epoch 00163: loss did not improve\n",
      "Epoch 164/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6195 - acc: 0.8713\n",
      "\n",
      "Epoch 00164: loss did not improve\n",
      "Epoch 165/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6094 - acc: 0.8711\n",
      "\n",
      "Epoch 00165: loss improved from 0.61607 to 0.60944, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 166/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6057 - acc: 0.8725\n",
      "\n",
      "Epoch 00166: loss improved from 0.60944 to 0.60568, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 167/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6080 - acc: 0.8698\n",
      "\n",
      "Epoch 00167: loss did not improve\n",
      "Epoch 168/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6092 - acc: 0.8717\n",
      "\n",
      "Epoch 00168: loss did not improve\n",
      "Epoch 169/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6066 - acc: 0.8698\n",
      "\n",
      "Epoch 00169: loss did not improve\n",
      "Epoch 170/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.6057 - acc: 0.8680\n",
      "\n",
      "Epoch 00170: loss improved from 0.60568 to 0.60566, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 171/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.6040 - acc: 0.8698\n",
      "\n",
      "Epoch 00171: loss improved from 0.60566 to 0.60398, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 172/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5973 - acc: 0.8749\n",
      "\n",
      "Epoch 00172: loss improved from 0.60398 to 0.59733, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 173/300\n",
      "18946/18946 [==============================] - 332s 17ms/step - loss: 0.5942 - acc: 0.8744\n",
      "\n",
      "Epoch 00173: loss improved from 0.59733 to 0.59421, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 174/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5996 - acc: 0.8676\n",
      "\n",
      "Epoch 00174: loss did not improve\n",
      "Epoch 175/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5981 - acc: 0.8677\n",
      "\n",
      "Epoch 00175: loss did not improve\n",
      "Epoch 176/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5939 - acc: 0.8685\n",
      "\n",
      "Epoch 00176: loss improved from 0.59421 to 0.59386, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 177/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5958 - acc: 0.8705\n",
      "\n",
      "Epoch 00177: loss did not improve\n",
      "Epoch 178/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5886 - acc: 0.8693\n",
      "\n",
      "Epoch 00178: loss improved from 0.59386 to 0.58857, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 179/300\n",
      "18946/18946 [==============================] - 332s 17ms/step - loss: 0.5879 - acc: 0.8692\n",
      "\n",
      "Epoch 00179: loss improved from 0.58857 to 0.58791, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 180/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5859 - acc: 0.8727\n",
      "\n",
      "Epoch 00180: loss improved from 0.58791 to 0.58586, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 181/300\n",
      "18946/18946 [==============================] - 332s 17ms/step - loss: 0.5924 - acc: 0.8680\n",
      "\n",
      "Epoch 00181: loss did not improve\n",
      "Epoch 182/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5889 - acc: 0.8714\n",
      "\n",
      "Epoch 00182: loss did not improve\n",
      "Epoch 183/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5849 - acc: 0.8699\n",
      "\n",
      "Epoch 00183: loss improved from 0.58586 to 0.58492, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 184/300\n",
      "18946/18946 [==============================] - 332s 17ms/step - loss: 0.5792 - acc: 0.8708\n",
      "\n",
      "Epoch 00184: loss improved from 0.58492 to 0.57918, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 185/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5813 - acc: 0.8741\n",
      "\n",
      "Epoch 00185: loss did not improve\n",
      "Epoch 186/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5779 - acc: 0.8741\n",
      "\n",
      "Epoch 00186: loss improved from 0.57918 to 0.57790, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 187/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5770 - acc: 0.8704\n",
      "\n",
      "Epoch 00187: loss improved from 0.57790 to 0.57697, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 188/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5799 - acc: 0.8700\n",
      "\n",
      "Epoch 00188: loss did not improve\n",
      "Epoch 189/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5779 - acc: 0.8756\n",
      "\n",
      "Epoch 00189: loss did not improve\n",
      "Epoch 190/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5730 - acc: 0.8767\n",
      "\n",
      "Epoch 00190: loss improved from 0.57697 to 0.57299, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 191/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5702 - acc: 0.8712\n",
      "\n",
      "Epoch 00191: loss improved from 0.57299 to 0.57023, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 192/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5758 - acc: 0.8741\n",
      "\n",
      "Epoch 00192: loss did not improve\n",
      "Epoch 193/300\n",
      "18946/18946 [==============================] - 332s 17ms/step - loss: 0.5704 - acc: 0.8714\n",
      "\n",
      "Epoch 00193: loss did not improve\n",
      "Epoch 194/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5693 - acc: 0.8774\n",
      "\n",
      "Epoch 00194: loss improved from 0.57023 to 0.56927, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 195/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5645 - acc: 0.8735\n",
      "\n",
      "Epoch 00195: loss improved from 0.56927 to 0.56450, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 196/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5653 - acc: 0.8754\n",
      "\n",
      "Epoch 00196: loss did not improve\n",
      "Epoch 197/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5678 - acc: 0.8746\n",
      "\n",
      "Epoch 00197: loss did not improve\n",
      "Epoch 198/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5656 - acc: 0.8730\n",
      "\n",
      "Epoch 00198: loss did not improve\n",
      "Epoch 199/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5673 - acc: 0.8768\n",
      "\n",
      "Epoch 00199: loss did not improve\n",
      "Epoch 200/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5621 - acc: 0.8732\n",
      "\n",
      "Epoch 00200: loss improved from 0.56450 to 0.56213, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5610 - acc: 0.8743\n",
      "\n",
      "Epoch 00201: loss improved from 0.56213 to 0.56103, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 202/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5659 - acc: 0.8734\n",
      "\n",
      "Epoch 00202: loss did not improve\n",
      "Epoch 203/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5611 - acc: 0.8734\n",
      "\n",
      "Epoch 00203: loss did not improve\n",
      "Epoch 204/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5555 - acc: 0.8747\n",
      "\n",
      "Epoch 00204: loss improved from 0.56103 to 0.55547, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 205/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5610 - acc: 0.8749\n",
      "\n",
      "Epoch 00205: loss did not improve\n",
      "Epoch 206/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5541 - acc: 0.8746\n",
      "\n",
      "Epoch 00206: loss improved from 0.55547 to 0.55407, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 207/300\n",
      "18946/18946 [==============================] - 332s 17ms/step - loss: 0.5558 - acc: 0.8737\n",
      "\n",
      "Epoch 00207: loss did not improve\n",
      "Epoch 208/300\n",
      "18946/18946 [==============================] - 331s 17ms/step - loss: 0.5571 - acc: 0.8733\n",
      "\n",
      "Epoch 00208: loss did not improve\n",
      "Epoch 209/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5519 - acc: 0.8774\n",
      "\n",
      "Epoch 00209: loss improved from 0.55407 to 0.55186, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 210/300\n",
      "18946/18946 [==============================] - 332s 17ms/step - loss: 0.5513 - acc: 0.8795\n",
      "\n",
      "Epoch 00210: loss improved from 0.55186 to 0.55134, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 211/300\n",
      "18946/18946 [==============================] - 332s 18ms/step - loss: 0.5549 - acc: 0.8702\n",
      "\n",
      "Epoch 00211: loss did not improve\n",
      "Epoch 212/300\n",
      "18946/18946 [==============================] - 333s 18ms/step - loss: 0.5519 - acc: 0.8762\n",
      "\n",
      "Epoch 00212: loss did not improve\n",
      "Epoch 213/300\n",
      "18946/18946 [==============================] - 334s 18ms/step - loss: 0.5478 - acc: 0.8766\n",
      "\n",
      "Epoch 00213: loss improved from 0.55134 to 0.54781, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 214/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.5560 - acc: 0.8739\n",
      "\n",
      "Epoch 00214: loss did not improve\n",
      "Epoch 215/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.5489 - acc: 0.8758\n",
      "\n",
      "Epoch 00215: loss did not improve\n",
      "Epoch 216/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.5441 - acc: 0.8769\n",
      "\n",
      "Epoch 00216: loss improved from 0.54781 to 0.54414, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 217/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.5409 - acc: 0.8773\n",
      "\n",
      "Epoch 00217: loss improved from 0.54414 to 0.54089, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 218/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.5445 - acc: 0.8764\n",
      "\n",
      "Epoch 00218: loss did not improve\n",
      "Epoch 219/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.5436 - acc: 0.8769\n",
      "\n",
      "Epoch 00219: loss did not improve\n",
      "Epoch 220/300\n",
      "18946/18946 [==============================] - 336s 18ms/step - loss: 0.5486 - acc: 0.8765\n",
      "\n",
      "Epoch 00220: loss did not improve\n",
      "Epoch 221/300\n",
      "18946/18946 [==============================] - 334s 18ms/step - loss: 0.5406 - acc: 0.8761\n",
      "\n",
      "Epoch 00221: loss improved from 0.54089 to 0.54058, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 222/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.5474 - acc: 0.8747\n",
      "\n",
      "Epoch 00222: loss did not improve\n",
      "Epoch 223/300\n",
      "18946/18946 [==============================] - 335s 18ms/step - loss: 0.5389 - acc: 0.8749\n",
      "\n",
      "Epoch 00223: loss improved from 0.54058 to 0.53886, saving model to checkpionts_1d_24000x5_mixup_fully_trained/best_99.h5\n",
      "Epoch 224/300\n",
      "16412/18946 [========================>.....] - ETA: 44s - loss: 0.5403 - acc: 0.8756"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "            \n",
    "config = Config(sampling_rate=24000, audio_duration=5, n_classes=41, n_folds=10, max_epochs=300, n_mfcc=128)\n",
    "PREDICTION_FOLDER = \"predictions_1d_24000x5_mixup_fully_trained\"\n",
    "CHECKPOINT_FOLDER = 'checkpionts_1d_24000x5_mixup_fully_trained'\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 44\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "#     y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = X_train[train_split]\n",
    "#     y = y_train[train_split]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "i = 99\n",
    "X = np.r_[X_train, X_train_mixup]\n",
    "y = np.r_[y_train, y_train_mixup]\n",
    "print('train shape: ', X.shape, y.shape)\n",
    "# print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "print(\"Fold: \", i)\n",
    "\n",
    "model = get_model()\n",
    "parallel_model = multi_gpu_model(model, gpus=3)\n",
    "\n",
    "#     model.load_weights('checkpoint_1d_24000x5' + '/best_99.h5')\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6, amsgrad=True)\n",
    "parallel_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=adam,\n",
    "                     metrics=['acc'])\n",
    "\n",
    "\n",
    "#     checkpoint = ModelCheckpoint(CHECKPOINT_FOLDER+'/best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "#     checkpoint = CustomModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "checkpoint = ParallelModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i, monitor='loss', \n",
    "                                     verbose=1, save_best_only=True, save_weights_only=True)\n",
    "early = EarlyStopping(monitor=\"loss\", mode=\"min\", patience=15)\n",
    "tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "rlrop = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5)\n",
    "log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "callbacks_list = [checkpoint, early, tb, rlrop, log]\n",
    "\n",
    "history = parallel_model.fit(X, y, validation_data=None, callbacks=callbacks_list, \n",
    "                    batch_size=batch_size, epochs=config.max_epochs, shuffle=True)\n",
    "\n",
    "\n",
    "# model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "# #     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "\n",
    "\n",
    "# # Save train predictions\n",
    "# predictions = model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "# np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "# # Save test predictions\n",
    "# predictions = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "# np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "# # Make a submission file\n",
    "# top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "# predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "# test['label'] = predicted_labels\n",
    "# test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9400/9400 [==============================] - 69s 7ms/step\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(CHECKPOINT_FOLDER + '/best_99.h5')\n",
    "parallel_model = multi_gpu_model(model, gpus=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Save train predictions\n",
    "# predictionparallel_modelparallel_model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "# np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "# Save test predictions\n",
    "predictions = parallel_model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "# Make a submission file\n",
    "top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "test['label'] = predicted_labels\n",
    "test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_list_mixup = []\n",
    "pred_list = []\n",
    "\n",
    "# for i in range(10):\n",
    "#     pred_list.append(np.load(\"predictions_log_mel_sp_24000x5_mixup/test_predictions_%d.npy\"%i))\n",
    "# for i in range(7):\n",
    "#     pred_list.append(np.load(\"../predictions_1d_24000x5/test_predictions_%d.npy\"%i))\n",
    "# for i in range(7):\n",
    "#     pred_list.append(np.load('predictions_24000x2_log_sp/test_predictions_%d.npy'%i))\n",
    "\n",
    "# for i in range(7):\n",
    "#     pred_list.append(np.load('predictions_log_mel_sp_24000x5_image_gen/test_predictions_%d.npy'%i))\n",
    "pred_list.append(np.load('predictions_1d_24000x5_mixup_fully_trained/test_predictions_99.npy'))\n",
    "\n",
    "# prediction_mixup = np.ones_like(pred_list[0])\n",
    "prediction = np.ones_like(pred_list[0])\n",
    "\n",
    "\n",
    "\n",
    "# for pred in pred_list_mixup:\n",
    "#     prediction_mixup = prediction_mixup * pred\n",
    "for pred in pred_list:\n",
    "    prediction = prediction * pred\n",
    "\n",
    "    \n",
    "# prediction_mixup = prediction_mixup**(1./len(pred_list_mixup))\n",
    "prediction = prediction**(1./len(pred_list))\n",
    "\n",
    "\n",
    "# prediction = 0.5 * prediction + prediction_mixup\n",
    "\n",
    "# Make a submission file\n",
    "top_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n",
    "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "test = pd.read_csv('../audio-data/sample_submission.csv')\n",
    "test['label'] = predicted_labels\n",
    "test[['fname', 'label']].to_csv(\"1d_fully.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (17036, 120000, 1) (17036, 41)\n",
      "val shape:  (955, 120000, 1) (955, 41)\n",
      "Fold:  0\n",
      "78\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 120000, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_169 (Conv1D)             (None, 120000, 64)   256         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_145 (Lambda)             (None, 120000, 32)   0           conv1d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_170 (Conv1D)             (None, 120000, 64)   6208        lambda_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_146 (Lambda)             (None, 120000, 32)   0           conv1d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_171 (Conv1D)             (None, 120000, 64)   6208        lambda_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_172 (Conv1D)             (None, 120000, 32)   64          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_147 (Lambda)             (None, 120000, 32)   0           conv1d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 120000, 32)   0           conv1d_172[0][0]                 \n",
      "                                                                 lambda_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 60000, 32)    0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 60000, 32)    128         max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_173 (Conv1D)             (None, 60000, 64)    6208        batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_148 (Lambda)             (None, 60000, 32)    0           conv1d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_174 (Conv1D)             (None, 60000, 64)    6208        lambda_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_149 (Lambda)             (None, 60000, 32)    0           conv1d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_175 (Conv1D)             (None, 60000, 64)    6208        lambda_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_150 (Lambda)             (None, 60000, 32)    0           conv1d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 60000, 32)    0           batch_normalization_49[0][0]     \n",
      "                                                                 lambda_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 30000, 32)    0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 30000, 32)    128         max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_176 (Conv1D)             (None, 30000, 128)   12416       batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_151 (Lambda)             (None, 30000, 64)    0           conv1d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_177 (Conv1D)             (None, 30000, 128)   24704       lambda_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_152 (Lambda)             (None, 30000, 64)    0           conv1d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_178 (Conv1D)             (None, 30000, 128)   24704       lambda_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_179 (Conv1D)             (None, 30000, 64)    2112        batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_153 (Lambda)             (None, 30000, 64)    0           conv1d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 30000, 64)    0           conv1d_179[0][0]                 \n",
      "                                                                 lambda_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 15000, 64)    0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 15000, 64)    256         max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_180 (Conv1D)             (None, 15000, 128)   24704       batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_154 (Lambda)             (None, 15000, 64)    0           conv1d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_181 (Conv1D)             (None, 15000, 128)   24704       lambda_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_155 (Lambda)             (None, 15000, 64)    0           conv1d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_182 (Conv1D)             (None, 15000, 128)   24704       lambda_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_156 (Lambda)             (None, 15000, 64)    0           conv1d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 15000, 64)    0           batch_normalization_51[0][0]     \n",
      "                                                                 lambda_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 7500, 64)     0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 7500, 64)     256         max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_183 (Conv1D)             (None, 7500, 256)    49408       batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_157 (Lambda)             (None, 7500, 128)    0           conv1d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_184 (Conv1D)             (None, 7500, 256)    98560       lambda_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_158 (Lambda)             (None, 7500, 128)    0           conv1d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_185 (Conv1D)             (None, 7500, 256)    98560       lambda_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_186 (Conv1D)             (None, 7500, 128)    8320        batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_159 (Lambda)             (None, 7500, 128)    0           conv1d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 7500, 128)    0           conv1d_186[0][0]                 \n",
      "                                                                 lambda_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 3750, 128)    0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 3750, 128)    512         max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_187 (Conv1D)             (None, 3750, 256)    98560       batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_160 (Lambda)             (None, 3750, 128)    0           conv1d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_188 (Conv1D)             (None, 3750, 256)    98560       lambda_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_161 (Lambda)             (None, 3750, 128)    0           conv1d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_189 (Conv1D)             (None, 3750, 256)    98560       lambda_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_162 (Lambda)             (None, 3750, 128)    0           conv1d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 3750, 128)    0           batch_normalization_53[0][0]     \n",
      "                                                                 lambda_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 1875, 128)    0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 1875, 128)    512         max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_190 (Conv1D)             (None, 1875, 512)    197120      batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_163 (Lambda)             (None, 1875, 256)    0           conv1d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_191 (Conv1D)             (None, 1875, 512)    393728      lambda_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_164 (Lambda)             (None, 1875, 256)    0           conv1d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_192 (Conv1D)             (None, 1875, 512)    393728      lambda_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_193 (Conv1D)             (None, 1875, 256)    33024       batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_165 (Lambda)             (None, 1875, 256)    0           conv1d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 1875, 256)    0           conv1d_193[0][0]                 \n",
      "                                                                 lambda_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 937, 256)     0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 937, 256)     1024        max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_194 (Conv1D)             (None, 937, 512)     393728      batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_166 (Lambda)             (None, 937, 256)     0           conv1d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_195 (Conv1D)             (None, 937, 512)     393728      lambda_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_167 (Lambda)             (None, 937, 256)     0           conv1d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_196 (Conv1D)             (None, 937, 512)     393728      lambda_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_168 (Lambda)             (None, 937, 256)     0           conv1d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 937, 256)     0           batch_normalization_55[0][0]     \n",
      "                                                                 lambda_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 256)          0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 256)          1024        global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 41)           10537       batch_normalization_56[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,933,097\n",
      "Trainable params: 2,931,177\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17036 samples, validate on 955 samples\n",
      "Epoch 1/300\n",
      "17036/17036 [==============================] - 797s 47ms/step - loss: 3.6714 - acc: 0.0998 - val_loss: 4.0939 - val_acc: 0.0503\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.09392, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 2/300\n",
      "17036/17036 [==============================] - 784s 46ms/step - loss: 3.1806 - acc: 0.1736 - val_loss: 3.6243 - val_acc: 0.1131\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.09392 to 3.62433, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 3/300\n",
      "17036/17036 [==============================] - 784s 46ms/step - loss: 3.1242 - acc: 0.1966 - val_loss: 3.2847 - val_acc: 0.1173\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.62433 to 3.28470, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 4/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 3.0236 - acc: 0.2251 - val_loss: 3.2492 - val_acc: 0.1571\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.28470 to 3.24915, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 5/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.9491 - acc: 0.2432 - val_loss: 2.8970 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.24915 to 2.89697, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 6/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.8804 - acc: 0.2630 - val_loss: 2.9350 - val_acc: 0.2702\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.8194 - acc: 0.2830 - val_loss: 2.9050 - val_acc: 0.2796\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.7779 - acc: 0.2911 - val_loss: 2.9266 - val_acc: 0.2733\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.7399 - acc: 0.3062 - val_loss: 3.2940 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.6876 - acc: 0.3183 - val_loss: 2.7808 - val_acc: 0.2984\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.89697 to 2.78077, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 11/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.6545 - acc: 0.3342 - val_loss: 2.9828 - val_acc: 0.2995\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.6168 - acc: 0.3440 - val_loss: 2.5241 - val_acc: 0.4021\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.78077 to 2.52414, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 13/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.5716 - acc: 0.3625 - val_loss: 2.7692 - val_acc: 0.2691\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.5643 - acc: 0.3611 - val_loss: 4.4865 - val_acc: 0.1770\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.5473 - acc: 0.3678 - val_loss: 2.5423 - val_acc: 0.4272\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.5025 - acc: 0.3750 - val_loss: 2.8311 - val_acc: 0.2419\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.4607 - acc: 0.3962 - val_loss: 7.0121 - val_acc: 0.1592\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.4207 - acc: 0.4063 - val_loss: 2.4676 - val_acc: 0.4105\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.52414 to 2.46761, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 19/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.4013 - acc: 0.4105 - val_loss: 2.6523 - val_acc: 0.3581\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.3807 - acc: 0.4212 - val_loss: 2.3376 - val_acc: 0.4178\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.46761 to 2.33759, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 21/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.3447 - acc: 0.4276 - val_loss: 2.2433 - val_acc: 0.4576\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.33759 to 2.24326, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 22/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.3259 - acc: 0.4318 - val_loss: 2.6091 - val_acc: 0.3654\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.2893 - acc: 0.4392 - val_loss: 2.2956 - val_acc: 0.4576\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/300\n",
      "17036/17036 [==============================] - 780s 46ms/step - loss: 2.2603 - acc: 0.4510 - val_loss: 2.1995 - val_acc: 0.4586\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.24326 to 2.19953, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 25/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.2750 - acc: 0.4441 - val_loss: 2.3292 - val_acc: 0.4471\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.2145 - acc: 0.4596 - val_loss: 2.2226 - val_acc: 0.4890\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.1737 - acc: 0.4785 - val_loss: 2.1742 - val_acc: 0.5079\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.19953 to 2.17421, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 28/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.1572 - acc: 0.4821 - val_loss: 2.4434 - val_acc: 0.4503\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.1288 - acc: 0.4867 - val_loss: 2.2196 - val_acc: 0.4503\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.1122 - acc: 0.4928 - val_loss: 2.1758 - val_acc: 0.4754\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.0881 - acc: 0.4992 - val_loss: 2.2515 - val_acc: 0.4440\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.0748 - acc: 0.5019 - val_loss: 2.2171 - val_acc: 0.4953\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.0555 - acc: 0.5080 - val_loss: 2.0928 - val_acc: 0.5707\n",
      "\n",
      "Epoch 00033: val_loss improved from 2.17421 to 2.09276, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 34/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 2.0180 - acc: 0.5187 - val_loss: 1.9160 - val_acc: 0.5466\n",
      "\n",
      "Epoch 00034: val_loss improved from 2.09276 to 1.91603, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 35/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 2.0007 - acc: 0.5218 - val_loss: 2.4348 - val_acc: 0.3267\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.9882 - acc: 0.5284 - val_loss: 2.0556 - val_acc: 0.5099\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 1.9598 - acc: 0.5410 - val_loss: 2.0164 - val_acc: 0.5194\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.9323 - acc: 0.5456 - val_loss: 2.1505 - val_acc: 0.5298\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.9297 - acc: 0.5436 - val_loss: 1.9571 - val_acc: 0.5686\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.8905 - acc: 0.5566 - val_loss: 2.2887 - val_acc: 0.5005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 1.7383 - acc: 0.6017 - val_loss: 1.5929 - val_acc: 0.6482\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.91603 to 1.59289, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 42/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.6898 - acc: 0.6167 - val_loss: 1.6042 - val_acc: 0.6723\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.6615 - acc: 0.6244 - val_loss: 1.5553 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.59289 to 1.55534, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 44/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.6438 - acc: 0.6279 - val_loss: 1.5716 - val_acc: 0.6586\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 1.6290 - acc: 0.6357 - val_loss: 1.5538 - val_acc: 0.6670\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.55534 to 1.55381, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 46/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.6091 - acc: 0.6386 - val_loss: 1.5359 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.55381 to 1.53590, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 47/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.6116 - acc: 0.6382 - val_loss: 1.5088 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.53590 to 1.50879, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 48/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5998 - acc: 0.6433 - val_loss: 1.5477 - val_acc: 0.6754\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5886 - acc: 0.6429 - val_loss: 1.5352 - val_acc: 0.6723\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5744 - acc: 0.6477 - val_loss: 1.5622 - val_acc: 0.6754\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5650 - acc: 0.6541 - val_loss: 1.5019 - val_acc: 0.6827\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.50879 to 1.50188, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 52/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5611 - acc: 0.6550 - val_loss: 1.5133 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5447 - acc: 0.6560 - val_loss: 1.4999 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00053: val_loss improved from 1.50188 to 1.49991, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 54/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5304 - acc: 0.6607 - val_loss: 1.4916 - val_acc: 0.6848\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.49991 to 1.49157, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 55/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5258 - acc: 0.6618 - val_loss: 1.4630 - val_acc: 0.6764\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.49157 to 1.46300, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 56/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5121 - acc: 0.6639 - val_loss: 1.4978 - val_acc: 0.6649\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.5039 - acc: 0.6684 - val_loss: 1.4914 - val_acc: 0.6838\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.4999 - acc: 0.6691 - val_loss: 1.4465 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00058: val_loss improved from 1.46300 to 1.44646, saving model to checkpionts_1d_24000x5_mixup/best_0.h5\n",
      "Epoch 59/300\n",
      "17036/17036 [==============================] - 778s 46ms/step - loss: 1.4862 - acc: 0.6731 - val_loss: 1.4521 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/300\n",
      "17036/17036 [==============================] - 779s 46ms/step - loss: 1.4836 - acc: 0.6739 - val_loss: 1.5038 - val_acc: 0.6691\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/300\n",
      "11145/17036 [==================>...........] - ETA: 4:24 - loss: 1.4540 - acc: 0.6764"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "            \n",
    "config = Config(sampling_rate=24000, audio_duration=5, n_classes=41, n_folds=10, max_epochs=300, n_mfcc=128)\n",
    "PREDICTION_FOLDER = \"predictions_1d_24000x5_mixup\"\n",
    "CHECKPOINT_FOLDER = 'checkpionts_1d_24000x5_mixup'\n",
    "\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=config.n_folds, shuffle=True, random_state=2)\n",
    "\n",
    "\n",
    "for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "    X = np.r_[X_train[train_split], X_train_mixup[train_split]]\n",
    "    y = np.r_[y_train[train_split], y_train_mixup[train_split]]\n",
    "    X_val = X_train[val_split]\n",
    "    y_val = y_train[val_split]\n",
    "    \n",
    "# for i, (train_split, val_split) in enumerate(skf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "#     X = X_train[train_split]\n",
    "#     y = y_train[train_split]\n",
    "#     X_val = X_train[val_split]\n",
    "#     y_val = y_train[val_split]\n",
    "    \n",
    "    print('train shape: ', X.shape, y.shape)\n",
    "    print('val shape: ', X_val.shape, y_val.shape)\n",
    "\n",
    "    print(\"Fold: \", i)\n",
    "    \n",
    "    model = get_model()\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#     model.load_weights('checkpoint_1d_24000x5' + '/best_99.h5')\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6, amsgrad=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=adam,\n",
    "                         metrics=['acc'])\n",
    "\n",
    "\n",
    "    #     checkpoint = ModelCheckpoint(CHECKPOINT_FOLDER+'/best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "#     checkpoint = CustomModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "    checkpoint = ParallelModelCheckpoint(model, CHECKPOINT_FOLDER + '/best_%d.h5'%i, monitor='val_loss', \n",
    "                                         verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    log = CSVLogger(PREDICTION_FOLDER + '/log_%d.csv'%i)\n",
    "    callbacks_list = [checkpoint, early, tb, rlrop, log]\n",
    "    \n",
    "    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=callbacks_list, \n",
    "                        batch_size=batch_size, epochs=config.max_epochs, shuffle=True)\n",
    "    \n",
    "\n",
    "    model.load_weights(CHECKPOINT_FOLDER + '/best_%d.h5'%i)\n",
    "#     parallel_model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "\n",
    "    \n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
