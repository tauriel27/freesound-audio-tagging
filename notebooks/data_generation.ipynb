{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import librosa\n",
    "import shutil\n",
    "import keras\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.applications import *\n",
    "from keras.utils import *\n",
    "from keras.callbacks import *\n",
    "from sklearn.model_selection import *\n",
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          fname         label  manually_verified\n",
      "0  00044347.wav        Hi-hat                  0\n",
      "1  001ca53d.wav     Saxophone                  1\n",
      "2  002d256b.wav       Trumpet                  0\n",
      "3  0033e230.wav  Glockenspiel                  1\n",
      "4  00353774.wav         Cello                  1\n",
      "          fname         label  manually_verified\n",
      "0  00044347.wav        Hi-hat                  0\n",
      "1  001ca53d.wav     Saxophone                  1\n",
      "2  002d256b.wav       Trumpet                  0\n",
      "3  0033e230.wav  Glockenspiel                  1\n",
      "4  00353774.wav         Cello                  1\n"
     ]
    }
   ],
   "source": [
    "os.listdir('../audio-data/')\n",
    "train_path = '../audio-data/audio_train/'\n",
    "test_path = '../audio-data/audio_test/'\n",
    "train = pd.read_csv('../audio-data/train.csv')\n",
    "test = pd.read_csv('../audio-data/sample_submission.csv')\n",
    "# print('training samples: ', len(os.listdir(train_path)))\n",
    "# print('test samples: ', len(os.listdir(test_path)))\n",
    "# print('training labels: ', len(train.label.unique()))\n",
    "print(train.head())\n",
    "LABELS = list(train.label.unique())\n",
    "label_idx = {label: i for i, label in enumerate(LABELS)}\n",
    "train.set_index('fname', inplace=True)\n",
    "test.set_index('fname', inplace=True)\n",
    "train['label_idx'] = train.label.apply(lambda x: label_idx[x])\n",
    "\n",
    "os.listdir('../audio-data/')\n",
    "train_path = '../audio-data/audio_train/'\n",
    "test_path = '../audio-data/audio_test/'\n",
    "train = pd.read_csv('../audio-data/train.csv')\n",
    "test = pd.read_csv('../audio-data/sample_submission.csv')\n",
    "# print('training samples: ', len(os.listdir(train_path)))\n",
    "# print('test samples: ', len(os.listdir(test_path)))\n",
    "# print('training labels: ', len(train.label.unique()))\n",
    "print(train.head())\n",
    "LABELS = list(train.label.unique())\n",
    "label_idx = {label: i for i, label in enumerate(LABELS)}\n",
    "train.set_index('fname', inplace=True)\n",
    "test.set_index('fname', inplace=True)\n",
    "train['label_idx'] = train.label.apply(lambda x: label_idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                sampling_rate=44100, audio_duration=2, n_classes=41,\n",
    "                use_mfcc=False, n_folds=10, learning_rate=0.0001,\n",
    "                max_epochs=50, n_mfcc=20, use_log_sp=False, \n",
    "                use_mixup=False, alpha=0.2, use_log_mel_sp=False, use_cqt=False):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = n_classes\n",
    "        self.use_log_sp = use_log_sp\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.use_mixup = use_mixup\n",
    "        self.use_log_mel_sp = use_log_mel_sp\n",
    "        self.use_cqt = use_cqt\n",
    "        self.alpha = alpha\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_folds = n_folds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            # np.floor 计算比每一个元素小或相等的最大的整数\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        elif self.use_log_sp:\n",
    "            self.dim = (self.audio_duration*100-1, self.sampling_rate//100+1, 1)\n",
    "        elif self.use_log_mel_sp or self.use_cqt:\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram.shape[0] = time*100 - 1\n",
    "# spectrogram.shape[1] = rate/100 + 1\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "def audio_norm(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data - min_data) / (max_data - min_data + 1e-6)\n",
    "    return data - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499, 442, 1)\n",
      "1000/9473\n",
      "2000/9473\n",
      "3000/9473\n",
      "4000/9473\n",
      "5000/9473\n",
      "6000/9473\n",
      "7000/9473\n",
      "8000/9473\n",
      "9000/9473\n",
      "1000/9400\n",
      "2000/9400\n",
      "3000/9400\n",
      "4000/9400\n",
      "5000/9400\n",
      "6000/9400\n",
      "7000/9400\n",
      "8000/9400\n",
      "9000/9400\n",
      "(9473, 499, 442, 1)\n",
      "(9400, 499, 442, 1)\n",
      "(9473, 41)\n",
      "CPU times: user 7min 8s, sys: 54.6 s, total: 8min 3s\n",
      "Wall time: 14min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def prepare_data(df, config, data_dir):\n",
    "    X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))\n",
    "    input_length = config.audio_length\n",
    "    count = 0\n",
    "    for i, fname in enumerate(df.index):\n",
    "        file_path = data_dir + str(fname)\n",
    "        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, \n",
    "                                    duration = config.audio_duration,\n",
    "                                    res_type=\"kaiser_fast\")\n",
    "\n",
    "        # Random offset / Padding\n",
    "        if len(data) > input_length:\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "        else:\n",
    "            if input_length > len(data):\n",
    "                max_offset = input_length - len(data)\n",
    "                offset = np.random.randint(max_offset)\n",
    "            else:\n",
    "                offset = 0\n",
    "            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "        if config.use_mfcc:\n",
    "            data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n",
    "            data = np.expand_dims(data, axis=-1)\n",
    "        elif config.use_log_sp:\n",
    "            freqs, times, data = log_specgram(data, config.sampling_rate)\n",
    "            data = np.expand_dims(data, axis=-1)\n",
    "        elif config.use_log_mel_sp:\n",
    "            mel_spec = librosa.feature.melspectrogram(data, sr=config.sampling_rate, n_mels=config.n_mfcc)\n",
    "            log_mel_spec = librosa.core.power_to_db(mel_spec)\n",
    "            data = np.expand_dims(log_mel_spec, axis=-1)\n",
    "        elif config.use_cqt:\n",
    "            chroma_cq = librosa.feature.chroma_cqt(data, sr=config.sampling_rate, n_chroma=config.n_mfcc)\n",
    "            data = np.expand_dims(chroma_cq, axis=-1)\n",
    "            \n",
    "        X[i,] = data\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print('%d/%d'%(count, len(df)))\n",
    "    return X\n",
    "\n",
    "config = Config(use_log_sp=True, sampling_rate=44100, audio_duration=5, n_classes=41, n_mfcc=200)\n",
    "print(config.dim)\n",
    "X_train = prepare_data(train, config, '../audio-data/audio_train/')\n",
    "X_test = prepare_data(test, config, '../audio-data/audio_test/')\n",
    "# y_train = to_categorical(train.label_idx, num_classes=41)\n",
    "\n",
    "np.save(\"X_44100x5_ls_train.npy\", X_train)\n",
    "np.save(\"X_44100x5_ls_test.npy\", X_test)\n",
    "# np.save(\"y_train.npy\", y_train)\n",
    "\n",
    "# X_train = np.load(\"X_train.npy\")\n",
    "# X_test = np.load(\"X_test.npy\")\n",
    "# y_train = np.load(\"y_train.npy\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
